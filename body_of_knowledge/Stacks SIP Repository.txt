# Preamble

SIP Number: 004

Title: Cryptographic Commitment to Materialized Views

Author: Jude Nelson <jude@stacks.org>

Consideration: Technical

Type: Consensus

Status: Ratified

Created: 15 July 2019

License: BSD 2-Clause

Sign-off: Jude Nelson <jude@stacks.org>, Technical Steering Committee Chair

Discussions-To: https://github.com/stacksgov/sips

# Abstract

Blockchain peers are replicated state machines, and as such, must maintain a
materialized view of all of the state the transaction log represents in order to
validate a subsequent transaction.  The Stacks blockchain in particular not only
maintains a materialized view of the state of every fork, but also requires
miners to cryptographically commit to that view whenever they mine a block.
This document describes a **Merklized Adaptive Radix Forest** (MARF), an
authenticated index data structure for efficiently encoding a 
cryptographic commitment to blockchain state.

The MARF's structure is part of the consensus logic in the Stacks blockchain --
every Stacks peer must process the MARF the same way.  Stacks miners announce
a cryptographic hash of their chain tip's MARF in the blocks they produce, and in
doing so, demonstrate to each peer and each light client that they have 
applied the block's transactions to the peer's state correctly.

The MARF represents blockchain state as an authenticated directory.  State is
represented as key/value pairs.  The MARF structure gives a peer the ability to
prove to a light client that a particular key has a particular value, given the
MARF's cryptographic hash.  The proof has _O(log B)_ space for _B_ blocks, and
takes _O(log B)_ time complexity to produce and verify.  In addition, it offers
_O(1)_ expected time and space complexity for inserts and queries.
The MARF proof allows a light client to determine:

* What the value of a particular key is,
* How much cumulative energy has been spent to produce the key/value pair,
* How many confirmations the key/value pair has.

# Introduction

In order to generate a valid transaction, a blockchain client needs to be able
to query the current state of the blockchain.  For example, in Bitcoin, a client
needs to query its unspent transaction outputs (UTXOs) in order to satisfy their
spending conditions in a new transaction.  As another example, in Ethereum, a
client needs to query its accounts' current nonces in order to generate a valid
transaction to spend their tokens.

Whether or not a blockchain's peers are required to commit to the current state
in the blocks themselves (i.e. as part of the consensus logic) is a
philosophical decision.  We argue that it is a highly desirable in Blockstack's
case, since it affords light clients more security when querying the blockchain state than
not.  This is because a client often queries state that was last updated several
blocks in the past (i.e. and is "confirmed").  If a blockchain peer can prove to
a client that a particular key in the state has a particular value, and was last
updated a certain number of blocks in the past, then the client can determine
whether or not to trust the peer's proof based on factors beyond simply trusting
the remote peer to be honest.  In particular, the client can determine how
difficult it would be to generate a dishonest proof, in terms of the number of
blocks that would need to be maliciously crafted and accepted by the network.
This offers clients some protection against peers that would lie to them -- a
lying peer would need to spend a large amount of energy (and money) in order to
do so.

Specific to Blockstack, we envision that many applications will run
their own Stacks-based blockchain peer networks that operate "on top" of the
Stacks blockchain through proof-of-burn.  This means that the Blockstack
application ecosystem will have many parallel "app chains" that users may wish
to interact with.  While a cautious power user may run validator nodes for each
app chain they are interested in, we expect that most users will not do so,
especially if they are just trying out the application or are casual users.  In
order to afford these users better security than simply telling them to find a
trusted validating peer, it is essential that each Stacks peer commits to its
materialized view in each block.

On top of providing better security to light clients, committing to the materialized
state view in each block has the additional benefit of helping the peer network
detect malfunctioning miners early on.  A malfunctioning miner will calculate a
different materialized view using the same transactions, and with overwhelmingly
high probability, will also calculate a different state view hash.  This makes
it easy for a blockchain's peers to reject a block produced in this manner
outright, without having to replay its transactions.

## Design Considerations

Committing to the materialized view in each block has a non-zero cost in terms
of time and space complexity.  This cost is paid for by the transaction fee, but
it effectively sets an upper bound on how many key/value insertions can happen
per block -- in particular, generating and validating the block must be faster
than the underlying burnchain block times. This means that it
is of paramount importance to keep the materialized view digest calculation as
fast as possible.

The following considerations have a non-trivial impact on the design of the
MARF:

**A transaction can read or write any prior state in the same fork.**  This
means that the index must support fast random-access reads and fast
random writes.

**The Stacks blockchain can fork, and a miner can produce a fork at any block
height in the past.**  As argued in SIP 001, a Stacks blockchain peer must process
all forks and keep their blocks around.  This also means that a peer needs to
calculate and validate the materialized view of each fork, no matter where it
occurs.  This is also necessary because a client may request a proof for some
state in any fork -- in order to service such requests, the peer must calculate
the materialized view for all forks.

**Forks can occur in any order, and blocks can arrive in any order.**  As such,
the runtime cost of calculating the materialized view must be _independent_ of the
order in which forks are produced, as well as the order in which their blocks
arrive.  This is required in order to avoid denial-of-service vulnerabilities,
whereby a network attacker can control the schedules of both
forks and block arrivals in a bid to force each peer to expend resources
validating the fork.  It must be impossible for an attacker to
significantly slow down the peer network by maliciously varying either schedule.
This has non-trivial consequences for the design of the data structures for
encoding materialized views.

# Specification

The Stacks peer's materialized view is realized as a flat key/value store.
Transactions encode zero or more creates, inserts, updates, and deletes on this
key/value store.  As a consequence of needing to support forks from any prior block,
no data is ever removed; instead, a "delete" on a particular key is encoded 
by replacing the value with a tombstone record.  The materialized view is the
subset of key/value pairs that belong to a particular fork in the blockchain.

The Stacks blockchain separates the concern of maintaining _authenticated
index_ over data from storing a copy of the data itself.  The blockchain peers
commit to the digest of the authenticated index, but can store the data however
they want.  The authenticated index is realized as a _Merklized Adaptive Radix
Forest_ (MARF).  The MARF gives Stacks peers the ability to prove that a
particular key in the materialized view maps to a particular value in a
particular fork.

A MARF has two principal data structures:  a _merklized adaptive radix trie_
for each block and a _merklized skip-list_ that
cryptographically links merklized adaptive radix tries in prior blocks to the
current block.

## Merklized Adaptive Radix Tries (ARTs)

An _adaptive radix trie_ (ART) is a prefix tree where each node's branching
factor varies with the number of children.  In particular, a node's branching
factor increases according to a schedule (0, 4, 16, 48, 256) as more and more
children are added.  This behavior, combined with the usual sparse trie
optimizations of _lazy expansion_ and _path compression_, produce a tree-like
index over a set key/value pairs that is _shallower_ than a perfectly-balanced
binary search tree over the same values.  Details on the analysis of ARTs can
be found in [1].

To produce an _index_ over new state introduced in this block, the Stacks peer
will produce an adaptive radix trie that describes each key/value pair modified.
In particular, for each key affected by the block, the Stacks peer will:
* Calculate the hash of the key to get a fixed-length trie path,
* Store the new value and this hash into its data store,
* Insert or update the associated value hash in the block's ART at the trie path,
* Calculate the new Merkle root of the ART by hashing all modified intermediate
  nodes along the path.

In doing so, the Stacks peer produces an authenticated index for all key/value
pairs affected by a block.  The leaves of the ART are the hashes of the values,
and the hashes produced in each intermediate node and root give the peer a
way to cryptographically prove that a particular value is present in the ART
(given the root hash and the key).

The Stacks blockchain employs _path compression_ and _lazy expansion_
to efficiently represent all key/value pairs while minimizing the number of trie
nodes.  That is, if two children share a common prefix, the prefix bytes are
stored in a single intermediate node instead of being spread across multiple
intermediate nodes (path compression).  In the special case where a path suffix
uniquely identifies the leaf, the path suffix will be stored alongside the leaf
instead as a sequence of intermediate nodes (lazy expansion).  As more and more
key/value pairs are inserted, intermediate nodes and leaves with multi-byte
paths will be split into more nodes.

**Trie Structure**

A trie is made up of nodes with radix 4, 16, 48, or 256, as well as leaves.  In
the documentation below, these are called `node4`, `node16`, `node48`,
`node256`, and `leaf` nodes.  An empty trie has a single `node256` as its root.
Child pointers occupy one byte.

**Notation**

The notation `(ab)node256` means "a `node256` who descends from its parent via
byte 0xab".

The notation `node256[path=abcd]` means "a `node256` that has a shared prefix
with is children `abcd`".

**Lazy Expansion**

If a leaf has a non-zero-byte path suffix, and another leaf is inserted that
shares part of the suffix, the common bytes will be split off of the existing
leaf to form a `node4`, whose two immediate children are the two leaves.  Each
of the two leaves will store the path bytes that are unique to them.  For
example, consider this trie with a root `node256` and a single leaf, located at
path `aabbccddeeff00112233` and having value hash `123456`:

```
node256
       \
        (aa)leaf[path=bbccddeeff00112233]=123456
```

If the peer inserts the value hash `98765` at path `aabbccddeeff998877`, the
single leaf's path will be split into a shared prefix and two distinct suffixes,
as follows:

```
insert (aabbccddeeff998877, 98765)

node256                            (00)leaf[path=112233]=123456
       \                          /
        (aa)node4[path-bbccddeeff]
                                  \
                                   (99)leaf[path=887766]=98765
```

Now, the trie encodes both `aabbccddeeff00112233=123456` and
`aabbccddeeff99887766=98765`.

**Node Promotion**

As a node with a small radix gains children, it will eventually need to be
promoted to a node with a higher radix.  A `node4` will become a `node16` when
it receives its 5th child; a `node16` will become a `node48` when it receives
its 17th child, and a `node48` will become a `node256` when it receives its 49th
child.  A `node256` will never need to be promoted, because it has slots for
child pointers with all possible byte values.

For example, consider this trie with a `node4` and 4 children:

```
node256                                (00)leaf[path=112233]=123456
       \                              /
        \                            /  (01)leaf[path=445566]=67890
         \                          /  /
          (aa)node4[path=bbccddeeff]---
                                    \  \
                                     \  (02)leaf[path=778899]=abcdef
                                      \
                                       (99)leaf[path=887766]=98765
```

This trie encodes the following:
   * `aabbccddeeff00112233=123456`
   * `aabbccddeeff01445566=67890`
   * `aabbccddeeff02778899=abcdef`
   * `aabbccddeeff99887766=9876`

Inserting one more node with a prefix `aabbccddeeff` will promote the
intermediate `node4` to a `node16`:

```
insert (aabbccddeeff03aabbcc, 314159)

node256                                 (00)leaf[path=112233]=123456
       \                               /
        \                             /  (01)leaf[path=445566]=67890
         \                           /  /
          (aa)node16[path=bbccddeeff]-----(03)leaf[path=aabbcc]=314159
                                     \  \
                                      \  (02)leaf[path=778899]=abcdef
                                       \
                                        (99)leaf[path=887766]=98765
```

The trie now encodes the following:
   * `aabbccddeeff00112233=123456`
   * `aabbccddeeff01445566=67890`
   * `aabbccddeeff03aabbcc=314159`
   * `aabbccddeeff02778899=abcdef`
   * `aabbccddeeff99887766=9876`

**Path Compression**

Intermediate nodes, such as the `node16` in the previous example, store path
prefixes shared by all of their children.  If a node is inserted that shares
some of this prefix, but not all of it, the path is "decompressed" -- a new
leaf is "spliced" into the compressed path, and attached to a `node4` whose two
children are the leaf and the existing node (i.e. the `node16` in this case)
whose shared path now contains the suffix unique to its children, but distinct
from the newly-spliced leaf.

For example, consider this trie with the intermediate `node16` sharing a path
prefix `bbccddeeff` with its 5 children:

```
node256                                 (00)leaf[path=112233]=123456
       \                               /
        \                             /  (01)leaf[path=445566]=67890
         \                           /  /
          (aa)node16[path=bbccddeeff]-----(03)leaf[path=aabbcc]=314159
                                     \  \
                                      \  (02)leaf[path=778899]=abcdef
                                       \
                                        (99)leaf[path=887766]=98765
```

This trie encodes the following:
   * `aabbccddeeff00112233=123456`
   * `aabbccddeeff01445566=67890`
   * `aabbccddeeff03aabbcc=314159`
   * `aabbccddeeff02778899=abcdef`
   * `aabbccddeeff99887766=9876`

If we inserted `(aabbcc001122334455, 21878)`, the `node16`'s path would be
decompressed to `eeff`, a leaf with the distinct suffix `1122334455` would be spliced
in via a `node4`, and the `node4` would have the shared path prefix `bbcc` with
its now-child `node16` and leaf.

```
insert (aabbcc00112233445566, 21878)

                               (00)leaf[path=112233445566]=21878
                              /
node256                      /                       (00)leaf[path=112233]=123456
       \                    /                       /
        (aa)node4[path=bbcc]                       /  (01)leaf[path=445566]=67890
                            \                     /  /
                             (dd)node16[path=eeff]-----(03)leaf[path=aabbcc]=314159
                                                  \  \
                                                   \  (02)leaf[path=778899]=abcdef
                                                    \
                                                     (99)leaf[path=887766]=98765
```

The resulting trie now encodes the following:
   * `aabbcc00112233445566=21878`
   * `aabbccddeeff00112233=123456`
   * `aabbccddeeff01445566=67890`
   * `aabbccddeeff03aabbcc=314159`
   * `aabbccddeeff02778899=abcdef`
   * `aabbccddeeff99887766=9876`

## Back-pointers

The materialized view of a fork will hold key/value pairs for data produced by
applying _all transactions_ in that fork, not just the ones in the last block.  As such,
the index over all key/value pairs in a fork is encoded in the sequence of 
its block's merklized ARTs.

To ensure that random reads and writes on the a fork's materialized view remain
fast no matter which block added them, a child pointer in an ART can point to
either a node in the same ART, or a node with the same path in a prior ART.  For
example, if the ART at block _N_ has a `node16` whose path is `aabbccddeeff`, and 10
blocks ago a leaf was inserted at path `aabbccddeeff99887766`, it will
contain a child pointer to the intermediate node from 10 blocks ago whose path is
`aabbccddeeff` and who has a child node in slot `0x99`.  This information is encoded
as a _back-pointer_.  To see it visually:

```
At block N


node256                                 (00)leaf[path=112233]=123456
       \                               /
        \                             /  (01)leaf[path=445566]=67890
         \                           /  /
          (aa)node16[path=bbccddeeff]-----(03)leaf[path=aabbcc]=314159
                                     \  \
                                      \  (02)leaf[path=778899]=abcdef
                                       \
                                        |
                                        |
                                        |
At block N-10 - - - - - - - - - - - - - | - - - - - - - - - - - - - - - - - - -
                                        |
node256                                 | /* back-pointer to block N-10 */
       \                                |
        \                               |
         \                              |
          (aa)node4[path=bbccddeeff]    |
                                    \   |
                                     \  |
                                      \ |
                                       (99)leaf[path=887766]=98765
```

By maintaining trie child pointers this way, the act of looking up a path to a value in
a previous block is a matter of following back-pointers to previous tries.
This back-pointer uses the _block-hash_ of the previous block to uniquely identify
the block. In order to keep the in-memory and on-disk representations of trie nodes succint,
the MARF structure uses a locally defined unsigned 32-bit integer to identify the previous 
block, along with a local mapping of such integers to the respective block header hash.

Back-pointers are calculated in a copy-on-write fashion when calculating the ART
for the next block.  When the root node for the ART at block N+1 is created, all
of its children are set to back-pointers that point to the immediate children of
the root of block N's ART.  Then, when inserting a key/value pair, the peer
walks the current ART to the insertion point, but whenever a
back-pointer is encountered, it copies the node it points to into the current
ART, and sets all of its non-empty child pointers to back-pointers.  The peer
then continues traversing the ART until the insertion point is found (i.e. a
node has an unallocated child pointer where the leaf should go), copying
over intermediate nodes lazily.

For example, consider the act of inserting `aabbccddeeff00112233=123456` into an
ART where a previous ART contains the key/value pair
`aabbccddeeff99887766=98765`:

```
At block N


node256                                (00)leaf[path=112233]=123456
^      \                              /
|       \                            /
|        \                          /
|         (aa)node4[path=bbccddeeff]
|                 ^                 \
|                 |                  \
| /* 1. @root. */ | /* 2. @node4.  */ \  /* 3. 00 is empty, so insert */
| /* copy up, &*/ | /* copy up, &  */  |
| /* make back-*/ | /* make back-  */  |
| /* ptr to aa */ | /* ptr to 99   */  |
|                 |                    |
|- At block N-10 -|- - - - - - - - - - | - - - - - - - - - - - - - - - - - -
|                 |                    |
node256           |                    |
       \          |                    |
        \         |                    |
         \        |                    |
          (aa)node4[path=bbccddeeff]   |
                                    \  |
                                     \ |
                                      \|
                                       (99)leaf[path=887766]=98765
```

In step 1, the `node256` in block _N_ would have a back-pointer to the `node4` in
block _N - 10_ in child slot `0xaa`.  While walking path `aabbccddeeff00112233`,
the peer would follow slot `0xaa` to the `node4` in block _N - 10_ and copy it
into block _N_, and would set its child pointer at `0x99` to be a back-pointer
to the `leaf` in block _N - 10_.  It would then step to the `node4` it copied,
and walk path bytes `bbccddeeff`.  When it reaches child slot `0x00`, the peer
sees that it is unallocated, and attaches the leaf with the unexpanded path
suffix `112233`.  The back-pointer to `aabbccddeeff99887766=98765` is thus
preserved in block _N_'s ART.

**Calculating the Root Hash with Back-pointers**

For reasons that will be explained in a moment, the hash of a child node that is a
back-pointer is not calculated the usual way when calculating the root hash of
the Merklized ART.  Instead of taking the hash of the child node (as would be
done for a child in the same ART), the hash of the _block header_ is used
instead.  In the above example, the hash of the `leaf` node whose path is
`aabbccddeeff99887766` would be the hash of block _N - 10_'s header, whereas the
hash of the `leaf` node whose path is `aabbccddeeff00112233` would be the hash
of the value hash `123456`.

The main reason for doing this is to keep block validation time down by a
significant constant factor.  The block header hash is always kept in RAM,
but at least one disk seek is required to read the hash of a child in a separate
ART (and it often takes more than one seek). This does not sacrifice the security
of a Merkle proof of `aabbccddeeff99887766=98765`, but it does alter the mechanics
of calculating and verifying it.

## Merklized Skip-list

The second principal data structure in a MARF is a Merklized skip-list encoded
from the block header hashes and ART root hashes in each block.  The hash of the
root node in the ART for block _N_ is derived not only from the hash of the
root's children, but also from the hashes of the block headers from blocks
`N - 1`, `N - 2`, `N - 4`, `N - 8`, `N - 16`, and so on.  This constitutes
a _Merklized skip-list_ over the sequence of ARTs.

The reason for encoding the root node's hash this way is to make it possible for
peers to create a cryptographic proof that a particular key maps to a particular
value when the value lives in a prior block, and can only be accessed by
following one or more back-pointers.  In addition, the Merkle skip-list affords
a client _two_ ways to verify key-value pairs:  the client only needs either (1)
a known-good root hash, or (2) the sequence of block headers for the Stacks
chain and its underlying burn chain.  Having (2) allows the client to determine
(1), but calculating (2) is expensive for a client doing a small number of
queries.  For this reason, both options are supported.

### Resolving Block Height Queries

For a variety of reasons, the MARF structure must be able to resolve
queries mapping from block heights (or relative block heights) to
block header hashes and vice-versa --- for example, the Clarity VM
allows contracts to inspect this information. Most applicable to the
MARF, though, is that in order to find the ancestor hashes to include
in the Merklized Skip-list, the data structure must be able to find
the block headers which are 1, 2, 4, 8, 16, ... blocks prior in the
same fork. This could be discovered by walking backwards from the
current block, using the previous block header to step back through
the fork's history. However, such a process would require _O(N)_ steps
(where _N_ is the current block height). But, if a mapping exists for
discovering the block at a given block height, this process would instead
be _O(1)_ (because a node will have at most 32 such ancestors).

But correctly implementing such a mapping is not trivial: a given
height could resolve to different blocks in different forks. However,
the MARF itself is designed to handle exactly these kinds of
queries. As such, at the beginning of each new block, the MARF inserts
into the block's trie two entries:

1. This block's block header hash -> this block's height.
2. This block's height -> this block's block header hash.

This mapping allows the ancestor hash calculation to proceed.

## MARF Merkle Proofs

A Merkle proof for a MARF is constructed using a combination of two types of
sub-proofs:  _segment proofs_, and _shunt proofs_.  A _segment proof_ is a proof
that a node belongs to a particular Merklized ART.  It is simply a Merkle tree
proof.  A _shunt proof_ is a proof that the ART for block _N_ is exactly _K_
blocks away from the ART at block _N - K_.  It is generated as a Merkle proof
from the Merkle skip-list.

Calculating a MARF Merkle proof is done by first calculating a segment proof for a
sequence of path prefixes, such that all the nodes in a single prefix are in the
same ART.  To do so, the node walks from the current block's ART's root node
down to the leaf in question, and each time it encounters a back-pointer, it
generates a segment proof from the _currently-visited_ ART to the intermediate
node whose child is the back-pointer to follow.  If a path contains _i_
back-pointers, then there will be _i+1_ segment proofs.

Once the peer has calculated each segment proof, it calculates a shunt proof
that shows that the _i+1_th segment was reached by walking back a given number
of blocks from the _i_th segment by following the _i_th segment's back-pointer.
The final shunt proof for the ART that contains the leaf node includes all of
the prior block header hashes that went into producing its root node's hash.
Each shunt proof is a sequence of sequences of block header hashes and ART root
hashes, such that the hash of the next ART root node can be calculated from the
previous sequence.

For example, consider the following ARTs:

```
At block N


node256                                 (00)leaf[path=112233]=123456
       \                               /
        \                             /  (01)leaf[path=445566]=67890
         \                           /  /
          (aa)node16[path=bbccddeeff]-----(03)leaf[path=aabbcc]=314159
                                     \  \
                                      \  (02)leaf[path=778899]=abcdef
                                       \
                                        |
                                        |
                                        |
At block N-10 - - - - - - - - - - - - - | - - - - - - - - - - - - - - - - - - -
                                        |
node256                                 | /* back-pointer to N - 10 */
       \                                |
        \                               |
         \                              |
          (aa)node4[path=bbccddeeff]    |
                                    \   |
                                     \  |
                                      \ |
                                       (99)leaf[path=887766]=98765
```

To generate a MARF Merkle proof, the client queries a Stacks peer for a
particular value hash, and then requests the peer generate a proof that the key
and value must have been included in the calculation of the current block's ART
root hash (i.e. the digest of the materialized view of this fork). 

For example, given the key/value pair `aabbccddeeff99887766=98765` and the hash
of the ART at block _N_, the peer would generate two segment proofs for the
following paths: `aabbccddeeff` in block _N_, and `aabbccddeeff99887766` in
block `N - 10`.

```
At block N


node256
       \   /* this segment proof would contain the hashes of all other */
        \  /* children of the root, except for the one at 0xaa.        */
         \
          (aa)node16[path=bbccddeeff]

At block N-10 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

node256    /* this segment proof would contain two sequences of hashes: */
       \   /* the hashes for all children of the root besides 0xaa, and */
        \  /* the hashes of all children of the node4, except 0x99.     */
         \
          (aa)node4[path=bbccddeeff]
                                    \
                                     \
                                      \
                                       (99)leaf[path=887766]=98765
```

Then, it would calculate two shunt proofs.  The first proof, called the "head shunt proof,"
supplies the sequence of block hashes for blocks _N - 11, N - 12, N - 14, N - 18, N - 26, ..._ and the 
hash of the children of the root node of the ART for block _N - 10_.  This lets the 
client calculate the hash of the root of the ART at block _N - 10_.  The second
shunt proof (and all subsequent shunt proofs, if there are more back-pointers to
follow) is comprised of the hashes that "went into" calculating the hashes on the
skip-list from the next segment proof's root hash.

In detail, the second shunt proof would have two parts:

* the block header hashes for block _N - 9_ _N - 12_, _N - 16_, _N - 24_, ...
* the block header hashes for _N - 1_, _N - 2_, _N - 4_, _N - 16_, _N - 32_, ...

The reason there are two sequences in this shunt proof is because "walking back"
from block _N_ to block _N - 10_ requires walking first to block _N - 8_ (i.e.
following the skip-list column for 2 ** 3), and then walking to block _N - 10_
from _N - 8_ (i.e. following its skip-list column for 2 ** 1).  The first segment
proof (i.e. with the leaf) lets the client calculate the hash of the children of
the ART root node in block _N - 10_, which when combined with the first part of
this shunt proof yields the ART root hash for _N - 8_.  Then, the client
uses the hash of the children of the root node in the ART of block _N_ (calculated from the second segment
proof), combined with the root hash from node _N - 8_ and with the hashes
in the second piece of this shunt proof, to calculate the ART root hash for
block _N_.  The proof is valid if this calculated root hash matches the root
hash for which it requested the proof.

In order to fully verify the MARF Merkle proof, the client would verify that:

* The first segment proof's path's bytes are equal to the hash of the key for
  which the proof was requested.
* The first segment proof ends in a leaf node, and the leaf node contains the
  hash of the value for which the proof was requested.
* Each segment proof is valid -- the root hash could only be calculated from the
  deepest intermediate node in the segment,
* Each subsequent segment proof was generated from a prefix of the path
  represented by the current segment proof,
* Each back-pointer at the tail of each segment (except the one that terminates
  in the leaf -- i.e. the first one) was a number of blocks back that is equal
  to the number of blocks skipped over in the shunt proof linking it to the next
  segment.
* Each block header was included in the fork the client is querying,
* Each block header was generated from its associated ART root hash,
* (Optional, but encouraged): The burn chain block headers demonstrate that the
  correct difficulty rules were followed.  This step can be skipped if the
client somehow already knows that the hash of block _N_ is valid.

Note that to verify the proof, the client would need to substitute the
_block header hash_ for each intermediate node at the tail of each segment
proof.  The block header hash can either be obtained by fetching the block
headers for both the Stacks chain and burn chain _a priori_ and verifying that
they are valid, or by fetching them on-the-fly.  The second strategy should only
be used if the client's root hash it submits to the peer is known out-of-band to
be the correct hash.

The security of the proof is similar to SPV proofs in Bitcoin -- the proof is
valid assuming the client is able to either verify that the final header hash
represents the true state of the network, or the client is able to fetch the
true burn chain block header sequence.  The client has some assurance that a
_given_ header sequence is the _true_ header sequence, because the header
sequence encodes the proof-of-work that went into producing it.  A header
sequence with a large amount of proof-of-work is assumed to be infeasible for an
attacker to produce -- i.e. only the majority of the burn chain's network hash
power could have produced the header chain.  Regardless of which data the client
has, the usual security assumptions about confirmation depth apply -- a proof
that a key maps to a given value is valid only if the transaction that set
it is unlikely to be reversed by a chain reorg.

## Performance

The time and space complexity of a MARF is as follows:

* **Reads are _O(1)_** While reads may traverse multiple tries, they are always
  descending the radix trie, and resolving back pointers is constant time.
* **Inserts and updates are _O(1)._** Inserts have the same complexity
  as reads, though they require more work by constant factors (in
  particular, hash recalculations).
* **Creating a new block is _O(log B)_.** Inserting a block requires
  including the Merkle skip-list hash in the root node of the new
  ART. This is _log B_ work, where _B_ is chain length.
* **Creating a new fork is _O(log B)_.** Forks do not incur any overhead relative
  to appending a block to a prior chain-tip.
* **Generating a proof is _O(log B)_ for B blocks**.  This is the cost of
  reading a fixed number of nodes, combined with walking the Merkle skip-list.
* **Verifying a proof is _O(log B)_**.  This is the cost of verifying a fixed
  number of fixed-length segments, and verifying a fixed number of _O(log B)_
  shunt proof hashes.
* **Proof size is _O(log B)_**.  A proof has a fixed number of segment proofs,
  where each node has a constant size.  It has _O(log B)_ hashes across all of
  its shunt proofs.

## Consensus Details

The hash function used to generate a path from a key, as well as the hash
function used to generate a node hash, is SHA2-512/256.  This was chosen because
it is extremely fast on 64-bit architectures, and is immune to length extension
attacks.

The hash of an intermediate node is the hash over the following data:

* a 1-byte node ID,
* the sequence of child pointer data (dependent on the type of node),
* the 1-byte length of the path prefix this node contains,
* the 0-to-32-byte path prefix

A single child pointer contains:
* a 1-byte node ID,
* a 1-byte path character,
* the 32-byte block header hash of the pointed-to block

A `node4`, `node16`, `node48`, and `node256` each have an array of 4,
16, 48, and 256 child pointers each.

Children are listed in a `node4`, `node16`, and `node48`'s child pointer arrays in the
order in which they are inserted.  While searching for a child in a `node4` or
`node16` requires a linear scan of the child pointer array, searching a `node48` is done 
by looking up the child's index in its child pointer array using the
path character byte as an index into the `node48`'s 256-byte child pointer
index, and then using _that_ index to look up the child pointer.  Children are
inserted into the child pointer array of a `node256` by using the 1-byte
path character as the index.

The disk pointer stored in a child pointer, as well as the storage mechanism for
mapping hashes of values (leaves in the MARF) to the values themselves, are both
unspecified by the consensus rules.  Any mechanism or representation is
permitted.

# Related Work

This section will be expanded upon after this SIP is ratified.

[1] https://db.in.tum.de/~leis/papers/ART.pdf

# Backwards Compatibility

Not applicable

# Activation

At least 20 miners must register a name in the `.miner` namespace in Stacks 1.0.
Once the 20th miner has registered, the state of Stacks 1.0 will be snapshotted.
300 Bitcoin blocks later, the Stacks 2.0 blockchain will launch.  Stacks 2.0
implements the MARF internally.

# Reference Implementations

Implemented in Rust.  See https://github.com/blockstack/stacks-blockchain.


          
# Preamble

SIP Number: 024

Title: Emergency Fix to Data Validation and Serialization Behavior

Authors:
    Aaron Blankstein <aaron@hiro.so>,
    Brice Dobry <brice@hiro.so>,
    Jude Nelson <jude@stacks.org>,
    Pavitthra Pandurangan <pavitthra@stacks.org>,

Consideration: Technical, Governance

Type: Consensus

Status: Ratified

Created: 11 May 2023

License: BSD 2-Clause

Sign-off: Jason Schrader <jason@joinfreehold.com> (Governance CAB Chair), Brice Dobry <brice@hiro.so> (Technical CAB Chair), Jude Nelson <jude@stacks.org> (Steering Committee Chair)

Discussions-To: https://github.com/stacksgov/sips

# Abstract

On 8 May 2023, a critical Denial-of-Service vulnerability manifested
in the Stacks network. While the initial DoS threat was remedied
through a non-consensus breaking hotfix, the underlying bug that
triggered the vulnerability requires consensus changes to fix.
This underlying bug has existed in the Stacks blockchain implementation
since the launch of Stacks 2.0, and has the potential to impact the
functionality of contracts even if they do not currently rely on the
buggy behavior.

This SIP proposes a **consensus-breaking change** to be included in
the SIP-022 hardfork (Epoch 2.4) to remediate this negative impact.

# Introduction

Stacks 2.0 allows contracts to include tuple types with _extra_ fields
to be included in lists with tuples with fewer fields:

```clarity
(list (tuple (a 1)) (tuple (b 1) (a 1)))
```

The Clarity runtime will treat each item of this list as if it only
had the field `a`, which creates an issue for the database on reads and writes.
On database reads, the Clarity database checks if the found type
matches the expected type, and discovers a mismatch. This mismatch
led to a DoS on 8 May 2023, and was fixed by converting the node
crash into a transaction invalidation.

However, transaction invalidation is _not_ sufficient as a long-term
solution due to the following:

1. Miners must be able to charge for these kinds of failures
2. Contracts which do not directly rely on this behavior could still
   receive buggy values because of the behavior (which could lead to storage failures).

# Specification

The proposed changes to the Epoch 2.4 hard fork will do the following:

* Add a value sanitization routine which eliminates any of these extra
  fields from the in-memory representation of a Clarity value.
* Invoke the sanitization routine on contract-call arguments and
  return values.
* Invoke the sanitization routine on database reads.
* Invoke the sanitization routine during Clarity value constructors
  which relied on the buggy type check behavior.

This will preserve the existing type system behavior, but it will ensure
that values constructed this way _match_ the expected type.

# Related work
The Stacks network has precedent for fixing consensus bugs through hard forks, some being released on 
short timelines. 

Other blockchains have also detected and fixed consensus critical bugs quickly. A prominent example of 
this happened on Bitcoin, which had a bug that would allow the minting of an arbitrary amount of BTC 
above the 21 million cap. A patched version was quickly released, and the network upgraded in a 
matter of hours. 

# Backwards Compatibility 
Everyone who runs a 2.3 node will be able to run a Stacks 2.4 node 
off of their existing chainstate. There are no changes to the chainstate database schemas in this SIP.

Stacks 2.4 nodes will not interact with Stacks 2.3 nodes on the peer network (defined in SIP-022)
after the Bitcoin block activation height of `791551`. In addition, Stacks 2.4 nodes
will ignore block-commits from Stacks 2.3 nodes (as well as from nodes on prior versions). 
Similar changes were made for Stacks 2.05 and Stacks 2.1 to ensure that the new network
cleanly separates from stragglers still following the old rules.

# Activation 
The changes described in this SIP will ship in the same release as the changes described in SIP-022, which discusses
and proposes a fix to the proof of transfer protocol.

This release will ship 500 blocks prior to reward cycle 60, which is Bitcoin block height 791,551. 
This gives stackers ample time (~3 days) to stack through the new contract. 

The node software for Stacks 2.4 shall be merged to the `master` branch of the
reference implementation no later than four days prior to the activation
height.  This means that everyone shall have at least three days to upgrade
their Stacks 2.3 nodes to Stacks 2.4. This change does not require a sync from genesis.

# Reference Implementation
The reference implementation of this SIP can be found in the
`feat/epoch-2.4-sanitize` branch of the Stacks blockchain reference implementation.  It is available at
https://github.com/stacks-network/stacks-blockchain.

          
# Preamble

SIP Number: 009

Title: Standard Trait Definition for Non-Fungible Tokens

Author: Friedger Müffke (mail@friedger.de), Muneeb Majeed

Consideration: Technical

Type: Standard

Status: Ratified

Created: 10 December 2020

License: CC0-1.0

Sign-off: Jude Nelson <jude@stacks.org>, Technical Steering Committee Chair

# Abstract

Non-fungible tokens or NFTs are digital assets registered on blockchain with unique identifiers and properties that distinguish them from each other.
It should be possible to uniquely identify, own and transfer a non-fungible token. This SIP aims to provide a flexible and easy-to-implement standard that can be used by developers on the Stacks blockchain when creating their own NFTs. This standard only specifies a basic set of requirements, non-fungible tokens can have more features than what's specified in this standard.

# License and Copyright

This SIP is made available under the terms of the Creative Commons CC0 1.0 Universal license, available at https://creativecommons.org/publicdomain/zero/1.0/
This SIP’s copyright is held by the Stacks Open Internet Foundation.

# Introduction

Tokens are digital assets registered on blockchain through a smart contract. A non-fungible token (NFT) is a token that is globally unique and can be identified through its unique identifier.

In blockchains with smart contracts, including the Stacks blockchain, developers and users can use smart contracts to register and interact with non-fungible tokens.

The Stacks blockchain's programming language for developing smart contracts, Clarity, has built-in language primitives to define and use non-fungible tokens. Although those primitives exists, there is value in defining a common interface (known in Clarity as a "trait") that allows different smart contracts to interoperate with non-fungible token contracts in a reusable way. This SIP defines that trait.

Each NFT always belong to one smart contract. NFTs of a smart contract are enumerated starting at 1. The current last ID is provided by a smart contract function. The asset ID together with the contract ID defines a globally unique NFT.

# Specification

Every SIP-009 compliant smart contract in Stacks blockchain must implement the trait, `nft-trait`, defined in the [Trait](#trait) section and must meet the requirements for the following functions:

### Last token ID

`(get-last-token-id () (response uint uint))`

Takes no arguments and returns the identifier for the last NFT registered using the contract. The returned ID can be used as the upper limit when iterating through all NFTs.

This function must never return an error response. It can be defined as read-only, i.e. `define-read-only`.

### Token URI

`(get-token-uri (uint) (response (optional (string-ascii 256)) uint))`

Takes an NFT identifier and returns a response containing a valid URI which resolves to the NFT's metadata. The URI string must be wrapped in an `optional`. If the corresponding NFT doesn't exist or the contract doesn't maintain metadata, the response must be `(ok none)`. If a valid URI exists for the NFT, the response must be `(ok (some "<URI>"))`. The length of the returned URI is limited to 256 characters. The specification of the metadata should be covered in a separate SIP.

This function must never return an error response. It can be defined as read-only, i.e. `define-read-only`.

### Owner

`(get-owner (uint) (response (optional principal) uint))`

Takes an NFT identifier and returns a response containing the principal owning the NFT with the given identifier. The principal must be wrapped in an optional. If the corresponding NFT doesn't exist, the response must be `(ok none)`. The owner can be a contract principal.

If a call to function `get-owner` returns some principal `A`, then it must return the same value until the `transfer` function is called with principal `A` as the sender.

For any call to `get-owner` with an ID greater than the last token ID returned by the `get-last-token-id` function, the call must return a response `(ok none)`.

This function must never return an error response. It can be defined as read-only, i.e. `define-read-only`.

### Transfer

`(transfer (uint principal principal) (response bool uint))`

The function changes the ownership of the NFT for the given identifier from the sender principal to the recipient principal.

This function must be defined with define-public, as it alters state, and must be externally callable.

After a successful call to `transfer`, the function `get-owner` must return the recipient of the `transfer` call as the new owner.

For any call to `transfer` with an ID greater than the last token ID returned by the `get-last-token-id` function, the call must return an error response.

It is recommended to use error codes from standardized list of codes and implement the function for converting the error codes to messages function that are defined in a separate SIP.

## Trait

```
(define-trait nft-trait
  (
    ;; Last token ID, limited to uint range
    (get-last-token-id () (response uint uint))

    ;; URI for metadata associated with the token
    (get-token-uri (uint) (response (optional (string-ascii 256)) uint))

     ;; Owner of a given token identifier
    (get-owner (uint) (response (optional principal) uint))

    ;; Transfer from the sender to a new principal
    (transfer (uint principal principal) (response bool uint))
  )
)
```

## Use of native asset functions

Although it is not possible to mandate in a Clarity trait, contract implementers must define at least one built-in native non-fungible [asset class](https://app.sigle.io/friedger.id/FDwT_3yuMrHDQm-Ai1OVS) that are provided as Clarity primitives. This allows clients to use Post Conditions (explained below), and takes advantages of other benefits, like native support for these asset balances and transfers through `stacks-blockchain-api`. The reference implementations included in this SIP use the native asset primitives, and provide a good boilerplate for their usage.

The native asset functions include:

- `define-non-fungible-token`
- `nft-burn?`
- `nft-get-owner?`
- `nft-mint?`
- `nft-transfer?`

The following requirements for using native asset functions are defined:

### Transfer

If the `transfer` function is called from a client without a [post-condition](https://docs.blockstack.org/understand-stacks/transactions#post-conditions) in deny mode or without any NFT condition about a changed owner, then the function call must fail with `abort_by_post_condition`.

# Using NFTs in applications

Developers who wish to use a non-fungible token contract in an application should first be provided, or keep track of, various different non-fungible token implementations. When validating a non-fungible token contract, they should fetch the interface and/or source code for that contract. If the contract implements the trait, then the application can use this standard's contract interface for making transfers and getting other details defined in this standard.

All of the functions in this trait return the `response` type, which is a requirement of trait definitions in Clarity. However, some of these functions should be "fail-proof", in the sense that they should never return an error. These "fail-proof" functions are those that have been recommended as read-only. If a contract that implements this trait returns an error for these functions, it may be an indication of a non-compliant contract, and consumers of those contracts should proceed with caution.

## Use of Post-Conditions

The Stacks blockchain includes a feature known as "Post-Conditions" or "Constraints". By defining post-conditions, users can create transactions that include pre-defined guarantees about what might happen in that contract.

For example, when applications call the `transfer` function, they should _always_ use post conditions to specify that the new owner of the NFT is the recipient principal in the `transfer` function call.

# Related Work

NFTs are an established asset class on blockchains. Read for example [here](https://www.ledger.com/academy/what-are-nft).

## EIP 721

Ethereum has [EIP 721](https://eips.ethereum.org/EIPS/eip-721) that defined non-fungible tokens on the Ethereum blockchain. Notable differences are that the transfer function in EIP 721 uses a different ordering of the arguments ending with the token id. The transfer function in this SIP uses the token ID as the first argument which is in line with the other native functions in Clarity. Furthermore, this SIP only defines a function for getting the URI pointing to the metadata of an NFT. The specifications for schema and other properties of the token metadata should be defined in a separate SIP.

# Backwards Compatibility

Not applicable

# Activation

This SIP is activated if 5 contracts are deployed that use the same trait that follows this specification. This must happen before Bitcoin tip #700,000.

A trait that follows this specification is available on mainnet as [`SP2PABAF9FTAJYNFZH93XENAJ8FVY99RRM50D2JG9.nft-trait.nft-trait`](https://explorer.stacks.co/txid/0x80eb693e5e2a9928094792080b7f6d69d66ea9cc881bc465e8d9c5c621bd4d07?chain=mainnet).

# Reference Implementations

## Source code

### Friedger's clarity-smart-contracts

https://github.com/friedger/clarity-smart-contracts/blob/master/contracts/sips/nft-trait.clar

## Deployed Contracts

- mainnet: [SP2PABAF9FTAJYNFZH93XENAJ8FVY99RRM50D2JG9.nft-trait.nft-trait](https://explorer.stacks.co/txid/SP2PABAF9FTAJYNFZH93XENAJ8FVY99RRM50D2JG9.nft-trait?chain=mainnet)
- testnet: [ST1NXBK3K5YYMD6FD41MVNP3JS1GABZ8TRVX023PT.nft-trait.nft-trait](https://explorer.stacks.co/txid/ST1NXBK3K5YYMD6FD41MVNP3JS1GABZ8TRVX023PT.nft-trait?chain=testnet)

          
# Preamble

SIP Number: 010

Title: Standard Trait Definition for Fungible Tokens

Author: Hank Stoever <hstove@gmail.com>, Pascal Belloncle <psq@nanorails.com>

Consideration: Technical

Type: Standard

Status: Ratified

Created: 25 January 2021

License: CC0-1.0

Sign-off: Jude Nelson <jude@stacks.org>, Technical Steering Committee Chair

Layer: Traits

Discussions-To: https://github.com/stacksgov/sips

# Abstract

Fungible tokens are digital assets that can be sent, received, combined, and divided. Most forms of cryptocurrencies are fungible tokens. They have become a building block of almost all blockchains. This SIP aims to provide a flexible and easy-to-implement standard that can be used by developers on the Stacks blockchain when creating their own tokens.

# License and Copyright

This SIP is made available under the terms of the Creative Commons CC0 1.0 Universal license, available at https://creativecommons.org/publicdomain/zero/1.0/
This SIP’s copyright is held by the Stacks Open Internet Foundation.

# Introduction

Digital assets can have the property to be fungible. A _fungible_ token can be broken down into small units and added together. An owner of a fungible asset only needs to care about their balance, that is, the total amount of a particular fungible asset that they own. Most well-known currencies are fungible. For fungible tokens, there is no difference between any two different amounts of the fungible token.

For example, if a user owns 10 units of a fungible asset, they may send 2 units to a different user. At this point, their balance is 8 units. If they later receive more units, their total balance will be updated.

On blockchains, fungible tokens are a core component. Blockchains with smart contracts, including the Stacks blockchain, allow developers and users to create and interact with smart contracts that use fungible tokens.

The Stacks blockchain has a native fungible token: the Stacks token (STX). In addition to the native STX token, the Stacks blockchain's programming language for developing smart contracts, Clarity, has built-in language primitives to define and use fungible tokens. Although those primitives exists, there is value in defining a common interface (known in Clarity as a "trait") that allows different smart contracts to interoperate with fungible token contracts in a reusable way. This SIP defines that trait.

# Specification

The fungible token trait, `sip-010-trait`, has 7 functions:

## Trait functions

### Transfer

`(transfer ((amount uint) (sender principal) (recipient principal) (memo (optional (buff 34)))) (response bool uint))`

Transfer the fungible token from the sender of this transaction to the recipient. The `amount` is an unsigned integer. It is recommended that implementing contracts use the built-in `ft-transfer?` Clarity method. If the sender does not have enough tokens to complete the transaction, the transaction should abort and return an `(err uint)`.

This method must be defined with `define-public`, as it alters state, and should be externally callable.

Contract implementers should take note to perform authorization of the `transfer` method. For example, a fungible token contract that wants to make sure that only the transaction's sender is able to move the requested tokens might first check that the sender argument is equal to tx-sender.

When returning an error in this function, the error codes should follow the same patterns as the built-in `ft-transfer?` and `stx-transfer?` functions.

| error code | reason                                          |
| ---------- | ----------------------------------------------- |
| u1         | `sender` does not have enough balance           |
| u2         | `sender` and `recipient` are the same principal |
| u3         | `amount` is non-positive                        |
| u4         | `sender` is not the same as `tx-sender`         |

Contract implementers should take note that in Stacks 2.0, the memo field won't be included in the event emitted by successful `ft-transfer?` operations. As a consequence, the implementer has to make sure that the memo is emitted by adding a `print` statement if the `ft-transfer?` is successful and the memo is not `none`. The memo should be upwrapped and emitted after the `ft-transfer?` operation.

Example:

```
  ...
  (try! (ft-transfer? token amount sender recipient))
  (match memo to-print (print to-print) 0x)
  ...
```

### Name

`(get-name () (response (string-ascii 32) uint))`

Return a human-readable name for the contract, such as "CoolPoints", etc.

This method should be defined as read-only, i.e. `define-read-only`.

### Symbol

`(get-symbol () (response (string-ascii 32) uint))`

Return a symbol that allows for a shorter representation of a token. This is sometimes referred to as a "ticker". Examples: "STX", "COOL", etc. Typically, a token could be referred to as $SYMBOL when referencing it in writing.

This method should be defined as read-only, i.e. `define-read-only`.

### Decimals

`(get-decimals () (response uint uint))`

The number of decimal places in a token. All fungible token balances must be represented as integers, but providing the number of decimals provides for an abstraction of a token that humans are more familiar dealing with. For example, the US Dollar has 2 decimals, if the base unit is "cents", as is typically done in accounting. Stacks has 6 decimals, Bitcoin has 8 decimals, and so on.

As another example, if a token has 4 decimals, and the `get-balance` method a particular user returns `100345000`, wallets and exchanges would likely represent that value as `10034.5`.

This method should be defined as read-only, i.e. `define-read-only`.

### Balance of

`(get-balance (principal) (response uint uint))`

Return the balance of a particular principal (also known as "address" or "account"). Implementations should typically use the built-in Clarity method `ft-get-balance`.

This method should be defined as read-only, i.e. `define-read-only`.

### Total supply

`(get-total-supply () (response uint uint))`

Return the total supply of this token. Implementations should typically use the built-in Clarity method `ft-get-supply`.

This method should be defined as read-only, i.e. `define-read-only`.

### Token URI

`(get-token-uri () (response (optional (string-utf8 256)) uint))`

Returns an optional string that is a valid URI which resolves to this token's metadata. This allows your token to provide off-chain metadata about the contract, such as a description and an image icon.

The JSON schema for this metadata is as follows:

```json
{
  "title": "Asset Metadata",
  "type": "object",
  "properties": {
    "name": {
      "type": "string",
      "description": "Identifies the asset to which this token represents"
    },
    "description": {
      "type": "string",
      "description": "Describes the asset to which this token represents"
    },
    "image": {
      "type": "string",
      "description": "A URI pointing to a resource with mime type image/* representing the asset to which this token represents. Consider making any images at a width between 320 and 1080 pixels and aspect ratio between 1.91:1 and 4:5 inclusive."
    }
  }
}
```

Clients that fetch this data should prefer any on-chain data, such as the name of the token, over metadata provided in `get-token-uri`.

## Trait implementation

An implementation of the proposed trait is provided below.

```clarity
(define-trait sip-010-trait
  (
    ;; Transfer from the caller to a new principal
    (transfer (uint principal principal (optional (buff 34))) (response bool uint))

    ;; the human readable name of the token
    (get-name () (response (string-ascii 32) uint))

    ;; the ticker symbol, or empty if none
    (get-symbol () (response (string-ascii 32) uint))

    ;; the number of decimals used, e.g. 6 would mean 1_000_000 represents 1 token
    (get-decimals () (response uint uint))

    ;; the balance of the passed principal
    (get-balance (principal) (response uint uint))

    ;; the current total supply (which does not need to be a constant)
    (get-total-supply () (response uint uint))

    ;; an optional URI that represents metadata of this token
    (get-token-uri () (response (optional (string-utf8 256)) uint))
  )
)
```

## Implementing in wallets and other applications

Developers who wish to interact with a fungible token contract should first be provided, or keep track of, various different fungible token implementations. When validating a fungible token contract, they should fetch the interface and/or source code for that contract. If the contract implements the trait, then the wallet can use this standard's contract interface for making transfers and getting balances.

Downstream consumers of contracts that implement this trait should be aware that the `get-name` and `get-symbol` function are not guaranteed to be globally unique. Because of this, consumers should be advised that `get-name` and `get-token` are only hints to provide a more human-readable experience. Care should always be taken to verify that a contract's identifier matches that of the token a client is intending to interact with.

All of the functions in this trait return the `response` type, which is a requirement of trait definitions in Clarity. However, some of these functions should be "fail-proof", in the sense that they should never return an error. These "fail-proof" functions are those that have been recommended as read-only. If a contract that implements this trait returns an error for these functions, it may be an indication of a faulty contract, and consumers of those contracts should proceed with caution.

## Use of native asset functions

Although it is not possible to mandate in a Clarity trait, contract implementers should always use the built-in native assets that are provided as Clarity primitives. This allows clients to use Post Conditions (explained below), and takes advantages of other benefits, like native support for these asset balances and transfers through `stacks-blockchain-api`. The reference implementations included in this SIP use the native asset primitives, and provide a good boilerplate for their usage.

The native asset primitives include:

- `define-fungible-token`
- `ft-burn?`
- `ft-get-balance`
- `ft-get-supply`
- `ft-mint?`
- `ft-transfer?`

## Use of post conditions

In addition to built-in methods for fungible token contracts, the Stacks blockchain includes a feature known as Post Conditions. By defining post conditions, users can create transactions that include pre-defined guarantees about what might happen in that contract.

One such post condition could be "I will transfer exactly 100 of X token", where "X token" is referenced as a specific contract's fungible token. When wallets and applications implement the `transfer` method, they should _always_ use post conditions to specify that the user will transfer exactly the amount of tokens that they specify in the `amount` argument of the `transfer` function. Only in very specific circumstances should such a post condition not be included.

# Related work

## Ethereum ERC20

[Ethereum ERC20 standard](https://ethereum.org/en/developers/docs/standards/tokens/erc-20/)

Perhaps the oldest, and most well known, standard for fungible tokens is Ethereum's [ERC20](https://ethereum.org/en/developers/docs/standards/tokens/erc-20/) standard. It has become one of the strongest building blocks for the Ethereum ecosystem. When all fungible tokens follow the same standard, any wallet or application developer can interact with it without having to create custom logic for handling each individual token.

Fungible tokens have become so popular that the Clarity smart contracting language has support for basic fungible token operations built-in. In fact, as can be seen in this proposal's reference implementation, very little code is required to implement a fungible token. The important part of this standard is defining a Clarity trait that all fungible tokens can implement. Even though Clarity has fungible token operations built-in, it is important for each contract to define the same methods so that their contracts are easy to integrate.

# Backwards Compatibility

Not applicable

# Activation

This trait has been deployed to mainnet: [SP3FBR2AGK5H9QBDH3EEN6DF8EK8JY7RX8QJ5SVTE.sip-010-trait-ft-standard](https://explorer.stacks.co/txid/0x99e01721e57adc2c24f7d371b9d302d581dba1d27250c7e25ea5f241af14c387?chain=mainnet)

This trait will be considered activated when this trait is deployed to mainnet, and 3 different implementations of the trait have been deployed to mainnet, no later than Bitcoin block 700000.

# Reference Implementations

An example implementation has been submitted with this proposal, along with a Javascript client and tests. https://github.com/hstove/stacks-fungible-token

Other examples of Clarity contracts that implement fungible tokens, although not exactly according to this specification:

- [@psq's trait and implementation](https://github.com/psq/flexr/blob/master/contracts/src20-trait.clar)
- [@friedger's fungible token implementation](https://github.com/friedger/clarity-smart-contracts/blob/master/contracts/tokens/fungible-token.clar)

## Deployed Contracts

- mainnet: [SP3FBR2AGK5H9QBDH3EEN6DF8EK8JY7RX8QJ5SVTE.sip-010-trait-ft-standard.sip-010-trait](https://explorer.stacks.co/txid/SP3FBR2AGK5H9QBDH3EEN6DF8EK8JY7RX8QJ5SVTE.sip-010-trait-ft-standard?chain=mainnet)
- testnet: [ST1NXBK3K5YYMD6FD41MVNP3JS1GABZ8TRVX023PT.sip-010-trait-ft-standard.sip-010-trait](https://explorer.stacks.co/txid/ST1NXBK3K5YYMD6FD41MVNP3JS1GABZ8TRVX023PT.sip-010-trait-ft-standard?chain=testnet)

          
# Preamble

SIP Number: 023

Title: Emergency Fix to Trait Invocation Behavior

Authors:
    Aaron Blankstein <aaron@hiro.so>,
    Brice Dobry <brice@hiro.so>,
    Jude Nelson <jude@stacks.org>,

Consideration: Technical, Governance

Type: Consensus

Status: Ratified

Created: 1 May 2023

License: BSD 2-Clause

Sign-off: Rafael Cárdenas <rafael@hiro.so> (SIP Editor), Jesse Wiley <jesse@stacks.org> (Acting Technical CAB Chair), Jason Schrader <jason@joinfreehold.com> (Governance CAB Chair), Jude Nelson <jude@stacks.org> (Steering Committee Chair)

Discussions-To: https://github.com/stacksgov/sips

# Abstract

On 1 May 2023, it was discovered that smart contracts deployed prior to Stacks 2.1
that exposed public methods with
trait arguments could not be invoked with previously working trait-implementing
contract arguments.

This bug was caused by the activation of Stacks Epoch 2.2 (https://github.com/stacksgov/sips/blob/main/sips/sip-022/sip-022-emergency-pox-fix.md).

This SIP proposes an **immediate consensus-breaking change** to
introduce a new Stacks epoch 2.3 that corrects this regression.

**This SIP proposes a Bitcoin activation height of 788,240**

# Introduction

Clarity 2, introduced in Stacks 2.1, includes a new type checker and type system which
impacts trait invocations. In order for existing contracts to remain
compatible, their types must be _canonicalized_. In the context of traits,
the type canonicalization rules implement the new trait semantics introduced in
[SIP-015](./sips/sip-015/sip-015-network-upgrade.md).

## Epoch 2.2 Bug Behavior

The type canonicalization method performed an exact check for the current epoch:

```rust
    pub fn canonicalize(&self, epoch: &StacksEpochId) -> TypeSignature {
        match epoch {
            StacksEpochId::Epoch21 => self.canonicalize_v2_1(),
            _ => self.clone(),
        }
    }
```

Therefore, a pre-2.1 function with trait arguments that is invoked in Stacks 2.2
will fail to canonicalize its trait arguments, and abort with a
runtime analysis error. Specifically:

* If a miner includes a contract call transaction with trait arguments in a block, the transaction will abort with a runtime error.

* If a user submits a contract call transaction with trait arguments to the
  mempool, it will be rejected.

* A read-only contract-call with trait arguments will fail with a runtime
  analysis error.

# Specification

This hard fork will do the following:

* In epoch 2.2, the current buggy behavior will be preserved.  All
  contract-calls with trait arguments must fail with a runtime analysis error.

* In epoch 2.3, the desired behavior will be restored.  The trait semantics
  described in SIP-015 will be restored, and trait arguments in
  contract-calls will be treated as they were in Stacks 2.1.

* Set the minimum required block-commit memo bits to `0x08`.  All block-commits
  after the Bitcoin block activation height must have a memo value of at least
`0x08`.  This ensures that miners that do not upgrade from Stacks 2.2 will not
be able to mine in Stacks 2.3.

* Set the mainnet peer network version bits to `0x18000008`.  This ensures that follower
  nodes that do not upgrade to Stacks 2.3 will not be able to talk to Stacks
2.3 nodes.

* Set the testnet peer network version bits to `0xfacade08`.  This ensures that
  testnet follower nodes that do not upgrade to Stacks 2.3 will not be able to
talk to Stacks 2.3 nodes.

The reference implementation will update the `canonicalize()` method to match on all epochs, setting
the epoch 2.3 behavior to `self.canonicalize_v2_1()`, and the epoch 2.2 behavior to `self.clone()`.
This will preserve the buggy 2.2 behavior during the 2.2 epoch (so that the
hard fork does not require rollback), but fix the behavior after activation
of the 2.3 epoch.

# Related Work

Several potential workarounds were explored first to try to solve this issue without a hard-fork. 
Unfortunately, attempts to wrap pre-2.1 contracts with 2.2 contracts can avoid the mempool rejection, 
but still hit the same error in the form of a runtime type-checker error.
Upon further inspection into the code paths, a hard-fork option was determined to be the only viable option in this case.

Consensus bugs requiring immediate attention such as this
have been detected and fixed in other blockchains.  In the
absence of a means of gathering user comments on proposed fixes, the task of
activating these bugfixes has fallen to miners, exchanges, and node runners.  As
long as sufficiently many participating entities upgrade, then a chain split is
avoided and the fixed blockchain survives.  A prominent example was Bitcoin
[CVE-2010-5139](https://www.cvedetails.com/cve/CVE-2010-5139/), in which a
specially-crafted Bitcoin transaction could mint arbitrarily many BTC well above
the 21 million cap.  The [developer
response](https://bitcointalk.org/index.php?topic=823.0) was to quickly release
a patched version of Bitcoin and rally enough miners and users to upgrade.  In a
matter of hours, the canonical Bitcoin chain ceased to include any transactions
that minted too much BTC.

# Backwards Compatibility

There are no changes to the chainstate database schemas in this SIP.  Everyone
who runs a Stacks 2.2 node today will be able to run a Stacks 2.3 node off of
their existing chainstates before the activation height.

Stacks 2.3 nodes will not interact with Stacks 2.2 nodes on the peer
network after the Bitcoin block activation height passes.  In
addition, Stacks 2.3 nodes will ignore block-commits from Stacks 2.2
nodes.  Similar changes were made for Stacks 2.05, Stacks 2.1, and
Stacks 2.2 to ensure that the new network cleanly separates from
stragglers still following the old rules.

# Activation

This SIP shall be considered Activated if the Stacks 2.3 network is live at the
Bitcoin block activation height.

The node software for Stacks 2.3 shall be merged to the `master` branch of the
reference implementation no later than two days prior to the activation
height. This means that everyone shall have at least two days to upgrade
their Stacks 2.2 nodes to Stacks 2.3.

# Reference Implementation

The reference implementation of this SIP can be found in the
`feat/2.3-traits-only-fix` branch of
the Stacks blockchain reference implementation.  It is available at
https://github.com/stacks-network/stacks-blockchain.

          
# Preamble

SIP Number: 005

Title: Blocks, Transactions, and Accounts

Author: Jude Nelson <jude@stacks.org>, Aaron Blankstein <aaron@blockstack.com>

Consideration: Technical

Type: Consensus

Status: Ratified

Created: 23 July 2019

License: BSD 2-Clause

Sign-off: Jude Nelson <jude@stacks.org>, Technical Steering Committee Chair

Discussions-To: https://github.com/stacksgov/sips

# Abstract

This SIP describes the structure, validation, and lifecycle for transactions and blocks in
the Stacks blockchain, and describes how each peer maintains a materialized view
of the effects of processing all state-transitions encoded in the blockchain's sequence of
transactions.  It presents the account model for the Stacks blockchain, and
describes how accounts authorize and pay for processing transactions on the
network.

# Introduction

The Stacks blockchain is a replicated state machine.
A _transaction_ encodes a single state-transition on the
Stacks blockchain.  The Stacks blockchain's state evolves by materializing the
effects of a sequence of transactions -- i.e., by applying each transaction's encoded
state-transitions to the blockchain's state.

Transactions in the Stacks blockchain encode various kinds of state-transitions,
the principal ones being:

* To instantiate a smart contract (see SIP 002)
* To invoke a public smart contract function
* To transfer STX tokens between accounts
* To punish leaders who fork their microblock streams (see SIP 001)
* To allow leaders to perform limited on-chain signaling

Processing transactions is not free.  Each step in the process of validating and
executing the transaction incurs a non-zero computational cost.  To incentivize 
peers and leaders to execute transactions, the transaction's computational costs
are paid for by an _account_.

An _account_ is the logical entity that executes and/or pays for transactions.  A transaction's
execution is governed by three accounts, which may or may not be distinct:

* The **originating account** is the account that creates and sends the
  transaction.  This is always an account owned by a user.  Each transaction is
_authorized_ by its originating account.

* The **paying account** is the account that is billed by the leader for the cost
  of validating and executing the transaction.  This is also always an account
owned by a user.  If not identified in the transaction, the paying account and
the originating account are the same account.

* The **sending account** is the account that identifies _who_ is
  _currently_ executing the transaction. The sending account can
  change during the course of transaction execution via the Clarity
  function `as-contract`, which executes the provided code block as
  the _current contract's_ account. Each transaction's initial sending
  account is its originating account -- i.e., the account that
  authorizes the transaction.  Smart contracts determine the sending
  account's principal using the `tx-sender` built-in function.

This document frames accounts in the Stacks blockchain as the unit of agency for 
processing transactions.  The tasks
that a transaction carries out are used to inform the decisions on what
data goes into the transaction, as well as the data that goes into a block.
As such, understanding blocks and transactions in the Stacks blockchain first
requires understanding accounts.

# Specification
## Accounts

Transactions in the Stacks blockchain originate from, are paid for by, and
execute under the authority of accounts.  An account is fully
described by the following information:

* **Address**.  This is a versioned cryptographic hash that uniquely identifies the
  account.  The type of account (described below) determines what information is
hashed to derive the address.  The address itself contains two or three fields:
   * A 1-byte **version**, which indicates whether or not the address
     corresponds to a mainnet or testnet account and what kind of hash algorithm
to use to generate its hash.
   * A 20-byte **public key hash**, which is calculated using the address
     version and the account's owning public key(s).
   * A variable-length **name**.  This is only used in contract accounts, and it
     identifies the code body that belongs to this account.  The name
     may be up to 128 bytes.  Accounts belonging to users do not have this field.

* **Nonce**.  This is a Lamport clock, used for ordering the transactions
  originating from and paying from an account.  The nonce ensures that a transaction
is processed at most once.  The nonce counts the number of times
an account's owner(s) have authorized a transaction (see below).
The first transaction from an account will have a nonce value equal to 0,
the second will have a nonce value equal to 1, and so
on.  A valid transaction authorization from this account's owner(s) must include the _next_ nonce
value of the account; when the transaction is accepted by the peer network, the
nonce is incremented in the materialized view of this account.

* **Assets**.  This is a mapping between all Stacks asset types and the
  quantities of each type owned by the account.  This includes the STX token, as
well as any other on-chain assets declared by a Clarity smart contract (i.e.,
fungible and non-fungible tokens).

All accounts for all possible addresses are said to exist, but nearly all of
them are "empty" -- they have a nonce value of 0, and their asset mappings
contain no entries.  The state for an account is lazily materialized once
the Stacks peer network processes a transaction that _funds_ it.
That is, the account state is materialized only once a transaction's state-transition inserts
an entry into an account's assets mapping for some (possibly zero) quantity of some asset.
Even if the account depletes all asset holdings, it remains materialized.
Materialized accounts are distinguished from empty accounts in that the former
are all represented in a leader's commitment to its materialized view of the blockchain state
(described below).

### Account Types

The Stacks blockchain supports two kinds of accounts:

* **Standard accounts**.  These are accounts owned by one or more private keys.
  Only standard accounts can originate and pay for transactions.  A transaction originating
from a standard account is only valid if a threshold of its private keys sign
it.  The address for a standard account is the hash of this threshold value and
all allowed public keys.  Due to the need for backwards compatibility with
Stacks v1, there are four ways to hash an account's public keys and threshold,
and they are identical to Bitcoin's pay-to-public-key-hash, multisig
pay-to-script-hash, pay-to-witness-public-key-hash, and multisig
pay-to-witness-script-hash hashing algorithms (see appendix).

* **Contract accounts**.  These are accounts that are materialized whenever a
smart contract is instantiated.  Each contract is paired with exactly one contract account.
It cannot authorize or pay for transactions, but may serve as the sending account
of a currently-executing transaction, via Clarity's `as-contract` function.  A
contract's address's public key hash matches the public key hash of the standard
account that created it, and each contract account's address contains a name for
its code body.  The name is unique within the set of code bodies instantiated by
the standard account.

Both kinds of accounts may own on-chain assets.  However, the nonce of a
contract account must always be 0, since it cannot be used to originate or pay
for a transaction.

### Account Assets

As described in SIP 002, the Stacks blockchain supports on-chain assets as a
first-class data type -- in particular, _fungible_ and _non-fungible_ assets are
supported.  All assets (besides STX) are scoped to a particular contract, since 
they are created by contracts.  Within a contract, asset types are unique.
Therefore, all asset types are globally-addressable via their identifier in the
contract and their fully-qualified contract names.

Regardless of where asset types are declared, a particular instance of an asset 
belongs to exactly one account at all times.  Once a contract declares an asset type,
instances of that asset can be sent to and owned by other accounts.

## Transactions

Transactions are the fundamental unit of execution in the Stacks blockchain.
Each transaction is originated from a standard account, and is retained in
the Stacks blockchain history for eternity.  Transactions are atomic -- they
either execute completely with respect to other transactions, or not at all.
Moreover, transactions are processed in the same total order by all Stacks
nodes.

At its core, a transaction is an authorization statement (see below),
a snippet of executable Clarity code, and a list of
_post-conditions_ that must be true before the transaction is accepted.  The
transaction body supplies the Stacks blockchain this code, as well as all of
the necessary metadata to describe how the transaction should be executed.
The various types of Stacks transactions encode different metadata, and
thus have different validation rules.

All transactions are originated from a set of private keys that own a standard
account, even if it is not yet materialized.  The owner(s) of these
private keys sign the transaction, attach a _transaction fee_ to it, and
relay it to the Stacks peer network.  If the transaction is well-formed,
then it will be propagated to all reachable Stacks peers.
Eventually, assuming the transaction remains resident in the peers' memories
for long enough, a Stacks leader will select the transaction for inclusion
in the longest fork's next block.  Once this happens, the state-transitions
encoded by the transaction are materialized in the blockchain state replicas in all
peers.

### Transaction Authorizations

The Stacks blockchain supports two ways to authorize a transaction:  a
_standard_ authorization, and a _sponsored_ authorization.  The distinction is
in whether or not the originating account is also the paying account.  In a
transaction with a standard authorization, the origin and paying accounts are
the same.  In a transaction with a sponsored authorization, the origin and
paying accounts are distinct, and both accounts must sign the transaction for it
to be valid (first the origin, then the spender).

The intended use-case for sponsored authorizations is to enable developers
and/or infrastructure operators to pay for users to call into their
smart contracts, even if users do not have the STX to do so.  The signing flow
for sponsored transactions would be to have the user first sign the transaction
with their origin account with the intent of it being sponsored (i.e., the user
must explicitly allow a sponsor to sign), and then have the sponsor sign with their paying
account to pay for the user's transaction fee.

### Transaction Payloads

The key difference between Stacks transaction payloads is what functionality is
available to them from the Clarity VM (and by extension, what side-effects are
materializable).  The reasons for distinguishing between these types of
transactions are to make static analysis cheaper for certain common use-cases,
and to provide greater security for the user(s) that own the account.

#### Type-0: Transferring an Asset

A type-0 transaction may only transfer a single asset from one account to
another.  It may not directly execute Clarity code.  A type-0
transaction can only send STX.  It cannot have post-conditions
(see below).

#### Type-1: Instantiating a Smart Contract

A type-1 transaction has unrestricted access to the Clarity VM,
and when successfully evaluated, will materialize a new smart contract
account.  Type-1 transactions are meant to instantiate smart contracts, and to
call into multiple smart contract functions and/or access their state
atomically.

#### Type-2: Calling an Existing Smart Contract

A type-2 transaction has restricted access to the Clarity VM.  A
type-2 transaction may only contain a single public function call (via
`contract-call?`), and may only supply Clarity `Value`s as its
arguments. These transactions do _not_ materialize a contract account.

The intended use-case for a type-2 transaction is to invoke an existing public
smart contract function.  Because they have such restricted access to the
Clarity VM, they are much cheaper to execute compared to a type-1 transaction.

#### Type-3: Punishing an Equivocating Stacks Leader

A type-3 transaction encodes two well-formed, signed, but conflicting
microblock headers.  That is, the headers are different, but have the same
sequence number and/or parent block hash.  If mined before the block reward
matures, this transaction will cause the offending leader to lose their block reward,
and cause the sender of this transaction to receive a fraction of the lost
coinbase as a reward for catching the bad behavior.
This transaction has no access to the Clarity VM.

#### Type-4: Coinbase

A type-4 transaction encodes a 32-byte scratch space for a block leader's own
use, such as signaling for network upgrades or announcing a digest of a set of
available peers.  This transaction must be the first transaction in an anchored
block in order for the block to be considered well-formed.  This transaction 
has no access to the Clarity VM.  Only one coinbase transaction may be mined per
epoch.

### Transaction Post-Conditions

A key use-case of smart contracts is to allow programmatic control over the
assets in one or more accounts.  However, where there is programmatic control,
there are bound to be bugs.  In the world of smart contract programming, bugs
(intentional or not) can have severe consequences to the user's well-being.
In particular, bugs can destroy a user's assets and cause them to lose wealth.
Transaction post-conditions are a feature meant to limit the damage a bug can
do in terms of destroying a user's assets.

Post-conditions are intended to be used to force a transaction to abort if the
transaction would cause a principal to send an asset in a way that is not to
the user's liking.  For example, a user may append a post-condition saying that
upon successful execution, their account's STX balance should have decreased by no more
than 1 STX (excluding the fee).  If this is not the case, then the transaction would abort
and the account would only pay the transaction fee of processing it.
As another example, a user purchasing a BNS name may append a post-condition saying that upon
successful execution, the seller will have sent the BNS name.  If it
did not, then the transaction aborts, the account is not billed for the name,
and the selling account receives no payment.

Each transaction includes a field that describes zero or more post-conditions
that must all be true when the transaction finishes running.  Each
post-condition is a quad that encodes the following information:

* The **principal** that sent the asset.  It can be a standard or contract address.
* The **asset name**, i.e., the name of one of the assets in the originating
  account's asset map.
* The **comparator**, described below.
* The **literal**, an integer or boolean value used to compare instances of the
  asset against via the condition.  The type of literal depends on both the
  type of asset (fungible or non-fungible) and the comparator.

The Stacks blockchain supports the following two types of comparators:

* **Fungible asset changes** -- that is, a question of _how much_ of a
  fungible asset was sent by a given account when the transaction ran.
  The post-condition can assert that the quantity of tokens increased,
  decreased, or stayed the same.
* **Non-fungible asset state** -- that is, a question of _whether or not_ an
  account sent a non-fungible asset when the transaction ran.

In addition, the Stacks blockchain supports an "allow" or "deny" mode for
evaluating post-conditions:  in "allow" mode, other asset transfers not covered
by the post-conditions are permitted, but in "deny" mode, no other asset
transfers are permitted besides those named in the post-conditions.

Post-conditions are meant to be added by the user (or by the user's wallet
software) at the moment they sign with their origin account.  Because the
user defines the post-conditions, the user has the power to protect themselves
from buggy or malicious smart contracts proactively, so even undiscovered bugs
cannot steal or destroy their assets if they are guarded with post-conditions.
Well-designed wallets would provide an intuitive user interface for
encoding post-conditions, as well as provide a set of recommended mitigations
based on whether or not the transaction would interact with a known-buggy smart contract.

Post-conditions may be used in conjunction with only contract-calls and smart contract 
instantiation transaction payloads.

#### Post-Condition Limitations

Post-conditions do not consider who _currently owns_ an asset when the
transaction finishes, nor do they consider the sequence of owners an asset
had during its execution.  It only encodes who _sent_ an asset, and how much.
This information is much cheaper to track, and requires no
I/O to process (processing time is _O(n)_ in the number of post-conditions).
Users who want richer post-conditions are encouraged to deploy their own
proxy contracts for making such queries.

### Transaction Encoding

A transaction includes the following information.  Multiple-byte fields are
encoded as big-endian.

* A 1-byte **version number**, identifying whether or not the transaction is
  meant as a mainnet or testnet transaction.
* A 4-byte **chain ID**, identifying which Stacks chain this transaction is
  destined for.
* A **transaction authorization** structure, described below, which encodes the
  following information (details are given in a later section):
   * The address of the origin account.
   * The signature(s) and signature threshold for the origin account.
   * The address of the sponsor account, if this is a sponsored transaction.
   * The signature(s) and signature threshold for the sponsor account, if given.
   * The **fee** to pay, denominated in microSTX.
* A 1-byte **anchor mode**, identifying how the transaction should be mined.  It
  takes one of the following values:
   * `0x01`:  The transaction MUST be included in an anchored block
   * `0x02`:  The transaction MUST be included in a microblock
   * `0x03`:  The leader can choose where to include the transaction.
* A 1-byte **post-condition mode**, identifying whether or not post-conditions
  must fully cover all transferred assets.  It can take the following values:
   * `0x01`:  This transaction may affect other assets not listed in the
     post-conditions.
   * `0x02`:  This transaction may NOT affect other assets besides those listed
     in the post-conditions.
* A length-prefixed list of **post-conditions**, describing properties that must be true of the
  originating account's assets once the transaction finishes executing.  It is encoded as follows:
   * A 4-byte length, indicating the number of post-conditions.
   * A list of zero or more post-conditions, whose encoding is described below.
* The **transaction payload**, described below.

#### Version Number

The version number identifies whether or not the transaction is a mainnet or
testnet transaction.  A mainnet transaction MUST have its highest bit cleared, and a
testnet transaction MUST have the highest bit set (i.e., `version & 0x80` must be
non-zero for testnet, and zero for mainnet).  The lower 7 bits are ignored for
now.

#### Chain ID

The chain ID identifies which instance of the Stacks blockchain this transaction
is destined for.  Because the main Stacks blockchain and Stacks app chains
(described in a future SIP) share the same transaction wire format, this field
is used to distinguish between each chain's transactions.  Transactions for the
main Stacks blockchain MUST have a chain ID of `0x00000000`.

#### Transaction Authorization

Each transaction contains a transaction authorization structure, which is used
by the Stacks peer to identify the originating account and sponsored account, to
determine the fee that the spending account will pay, and to
and determine whether or not it is allowed to carry out the encoded state-transition.
It is encoded as follows:

* A 1-byte **authorization type** field that indicates whether or not the
  transaction has a standard or sponsored authorization.
   * For standard authorizations, this value MUST be `0x04`.
   * For sponsored authorizations, this value MUST be `0x05`.
* One or two **spending conditions**, whose encoding is described below.  If the
  transaction's authorization type byte indicates that it is a standard
authorization, then there is one spending condition.  If it is a sponsored
authorization, then there are two spending conditions that follow.

_Spending conditions_ are encoded as follows:

* A 1-byte **hash mode** field that indicates how the origin account authorization's public
  keys and signatures should be used to calculate the account address.  Four
modes are supported, in the service of emulating the four hash modes supported
in Stacks v1 (which uses Bitcoin hashing routines):
   * `0x00`: A single public key is used.  Hash it like a Bitcoin P2PKH output.
   * `0x01`: One or more public keys are used.  Hash them as a Bitcoin multisig P2SH redeem script.
   * `0x02`: A single public key is used.  Hash it like a Bitcoin P2WPKH-P2SH
     output.
   * `0x03`: One or more public keys are used.  Hash them as a Bitcoin
     P2WSH-P2SH output.
* A 20-byte **public key hash**, which is derived from the public key(s) according to the
  hashing routine identified by the hash mode.  The hash mode and public key
hash uniquely identify the origin account, with the hash mode being used to
derive the appropriate account version number.
* An 8-byte **nonce**.
* An 8-byte **fee**.
* Either a **single-signature spending condition** or a **multisig spending
  condition**, described below.  If the hash mode byte is either `0x00` or
`0x02`, then a single-signature spending condition follows.  Otherwise, a
multisig spending condition follows.

A _single-signature spending condition_ is encoded as follows:

* A 1-byte **public key encoding** field to indicate whether or not the
  public key should be compressed before hashing.  It will be:
   * `0x00` for compressed
   * `0x01` for uncompressed
* A 65-byte **recoverable ECDSA signature**, which contains a signature
and metadata for a secp256k1 signature.

A _multisig spending condition_ is encoded as follows:

* A length-prefixed array of **spending authorization fields**, described
  below.
* A 2-byte **signature count** indicating the number of signatures that
  are required for the authorization to be valid.

A _spending authorization field_ is encoded as follows:

* A 1-byte **field ID**, which can be `0x00`, `0x01`, `0x02`, or
  `0x03`.
* The **spending field body**, which will be the following,
  depending on the field ID:
   * `0x00` or `0x01`:  The next 33 bytes are a compressed secp256k1 public key.
     If the field ID is `0x00`, the key will be loaded as a compressed
     secp256k1 public key.  If it is `0x01`, then the key will be loaded as
     an uncompressed secp256k1 public key.
   * `0x02` or `0x03`:  The next 65 bytes are a recoverable secp256k1 ECDSA
     signature.  If the field ID is `0x02`, then the recovered public
     key will be loaded as a compressed public key.  If it is `0x03`,
     then the recovered public key will be loaded as an uncompressed
     public key.

A _compressed secp256k1 public key_ has the following encoding:

* A 1-byte sign byte, which is either `0x02` for even values of the curve's `y`
  coordinate, or `0x03` for odd values.
* A 32-byte `x` curve coordinate.

An _uncompressed secp256k1 public key_ has the following encoding:

* A 1-byte constant `0x04`
* A 32-byte `x` coordinate
* A 32-byte `y` coordinate

A _recoverable ECDSA secp256k1 signature_ has the following encoding:

* A 1-byte **recovery ID**, which can have the value `0x00`, `0x01`, `0x02`, or
  `0x03`.
* A 32-byte `r` curve coordinate
* A 32-byte `s` curve coordinate.  Of the two possible `s` values that may be
  calculated from an ECDSA signature on secp256k1, the lower `s` value MUST be
used.

The number of required signatures and the list of public keys in a spending
condition structure uniquely identifies a standard account.
and can be used to generate its address per the following rules:

| Hash mode | Spending Condition | Mainnet version | Hash algorithm |
| --------- | ------------------ | --------------- | -------------- |
| `0x00` | Single-signature | 22 | Bitcoin P2PKH |
| `0x01` | Multi-signature | 20 | Bitcoin redeem script P2SH |
| `0x02` | Single-signature | 20 | Bitcoin P2WPK-P2SH |
| `0x03` | Multi-signature | 20 | Bitcoin P2WSH-P2SH |

The corresponding testnet address versions are:
*  For 22 (`P` in the c32 alphabet), use 26 (`T` in the c32 alphabet)
*  For 20 (`M` in the c32 alphabet), use 21 (`N` in the c32 alphabet).

The hash algorithms are described below briefly, and mirror hash algorithms used
today in Bitcoin.  This is necessary for backwards compatibility with Stacks v1
accounts, which rely on Bitcoin's scripting language for authorizations.

_Hash160_:  Takes the SHA256 hash of its input, and then takes the RIPEMD160
hash of the 32-byte

_Bitcoin P2PKH_:  This algorithm takes the ECDSA recoverable signature and
public key encoding byte from the single-signature spending condition, converts them to 
a public key, and then calculates the Hash160 of the key's byte representation
(i.e., by serializing the key as a compressed or uncompressed secp256k1 public
key).

_Bitcoin redeem script P2SH_:  This algorithm converts a multisig spending
condition's public keys and recoverable signatures
into a Bitcoin BIP16 P2SH redeem script, and calculates the Hash160
over the redeem script's bytes (as is done in BIP16).  It converts the given ECDSA
recoverable signatures and public key encoding byte values into their respective
(un)compressed secp256k1 public keys to do so.

_Bitcoin P2WPKH-P2SH_:  This algorithm takes the ECDSA recoverable signature and
public key encoding byte from the single-signature spending condition, converts
them to a public key, and generates a P2WPKH witness program, P2SH redeem
script, and finally the Hash160 of the redeem script to get the address's public
key hash.

_Bitcoin P2WSH-P2SH_:  This algorithm takes the ECDSA recoverable signatures and
public key encoding bytes, as well as any given public keys, and converts them
into a multisig P2WSH witness program.  It then generates a P2SH redeem script
from the witness program, and obtains the address's public key hash from the
Hash160 of the redeem script.

The resulting public key hash must match the public key hash given in the
transaction authorization structure.  This is only possible if the ECDSA
recoverable signatures recover to the correct public keys, which in turn is only
possible if the corresponding private key(s) signed this transaction.

#### Transaction Post-Conditions

The list of post-conditions is encoded as follows:
* A 4-byte length prefix
* Zero or more post-conditions.

A post-condition can take one of the following forms:
* A 1-byte **post-condition type ID**
* A variable-length **post-condition**

The _post-condition type ID_ can have the following values:
* `0x00`:  A **STX post-condition**, which pertains to the origin account's STX.
* `0x01`:  A **Fungible token post-condition**, which pertains to one of the origin
account's fungible tokens.
* `0x02`:   A **Non-fungible token post-condition**, which pertains to one of the origin
account's non-fungible tokens.

A _STX post condition_ body is encoded as follows:
* A variable-length **principal**, containing the address of the standard account or contract
  account
* A 1-byte **fungible condition code**, described below
* An 8-byte value encoding the literal number of microSTX

A _Fungible token post-condition_ body is encoded as follows:
* A variable-length **principal**, containing the address of the standard account or contract
  account
* A variable-length **asset info** structure that identifies the token type, described below
* A 1-byte **fungible condition code**
* An 8-byte value encoding the literal number of token units

A _Non-fungible token post-condition_ body is encoded as follows:
* A variable-length **principal**, containing the address of the standard account or contract
  account
* A variable-length **asset info** structure that identifies the token type
* A variable-length **asset name**, which is the Clarity value that names the token instance,
  serialized according to the Clarity value serialization format.
* A 1-byte **non-fungible condition code**

A **principal** structure encodes either a standard account address or a
contract account address.
* A standard account address is encoded as a 1-byte version number and a 20-byte
  Hash160.
* A contract account address is encoded as a 1-byte version number, a 20-byte
  Hash160, a 1-byte name length, and a variable-length name of up to 128
characters.  The name characters must be a valid contract name (see below).

The _standard principal_ variant of the principal post-condition field is
encoded as follows:
* A 1-byte type prefix of `0x02`
* The standard principal's 1-byte version
* The standard principal's 20-byte Hash160

The _contract principal_ variant of the principal post-condition field is
encoded as follows:
* A 1-byte type prefix of `0x03`
* The 1-byte version of the standard principal that issued the contract
* The 20-byte Hash160 of the standard principal that issued the contract
* A 1-byte length of the contract name, up to 128
* The contract name

An **asset info** structure identifies a token type declared somewhere in an
earlier-processed Clarity smart contract.  It contains the following fields:
* An **address**, which identifies the standard account that created the
  contract that declared the token.  This is encoded as a 1-byte version,
  followed by a 20-byte public key hash (i.e., a standard account address).
* A **contract name**, a length-prefixed Clarity string that encodes the
  human-readable part of the contract's name.
* An **asset name**, a length-prefixed Clarity string that encodes the name of
  the token as declared in the Clarity code. 

The _address_ and _contract name_ fields together comprise the smart contract's
fully-qualified name, and the asset name field identifies the specific token
declaration within the contract.

The _contract name_ is encoded as follows:
* A 1-byte length prefix, up to 128 
* A variable-length string of valid ASCII characters (up to 128 bytes).  This
  string must be accepted by the regex `^[a-zA-Z]([a-zA-Z0-9]|[-_])`<code>&ast;</code>`$`.

The _asset name_ is encoded as follows:
* A 1-byte length prefix, up to 128 
* A variable length string of valid ASCII characters (up to 128 bytes).  This
  string must be accepted by the regex `^[a-zA-Z]([a-zA-Z0-9]|[-_!?])`<code>&ast;</code>`$`.

A **fungible condition code** encodes a statement being made for either STX or
a fungible token, with respect to the originating account.  It can take one of the 
following values, with the following meanings regarding the associated token
units:
* `0x01`: "The account sent an amount equal to the number of units"
* `0x02`: "The account sent an amount greater than the number of units"
* `0x03`: "The account sent an amount greater than or equal to the number of units"
* `0x04`: "The account sent an amount less than the number of units"
* `0x05`: "The account sent an amount less than or equal to the number of units"

A **non-fungible condition code** encodes a statement being made about a
non-fungible token, with respect to whether or not the particular non-fungible
token is owned by the account.  It can take the following values:
* `0x10`: "The account will SEND this non-fungible token"
* `0x11`: "The account will NOT SEND this non-fungible token"

Post-conditions are defined in terms of which assets each account sends or
does not send during the transaction's execution.  To enforce post-conditions,
the Clarity VM records which assets each account sends as the transaction
is evaluated to produce an "asset map."  The asset map is used to evaluate the post-conditions.

#### Transaction Payloads

There are five different types of transaction payloads.  Each payload is encoded
as follows:
* A 1-byte **payload type ID**, between 0 and 5 exclusive.
* A variable-length **payload**, of which there are five varieties.

The _payload type ID_ can take any of the following values:
* `0x00`:  the payload that follows is a **token-transfer payload**
* `0x01`:  the payload that follows is a **smart-contract payload**
* `0x02`:  the payload that follows is a **contract-call payload**
* `0x03`:  the payload that follows is a **poison-microblock payload**
* `0x04`:  the payload that follows is a **coinbase payload**.

The _STX token-transfer_ structure is encoded as follows:
* A **recipient principal** encoded as follows:
  * A 1-byte type field indicating whether the principal is
    * `0x05`: a recipient address
    * `0x06`: a contract recipient
  * If a simple recipient address, the 1-byte type is followed by a
    1-byte address version number and a 20-byte hash identifying a standard
    recipient account.
  * If a contract recipient address, the 1-byte type is followed by
    the issuer address of the contract, encoded with a 1-byte address
    version number and the 20-byte hash that identifies the standard
    account of the issuer. This is followed by the encoding of the
    contract name -- encoded as described above.
* An 8-byte number denominating the number of microSTX to send to the recipient
  address's account.

Note that if a transaction contains a token-transfer payload, it MUST
have only a standard authorization field. It cannot be sponsored. The
recipient principal does not need to be a materialized account -- STX
may be transferred to an account which has not been used in any prior
transactions. In the case of a contract principal, the unmaterialized
contract principal will receive the funds and maintain a balance in
the STX holdings map. If and when that contract is published, the contract
will be able to spend those STX via `(as-contract (stx-transfer? ...`
invocations.

A _smart-contract payload_ is encoded as follows:
* A **contract name** string, described above, that encodes the human-readable
  part of the contract's fully-qualified name.
* A **code body** string that encodes the Clarity smart contract itself.  This
  string is encoded as:
   * A 4-byte length prefix
   * Zero or more human-readable ASCII characters -- specifically, those between `0x20` and
     `0x7e` (inclusive), and the whitespace characters `\n` and `\t`.

Note that when the smart contract is instantiated, its fully-qualified name will
be computed from the transaction's origin account address and the given contract
name.  The fully-qualified name must be globally unique -- the transaction will
not be accepted if its fully-qualified name matches an already-accepted smart
contract.

A _contract-call payload_ is encoded as follows:
* A **contract address**, comprised of a 1-byte address version number and a
  20-byte public key hash of the standard account that created the smart
contract whose public function is to be called,
* A length-prefixed **contract name** string, described above, that encodes the
  human readable part of the contract's fully-qualified name,
* A length-prefixed **function name** string, comprised of a 1-byte length and
  up to 128 characters of valid ASCII text, that identifies the public function
to call.  The characters must match the regex `^[a-zA-Z]([a-zA-Z0-9]|[-_!?])`<code>&ast;</code>`$`.
* A length-prefixed list of **function arguments**, encoded as follows:
   * A 4-byte length prefix, indicating the number of arguments
   * Zero or more binary strings encoding the arguments as Clarity values.
     Clarity values are serialized as described in the section
     [Clarity Value Representation](#clarity-value-representation).

Note that together, the _contract address_ and _contract name_ fields uniquely identify
the smart contract within the Clarity VM.

A _poison microblock payload_ is encoded as follows:
* Two Stacks microblock headers, such that either the `prev_block` or `sequence`
  values are equal.  When validated, the ECDSA recoverable `signature` fields of both microblocks
must recover to the same public key, and it must hash to the leader's parent
anchored block's public key hash.  See the following sections for the exact
encoding of a Stacks microblock header.

This transaction type is sent to punish leaders who intentionally equivocate
about the microblocks they package, as described in SIP 001.

A _coinbase payload_ is encoded as follows:
* A 32-byte field called a **coinbase buffer** that the Stacks leader can fill with whatever it wants.

Note that this must be the first transaction in an anchored block in order for the
anchored block to be considered well-formed (see below).

#### Transaction Signing and Verifying

A transaction may have one or two spending conditions.  The act of signing
a transaction is the act of generating the signatures for its authorization
structure's spending conditions, and the act of verifying a transaction is the act of (1) verifying
the signatures in each spending condition and (2) verifying that the public key(s) 
of each spending condition hash to its address.

Signing a transaction is performed after all other fields in the transaction are
filled in.  The high-level algorithm for filling in the signatures in a spending
condition structure is as follows:

0. Set the spending condition address, and optionally, its signature count.
1. Clear the other spending condition fields, using the appropriate algorithm below.
   If this is a sponsored transaction, and the signer is the origin, then set the sponsor spending condition
   to the "signing sentinel" value (see below).
2. Serialize the transaction into a byte sequence, and hash it to form an
   initial `sighash`.
3. Calculate the `presign-sighash` over the `sighash` by hashing the 
   `sighash` with the authorization type byte (0x04 or 0x05), the fee (as an 8-byte big-endian value),
   and the nonce (as an 8-byte big-endian value).
4. Calculate the ECDSA signature over the `presign-sighash` by treating this
   hash as the message digest.  Note that the signature must be a `libsecp256k1`
   recoverable signature.
5. Calculate the `postsign-sighash` over the resulting signature and public key
   by hashing the `presign-sighash` hash, the signing key's public key encoding byte, and the
   signature from step 4 to form the next `sighash`.  Store the message
   signature and public key encoding byte as a signature auth field.
6. Repeat steps 3-5 for each private key that must sign, using the new `sighash`
   from step 5.

The algorithms for clearing an authorization structure are as follows:
* If this is a single-signature spending condition, then set the fee and
  nonce to 0, and set the signature bytes to 0 (note that the address is _preserved_).
* If this is a multi-signature spending condition, then set the fee and
  nonce to 0, and set the vector of authorization fields to the empty vector
  (note that the address and the 2-byte signature count are _preserved_).

While signing a transaction, the implementation keeps a running list of public
keys, public key encoding bytes, and signatures to use to fill in the spending condition once signing
is complete.  For single-signature spending conditions, the only data the
signing algorithm needs to return is the public key encoding byte and message signature.  For multi-signature
spending conditions, the implementation returns the sequence of public keys and
(public key encoding byte, ECDSA recoverable signature) pairs that make up the condition's authorization fields.
The implementation must take care to preserve the order of public keys and
(encoding-byte, signature) pairs in the multisig spending condition, so that
the verifying algorithm will hash them all in the right order when verifying the
address.

When signing a sponsored transaction, the origin spending condition signatures
are calculated first, and the sponsor spending conditions are calculated second.
When the origin key(s) sign, the set the sponsor spending condition to a
specially-crafted "signing sentinel" structure.  This structure is a
single-signature spending condition, with a hash mode equal to 0x00, an
address and signature of all 0's, a fee and a nonce equal to 0, and a
public key encoding byte equal to 0x00.  This way, the origin commits to the
fact that the transaction is sponsored without having to know anything about the
sponsor's spending conditions.

When sponsoring a transaction, the sponsor uses the same algorithm as above to
calculate its signatures.  This way, the sponsor commits to the signature(s) of
the origin when calculating its signatures.

When verifying a transaction, the implementation verifies the sponsor spending
condition (if present), and then the origin spending condition.  It effectively
performs the signing algorithm again, but this time, it verifies signatures and
recovers public keys.

0. Extract the public key(s) and signature(s) from the spending condition.
1. Clear the spending condition.
2. Serialize the transaction into a byte sequence, and hash it to form an
   initial `sighash`.
3. Calculate the `presign-sighash` from the `sighash`, authorization type byte,
   fee, and nonce.
4. Use the `presign-sighash` and the next (public key encoding byte,
   ECDSA recoverable signature) pair to recover the public key that generated it.
5. Calculate the `postsign-sighash` from `presign-sighash`, the signature, public key encoding
   byte, 
6. Repeat steps 3-5 for each signature, so that all of the public keys are
   recovered.
7. Verify that the sequence of public keys hash to the address, using
   the address's indicated public key hashing algorithm.

When verifying a sponsored transaction, the sponsor's signatures are verified
first.  Once verified, the sponsor spending condition is set to the "signing
sentinel" value in order to verify the origin spending condition.

## Blocks

Blocks are batches of transactions proposed by a single Stacks leader.  The
Stacks leader gathers transactions from the peer network (by means of a
_mempool_), selects the ones they wish to package together into the next block
("mines" them), and then announces the block to the rest of the peer network.

A block is considered valid if (1) it is well-formed, (2) it contains a valid
sequence of transactions -- i.e., each transaction's state-transitions are
permitted, and (3) it follows the rules described in this document below.

Per SIP 001, there are two kinds of blocks: anchored blocks, and streamed
microblocks.  An anchored block is comprised of the following two fields:

* A **block header**
* A list of one or more **transactions**, encoded as:
   * A 4-byte length, counting the number of transactions,
   * A coinbase transaction, with an anchor mode of `0x01` or `0x03`,
   * Zero or more additional transactions, all of which must have an anchor mode
     byte set to either `0x01` or `0x03`.

A _block header_ is encoded as follows:
* A 1-byte **version number** to describe how to validate the block.
* The **cumulative work score** for this block's fork, described below.
* An 80-byte **VRF proof** which must match the burn commitment transaction on the burn
  chain (in particular, it must hash to its VRF seed), described below.
* A 32-byte **parent block hash**, which must be the SHA512/256 hash of the last _anchored_ block
  that precedes this block in the fork to which this block is to be appended.
* A 32-byte **parent microblock hash**, which must be the SHA512/256 hash of the last _streamed_
  block that precedes this block in the fork to which this block is to be appended.
* A 2-byte **parent microblock sequence number**, which indicates the sequence
  number of the parent microblock to which this anchored block is attached.
* A 32-byte **transaction Merkle root**, the SHA512/256 root hash of a binary Merkle tree
  calculated over the sequence of transactions in this block (described below).
* A 32-byte **state Merkle root**, the SHA512/256 root hash of a MARF index over the state of
  the blockchain (see SIP-004 for details).
* A 20-byte **microblock public key hash**, the Hash160 of a compressed public key whose private
  key will be used to sign microblocks during the peer's tenure.

The _VRF proof_ field contains the following fields:
* A 32-byte **Gamma** Ristretto point, which itself is a compressed Ed25519
  curve point (see https://ristretto.group).
* A 16-byte **c scalar**, an unsigned integer (encoded big-endian)
* A 32-byte **s scalar**, an unsigned integer mod 2^255 - 19 (big-endian)

The _cumulative work score_ contains the following two fields:
* An 8-byte unsigned integer that encodes the sum of all burnchain tokens
  burned or transferred in this fork of the Stacks blockchain (i.e., by means of
  proof-of-burn or proof-of-transfer, whichever is in effect).
* An 8-byte unsigned integer that encodes the total proof-of-work done in this
  fork of the burn chain.

In-between two consecutive anchored blocks in the same fork there can exist
zero or more Stacks microblocks.

A _microblock_ is comprised of two fields:
* A **microblock header**,
* A length-prefixed list of transactions, all of which have an
  anchor mode set to `0x02` or `0x03`.  This is comprised of:
   * A 4-byte length, which counts the number of transactions,
   * Zero or more transactions.

Each _microblock header_ contains the following information:
* A 1-byte **version number** to describe how to validate the block.
* A 2-byte **sequence number** as a hint to describe how to order a set of
  microblocks.
* A 32-byte **parent microblock hash**, which is the SHA512/256 hash of the previous signed microblock
  in this stream.
* A 32-byte **transaction Merkle root**, the SHA512/256 root hash of a binary Merkle tree
  calculated over this block's sequence of transactions.
* A 65-byte **signature** over the block header from the Stacks peer that produced
  it, using the private key whose public key was announced in the anchored
  block.  This is a recoverable ECDSA secp256k1 signature, whose recovered
  compressed public key must hash to the same value as the parent anchor block's microblock
  public key hash field.

For both blocks and microblocks, a block's hash is calculated by first
serializing its header to bytes, and then calculating the SHA512/256 hash over those bytes.

### Block Validation

The hash of the anchored block's header is written to the burn chain via a block commitment
transaction, per SIP 001.  When a well-formed anchored block is received from the peer
network, the peer must confirm that:

* The block header hashes to a known commitment transaction that won
  cryptographic sortition.
* All transactions are well-formed and have the appropriate anchor byte.
* All transactions, when assembled into a Merkle tree, hash to the given
  transaction Merkle root.
* The first transaction is a coinbase transaction.
* The block version is supported.
* The cumulative work score is equal to the sum of all work
  scores on this fork.
* The block header's VRF proof hashes to the burn commitment transaction's VRF
  seed.  Note that this is the VRF seed produced by the burnchain block just before the
  burnchain block that contains the block commitment; it is _not_ the parent
  Stacks block's VRF seed.

If any of the above are false, then there is _no way_ that the block can be
valid, and it is dropped.

Once a block passes this initial test, it is queued up for processing in a
"staging" database.  Blocks remain in this staging database
until there exists a chain tip to which to append
the block (where the chain tip in this case refers both to the parent anchored
block and parent microblock).

An anchored block is _processed_ and either _accepted_ or _rejected_ once its
parent anchored block _and_ its parent microblocks are available. 
To accept the anchored block, the peer applies the parent microblock stream's
transactions to the chain state, followed by the anchored block's transactions.
If the resulting state root matches the block's state root, then the block is
valid and the leader is awarded the anchored block's coinbase and 60% of the
microblock stream's transaction fees, released over a maturation period
(per SIP 001).  The microblock stream and anchored block are marked as
_accepted_ and will be made available for other peers to download.

Not every anchored block will have a parent microblock stream.  Anchored blocks
that do not have parent microblock streams will have their parent microblock
header hashes set to all 0's, and the parent microblock sequence number set to
0.

### Microblock Validation

When a well-formed microblock arrives from the peer network, the peer 
first confirms that:

* The parent anchored block is either fully accepted, or is queued up.
* The parent anchored block's leader signed the microblock.

If all these are true, then the microblock is queued up for processing.  It will
be processed when its descendent anchored block is ready for processing.

As discussed in SIP 001, a Stacks leader can equivocate while packaging
transactions as microblocks by deliberately creating a microblock stream fork.
This will be evidenced by the discovery of either of the following:

* Two well-formed, signed microblocks with the same parent hash
* Two well-formed, signed microblocks with the same sequence number

If such a discovery is made, the microblock stream is truncated to the last
microblock before the height in the microblock stream of the equivocation, and
this microblock (or any of its predecessor microblocks in the stream) remain
viable chain tips for subsequent leaders to build off of.  In the meantime,
anyone can submit a poison-microblock transaction with both signed headers in
order to (1) destroy the equivocating leader's coinbase and fees, and (2) receive
5% of the destroyed tokens as a reward, provided that the poison-microblock
transaction is processed before the block reward becomes spendable by the
equivocating leader.

Because microblocks are released quickly, it is possible that they will not
arrive in order, and may even arrive before their parent microblock.  Peers are
expected to cache well-formed microblocks for some time, in order to help ensure that
they are eventually enqueued for processing if they are legitimate.

Valid microblocks in the parent stream may be orphaned by the child anchored block
because the leader didn't see them in time to build off of them.
If this happens, then the orphaned microblocks are dropped.

## Block Processing

Block processing is the act of calculating the next materialized view of the
blockchain, using both the anchored block and the parent microblock stream
that connects it to its parent anchored block.
Processing the anchored block entails applying all of the transactions of its ancestor
microblocks, applying all of the anchored transactions,
and verifying that the cryptographic digest of the materialized view encoded
in the anchored block header matches the cryptographic digest calculated by applying
these transactions.

To begin _processing_ the anchored block and its parent microblock stream,
the peer must first ensure that:

* It has received all microblocks between the parent anchored block and the
  newly-arrived anchored block.
* The microblocks are well-formed.
* The microblocks are contiguous.  That is:
   * The first microblock's sequence number is 0, and its parent block hash is
     equal to the parent anchored block's hash.
   * The *i*th microblock's parent block hash is equal to the block hash of the
     *i-1*th microblock.
   * The *i*th microblock has a sequence number that is equal to 1 + the
     sequence number of the *i-1*th microblock.
   * The last microblock's hash and sequence number match the anchored block's
     parent microblock hash and parent microblock sequence number.
   * There are at most 65536 microblocks per epoch.

If all of these are true, then the peer may then proceed to process the microblocks' transactions.

To process a microblock stream, the peer will do the following for each
microblock:

1. Verify that each transaction authorization is valid.  If not, then reject and
   punish the previous Stacks leader.
2. Verify that each paying account has sufficient STX to pay their transaction
   fees.  If not, then reject and punish the previous Stacks leader.
3. For each transaction, grant the previous Stacks leader 40% of the transaction
   fee, and the current leader 60% of the transaction fee.  This encourages the
leader that produced the current anchored block to build on top of as many
of the parent's microblocks as possible.

If a microblock contains an invalid transaction, then parent block's leader forfeits their 
block reward.  The deepest valid microblock remains a valid chain tip to which
subsequent anchored blocks may be attached.

Once the end of the stream is reached, the peer processes the anchored block.
To process the anchored block, the peer will process the state-transitions of
each transaction iteratively.  To do so, it will first:

1. Verify that each transaction authorization is valid.  If not, then the block
   and any of its descendent microblocks will be rejected, and the leader
punished by forfeiting the block reward.
2. Verify that each paying account has sufficient STX to pay their advertised
   fees.  If one or more do not, then reject the block and its descendent
microblocks and punish the leader.
3. For each transaction in block order, debit the paying account and process the
   payload.  If the paying account becomes negative, then reject the block as
invalid.

Leaders do not receive their block rewards immediately.  Instead, they must
mature for 100 Stacks epochs before they become spendable.

### Calculating the Materialized View

Once the microblock stream and anchored block transactions have been validated,
and the peer has determined that each paying account has sufficient funds
to pay their transaction fees, the peer will process the contained Clarity
code to produce the next materialized view of the
blockchain.  The peer determines that the previous leader processed them
correctly by calculating a cryptographic digest over the resulting materialized
view of the blockchain state.  The digest must match the digest provided in the
anchored block.  If not, then the anchored block and its parent microblock
stream are rejected, and the previous leader is punished.

Fundamentally, the materialized view of a fork is a set of sets of
key/value pairs.  Each set of key/value pairs is calculated in the service
of _light clients_ who will want to query them.  In this capacity,
the Stacks peer tracks the following sets of key/value pairs:

* the mapping between account addresses and their nonces and asset maps
* the mapping between fully-qualified smart contract names and a bundle of
  metadata about them (described below).
* the mapping between fully-qualified smart contract data keys and their 
  associated values. 

The first set of key/value pairs is the **account state**.  The Stacks peer
calculates an index over all accounts in each fork as they are created.

The second set of key/value pairs is the **smart contract context state**.  It maps the
_fully-qualified name_ of the smart contract to:
   * the transaction ID that created the smart contract (which can be used to
     derive the contract account address and to query its code),

The fully-qualified name of a smart contract is composed of the c32check-encoded
standard account address that created it, followed by an ASCII period `.`, as well
as an ASCII-encoded string chosen by the standard account owner(s) when the contract
is instantiated (subject to the constraints mentioned in the above sections).  Note that all
fully-qualified smart contract names are globally unique -- the same standard
account cannot create two smart contracts with the same name.

The third set of key/value pairs is the **smart contract data state**.
It maps the _fully-qualified_ data keys to their values. These store
all data related to a smart contract: the values associated with data
map keys, the current value of any data variables, and the ownership
of fungible and non-fungible tokens. The construction of these keys and
values is described below.

All sets of key/value pairs are stored in the same MARF index.  Keys are
prefixed with the type of state they represent in order to avoid key collisions
with otherwise-identically-named objects.

When a key/value pair is inserted into the MARF, the hash of its key is
calculated using the MARF's cryptographic hash function in order to determine
where to insert the leaf.  The hash of the value is inserted as the leaf node,
and the (hash, leaf) pair is inserted into the peer's data store.  This ensures
that the peer can query any key/value pair on any fork it knows about in
constant-time complexity.

The text below describes the canonical encoding of key/value pairs that will be
inserted into the MARF.

#### Calculating a Fully-Qualified Object Name

All objects' fully-qualified names start with the type of object they are,
followed by an ASCII period `.`.  This can be the ASCII string "account",
"smart-contract", "data-variable", or "data-map".

Within an object type, a c32check-encoded addresses act as "namespaces" for keys in the state.  In all
sets of key/value pairs, an ASCII period `.` is used to denote the separation between
the c32check-encoded address and the following name.  Note that the c32 alphabet
does _not_ include the ASCII period.

#### Clarity Value Representation

Clarity values are represented through a specific binary encoding.  Each value
representation is comprised of a 1-byte type ID, and a variable-length
serialized payload.  The payload itself may be composed of additional Clarity
values.

The following type IDs indicate the following values:
* 0x00: 128-bit signed integer
* 0x01: 128-bit unsigned integer
* 0x02: buffer
* 0x03: boolean `true`
* 0x04: boolean `false`
* 0x05: standard principal
* 0x06: contract principal
* 0x07: Ok response
* 0x08: Err response
* 0x09: None option
* 0x0a: Some option
* 0x0b: List
* 0x0c: Tuple
* 0x0d: StringASCII
* 0x0e: StringUTF8

The serialized payloads are defined as follows:

**128-bit signed integer**

The following 16 bytes are a big-endian 128-bit signed integer

**128-bit unsigned integer**

The following 16 bytes are a big-endian 128-bit unsigned integer

**Buffer**

The following 4 bytes are the buffer length, encoded as a 32-bit unsigned big-endian
integer.  The remaining bytes are the buffer data.

**Boolean `true`**

No bytes follow.

**Boolean `false`**

No bytes follow.

**Standard principal**

The next byte is the address version, and the following 20 bytes are the
principal's public key(s)' Hash160.

**Contract Principal**

The next byte is the address version, the following 20 bytes are a Hash160, the
21st byte is the length of the contract name, and the remaining bytes (up to
128, exclusive) encode the name itself.

**Ok Response**

The following bytes encode a Clarity value.

**Err Response**

The following bytes encode a Clarity value.

**None option**

No bytes follow.

**Some option**

The following bytes encode a Clarity value.

**List**

The following 4 bytes are the list length, encoded as a 32-bit unsigned
big-endian integer.  The remaining bytes encode the length-given number of
concatenated Clarity values.

**Tuple**

The following 4 bytes are the tuple length, encoded as a 32-bit unsigned
big-endian integer.  The remaining bytes are encoded as a concatenation of tuple
items.  A tuple item's serialized representation is a 
Clarity name (a 1-byte length and up to 128 bytes (exclusive) of valid Clarity
name characters) followed by a Clarity value.

**StringASCII**

The following 4 bytes are the string length, encoded as a 32-bit unsigned
big-endian integer.  The remaining bytes are the ASCII-encoded string.  Each
byte in the string must be an alphanumeric or punctuation ASCII byte.

**StringUTF8**

The following 4 bytes are the total number of bytes that make up the UTF8
string, encoded as a 32-bit unsigned big-endian integer.  The remaining bytes
are the UTF-8-encoded string.  The byte sequence must be comprised of valid
UTF-8 codepoints.

#### Calculating the State of an Account

An account's canonical encoding is a set of key/value pairs that represent the
account's nonce, STX tokens, and assets owned.

The nonce is encoded as follows:

* Key: the string `"vm-account::"`, a c32check-encoded address, and the string
  `"::18"`
* Value: a serialized Clarity `UInt`

Example: `"vm-account::SP2RZRSEQHCFPHSBHJTKNWT86W6VSK51M7BCMY06Q::18"` refers to
the nonce of account `SP2RZRSEQHCFPHSBHJTKNWT86W6VSK51M7BCMY06Q`.

The STX balance is encoded as follows:

* Key: the string "vm-account::", a Principal Address (see below), and the string
  `"::19"`
* Value: a serialized Clarity `UInt`

Example: `"vm-account::SP2RZRSEQHCFPHSBHJTKNWT86W6VSK51M7BCMY06Q::19"` refers to
the STX balance of account `SP2RZRSEQHCFPHSBHJTKNWT86W6VSK51M7BCMY06Q`.

A fungible token balance owned by an account is encoded as follows:

* Key: the string `"vm::"`, the fully-qualified contract identifier, the string `"::2::"`,
  the name of the token as defined in its Clarity contract, the string `"::"`, and the
Principal Address of the account owning the token (see below).
* Value: a serialized Clarity `UInt`

Example: `"vm::SP13N5TE1FBBGRZD1FCM49QDGN32WAXM2E5F8WT2G.example-contract::2::example-token::SP2RZRSEQHCFPHSBHJTKNWT86W6VSK51M7BCMY06Q"`
refers to the balance of `example-token` -- a fungible token defined in contract `SP13N5TE1FBBGRZD1FCM49QDGN32WAXM2E5F8WT2G.example-contract` -- 
that is owned by account `SP2RZRSEQHCFPHSBHJTKNWT86W6VSK51M7BCMY06Q`.

A non-fungible token owned by an account is encoded as follows:

* Key: the string `"vm::"`, the fully-qualified contract identifier, the string `"::4::"`,
  the name of the token as defined in its Clarity contract, the string `"::"`,
and the serialized Clarity value that represents the token.
* Value: a serialized Clarity Principal (either a Standard Principal or a Contract Principal)

Example: `"vm::SP13N5TE1FBBGRZD1FCM49QDGN32WAXM2E5F8WT2G.example-contract::4::example-nft::\x02\x00\x00\x00\x0b\x68\x65\x6c\x6c\x6f\x20\x77\x6f\x72\x6c\x64"` 
refers to the non-fungible token `"hello world"` (which has type `buff` and is
comprised of 11 bytes), defined in Clarity contract `SP13N5TE1FBBGRZD1FCM49QDGN32WAXM2E5F8WT2G.example-contract`
as a type of non-fungible token `example-nft`.

A Principal Address is either a c32check-encoded address in the case of standard principal, or a c32check-encoded address, followed by an ASCII period `.`, and an ASCII-encoded string for a contract principal.

#### Calculating the State of a Smart Contract

Smart contract state includes data variables, data maps, the contract code, and
type metadata.  All of this state is represented in the MARF via a layer of indirection.

Contract and type metadata is _not_ committed to by the MARF.  The MARF only
binds the contract's fully-qualified name to a "contract commitment" structure,
comprised of the contract's source code hash and the block height at which it
was instantiated.  This contract commitment, in turn, is used to refer to
implementation-defined contract analysis data, including the computed AST, cost
analysis, type information, and so on.

A contract commitment structure is comprised of the SHA512/256 hash of the
contract source code body (taken verbatim from the transaction), and the block
height at which the transaction containing it was mined.  The contract
commitment is serialized as follows:

* Bytes 0-64: the ASCII-encoding of the hash
* Bytes 65-72: the ASCII-encoding of the block height, itself as a big-endian
  unsigned 32-bit integer.

Example: The contract commitment of a contract whose code's SHA512/256 hash is
`d8faa525ecb3661e7f88f0bd18b8f6676ec3c96fcd5915cf47d48778da1b7ce0` at block
height 123456 would be `"d8faa525ecb3661e7f88f0bd18b8f6676ec3c96fcd5915cf47d48778da1b7ce0402e0100"`.

When processing a new contract, the Stacks node only commits to the serialized
contract commitment structure, and stores its analysis data separately.  For
example, the reference implementation uses the contract commitment structure as
a key prefix in a separate key/value store for loading and storing its contract
analysis data.

The MARF commits to the contract by inserting this key/value pair:

* Key: the string `"clarity-contract::", followed by the fully-qualified
  contract identifier.
* Value: A serialized `ContractCommitment` structure.

Example: `"clarity-contract::SP13N5TE1FBBGRZD1FCM49QDGN32WAXM2E5F8WT2G.example-contract"` 
refer to the contract commitment for the contract
`SP13N5TE1FBBGRZD1FCM49QDGN32WAXM2E5F8WT2G.example-contract`.

### Cryptographic Commitment

The various key/value sets that make up the materialized view of the fork are
each indexed within the same MARF.  To validate an anchored block, each Stacks
peer will:

* Load the state of the MARF as of the anchored block's parent anchored block.
* Insert a mapping between this anchored block's height and a sentinel anchor 
  hash (see below)
* Insert a mapping between the parent anchored block's height and its "anchor
  hash" derived from both the parent block's hash and the burnchain block that
  selected it (see below)
* Process all transactions in this anchored block's parent microblock stream,
  thereby adding all keys and values described above to the materialized view.
* Process all transactions in the anchored block, thereby adding all keys and
  values described above to the materialized view.
* Insert the rewards from the latest now-matured block (i.e., the
  leader reward for the Stacks block 100 epochs ago in this fork) into the
leader rewards contract in the Stacks chain boot code.  This rewards the leader
and all users that burned in support of the leader's block.

Once this process is complete, the Stacks peer checks the root hash of its MARF
against the root hash in the anchored block.  If they match, then the block is
accepted into the chain state.  If they do not match, then the block is invalid.

#### Measuring Block Height

Stacks counts its forks' lengths on a per-fork basis within each fork's MARF.
To do so, a leader always inserts five key/value pairs into the MARF when it
starts processing the next cryptographic commitment:  two to map the block's parent's height to
its anchor hash and vice versa, two to map this block's height to a sentinel 
anchor hash (and vice versa), and one to represent this block's height.
These are always added before processing any transactions.

The anchored block's _anchor hash_ is the SHA512/256 hash of the anchored block's
header concatenated with the hash of the underlying burn chain block's header.
For example, if an anchored block's header's hash is
`7f3f0c0d5219f51459578305ed2bbc198588758da85d08024c79c1195d1cd611`, and the
underlying burn chain's block header hash is
`e258d248fda94c63753607f7c4494ee0fcbe92f1a76bfdac795c9d84101eb317`, then the
(little-endian) anchor hash would be
`7fbeb26cae32d96dbc1329f7e59f821b2c99b0a71943e153c071906ca7205f5f`.  In the case
where Bitcoin is the burn chain, the block's header hash is the double-SHA256 of
its header, in little-endian byte order (i.e., the 0's are trailing).

When beginning to process the anchored block (and similarly, when a leader
begins to produce its anchored block), the peer adds the following key/value
pairs to the MARF, in this order:

* Key: The string
  `"_MARF_BLOCK_HEIGHT_TO_HASH::af425f228a92ebe4d7741b129bb2c2f4326179f682da305b030250ccea9d4cd5"`
* Value: the height of the current Stacks block, encoded as a 4-byte
  little-endian 32-bit integer

The hash `af425f228a92ebe4d7741b129bb2c2f4326179f682da305b030250ccea9d4cd5` is
the sentinel anchored hash.  It is the SHA512/256 hash of a 64 `0x01` bytes --
equivalent to calculating an anchored hash from a Stacks block header and a burn
chain block header whose hashes were both `0101010101010101010101010101010101010101010101010101010101010101`.

* Key: The string `"_MARF_BLOCK_HASH_TO_HEIGHT::"`, followed by the ASCII string
  representation of the block height
* Value: the 32-byte sentinel anchor hash

Example: The key `"_MARF_BLOCK_HEIGHT_TO_HASH:124"` would map to the sentinel
anchor hash if the Stacks block being appended was the 124th block in the fork.

* Key: The string `"_MARF_BLOCK_HEIGHT_SELF"`
* Value: the ASCII representation of the block's height.

Example: The key `"_MARF_BLOCK_HEIGHT_SELF"` would map to the string `"123"` if
this was the 123rd block in this fork.

* Key: The string `"_MARF_BLOCK_HEIGHT_TO_HASH::"`, followed by the ASCII string
representation of the anchored block's parent's height.  Note that when
processing an anchored block, the parent's block hash will be known, so the
sentinel anchor hash is _not_ used.  The only exception is the boot block (see
below)
* Value: The 32-byte anchor hash of the block

Example: The key `"_MARF_BLOCK_HEIGHT_TO_HASH::123"` would map to the anchor
hash of the 123rd anchored Stacks block.

* Key: The string "_MARF_BLOCK_HASH_TO_HEIGHT::"`, followed by 64 characters in
  the ASCII range `[0-9a-f]`.
* Value: The little-endian 32-bit block height

Example: The key `"_MARF_BLOCK_HASH_TO_HEIGHT::7fbeb26cae32d96dbc1329f7e59f821b2c99b0a71943e153c071906ca7205f5f"` 
would map to the height of the block whose anchored hash was
`7fbeb26cae32d96dbc1329f7e59f821b2c99b0a71943e153c071906ca7205f5f`.

Using these five key/value pairs, the MARF is able to represent the height of
a fork terminating in a given block hash, and look up the height of a block in a
fork, given its anchor hash.

### Processing the Boot Block

The first-ever block in the Stacks v2 chain is the **boot block**.  It contains
a set of smart contracts and initialization code for setting up miner reward
maturation, for handling BNS names, for migrating BNS state from Stacks v1, and
so on.

When processing the boot block, the anchor hash will always be
`8aeecfa0b9f2ac7818863b1362241e4f32d06b100ae9d1c0fbcc4ed61b91b17a`, which is
equal to the anchor hash calculated from a Stacks block header hash and a
burnchain block header hash of all 0's.  The `_MARF_BLOCK_HASH_TO_HEIGHT::0` key
will be mapped to this ASCII-encoded hash, the key `_MARF_BLOCK_HASH_TO_HEIGHT::8aeecfa0b9f2ac7818863b1362241e4f32d06b100ae9d1c0fbcc4ed61b91b17a` 
will be mapped to `"0"`, and `_MARF_BLOCK_HEIGHT_SELF` will be mapped to `"0"`.  After these three  keys are inserted, the block is
processed like a normal Stacks anchored block.  The boot block has no parent,
and so it will not have height-to-hash mappings for one.

When processing a subsequent block that builds directly on top of the boot
block, the parent Stacks block header hash should be all 0's.

# Related Work

This section will be expanded upon after this SIP is ratified.

# Backwards Compatibility

All Stacks accounts from Stacks 1.0 will be imported into Stacks 2.0 when it
launches.  The state of the Stacks 1.0 chain will be snapshotted at Bitcoin
block 665750.

# Activation

At least 20 miners must register a name in the `.miner` namespace in Stacks 1.0.
Once the 20th miner has registered, the state of Stacks 1.0 will be snapshotted.
300 Bitcoin blocks later (Bitcoin block 666050), the Stacks 2.0 blockchain will launch.  Stacks 2.0
implements this SIP.

# Reference Implementations

Implemented in Rust.  See https://github.com/blockstack/stacks-blockchain.

          
# Preamble

SIP Number: 022

Title: Emergency Fix to PoX Stacking Increases

Authors:
    Aaron Blankstein <aaron@hiro.so>,
    Brice Dobry <brice@hiro.so>,
    Friedger Müffke <mail@friedger.de>,
    Jude Nelson <jude@stacks.org>,
    Pavitthra Pandurangan <pavi@stacks.org>,

Consideration: Technical, Governance

Type: Consensus

Status: Ratified

Created: 19 April 2023

License: BSD 2-Clause

Sign-off: Rafael Cárdenas <rafael@hiro.so> (SIP Editor), Jesse Wiley <jesse@stacks.org> (Acting Technical CAB Chair), Jason Schrader <jason@joinfreehold.com> (Governance CAB Chair), Jude Nelson <jude@stacks.org> (Steering Committee Chair)

Discussions-To: https://github.com/stacksgov/sips

# Abstract

On 19 April 2023, it was discovered that there was a bug in the PoX smart
contract which would allow a user to claim that they have stacked far, far more
STX than they had locked.  Exploiting this bug both allows the user to increase
their PoX reward slot allotment, while also driving up the stacking minimum.
Furthermore, it creates the conditions for a network-wide crash:  if the total
amount of STX stacked as reported by the PoX smart contract were to ever exceed
the total amount of liquid STX, then the node would crash into an irrecoverable
state.  This bug has already been triggered in the wild.

This SIP proposes **two immediate consensus-breaking changes** that both prevents this bug
from being triggered in subsequent reward cycles, and replaces the current PoX
implementation with a new PoX implementation without the bug.  If ratified, this
SIP would activate two hard forks 200 Bitcoin blocks prior to the start of
reward cycles #58 and #59.

This SIP would constitute two consensus-rules version bumps.  During cycle #58, the system 
version would be Stacks 2.2.  During and after cycle #59, the system would be
Stacks 2.3.

# Addendum

*The following was added after this SIP was accepted, where some version number changes were necessary. The following section addresses these changes **without** changing the ratified text* 

[SIP-023](./sips/sip-023/sip-023-emergency-fix-traits.md), which was accepted by the required CAB's on May 2nd, 2023 necessitates some changes to this SIP. The changes introduced in [SIP-023](./sips/sip-023/sip-023-emergency-fix-traits.md) are in response to a bug introduced by the first hard fork, referred to later in this SIP as `Stacks 2.2`. To address the issue, an intermediary version `Stacks 2.3` was created which supercedes the second hard fork as defined here. 
As a result, the second hard fork defined later in this SIP, `Stacks 2.3` **is now changed to `Stacks 2.4`** due to the intermediary release required as a result of the first hard fork `Stacks 2.2`. 

In addition to the version number change, [SIP-023](./sips/sip-023/sip-023-emergency-fix-traits.md) also necessitates a change to the peer network version defined later in this SIP as follows:

* Change the mainnet peer network version bits from `0x18000008` to `0x18000009`. 
* Change the testnet peer network version bits from `0xfacade08` to `0xfacade09`. 

# Addendum II

_The following was added after this SIP was accepted to reflect the updated activation height for cycle 60, where the current text addresses cycle 59 activation height. The following section addresses these changes **without** changing the ratified text_

[SIP-023](./sips/sip-023/sip-023-emergency-fix-traits.md), which was accepted by the required CAB's on May 2nd, 2023 defines specific [activation criteria](./sips/sip-022/sip-022-emergency-pox-fix.md#activation) for cycle #59. As a result of more testing being done prior to the release, this window shall be missed and the new target activation will be in cycle #60, or Bitcoin block `792051`.

Current text:
> The second hard fork will take effect 200 Bitcoin blocks prior to the start of reward cycle #59, which is Bitcoin block height 789751.

Is now changed to the following:
> The second hard fork will take effect 400 Bitcoin blocks prior to the start of the prepare phase for cycle #60, which is Bitcoin block height 791551.


The 3 values changed:

- Reward cycle is changing from `59` to `60`
- Activation height is change from `789751` to `791551 `
- Blocks prior to activation height for the start of the reward cycle is changed from `200` to `400` to allow more time for contract deployments and stacking transactions



# Introduction

[SIP-015](./sips/sip-015/sip-015-network-upgrade.md) proposed a new PoX smart
contract, `pox-2`, which included a new public function `stack-increase`.  This
function allows a user to increase the amount of STX locked for PoX while the
account has locked STX.  The `stack-increase` function calls an internal function
`increase-reward-cycle-entry` to update the PoX contract's data space to record
the increase.

The `increase-reward-cycle-entry` function has a bug in this code path.  The bug
itself is annotated with comment lines starting with "(BUG)".

```clarity
(let ((existing-entry (unwrap-panic (map-get? reward-cycle-pox-address-list { reward-cycle: reward-cycle, index: reward-cycle-index })))
      (existing-total (unwrap-panic (map-get? reward-cycle-total-stacked { reward-cycle: reward-cycle })))
      (total-ustx (+ (get total-ustx existing-total) (get add-amount data))))
    ;; stacker must match
    (asserts! (is-eq (get stacker existing-entry) (some (get stacker data))) none)
    ;; update the pox-address list
    (map-set reward-cycle-pox-address-list
             { reward-cycle: reward-cycle, index: reward-cycle-index }
             { pox-addr: (get pox-addr existing-entry),
               ;; (BUG) ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
               ;; (BUG) `total-ustx` is the amount of uSTX locked in this cycle, not
               ;; (BUG) the stacker's total amount of uSTX. This value ought to be
               ;; (BUG) `(+ (get total-ustx existing-entry) (get add-amount data))`
               ;; (BUG) ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
               total-ustx: total-ustx,
               stacker: (some (get stacker data)) })
    ;; update the total
    (map-set reward-cycle-total-stacked
             { reward-cycle: reward-cycle }
             { total-ustx: total-ustx })
    (some { first-cycle: first-cycle,
            reward-cycle: (+ u1 reward-cycle),
            stacker: (get stacker data),
            add-amount: (get add-amount data) })))))
```

The bug is reachable by any solo Stacker who calls `stack-increase`.  Any solo
Stacker who increases their total STX locked will erroneously set the amount of
uSTX backing their PoX address to be equal to the current total number of uSTX
locked in this cycle (the value `total-ustx`), instead of the sum of their
current locked uSTX amount and their added amount.  The act of triggering this
bug is an unavoidable consequence of calling the `stack-increase` function --
individuals who call `stack-increase` are not thought to be deliberately
exploiting the system.

This bug was first triggered in the wild by Stacks transaction
`20e708e250bad3fb5d5ab84a70365c3c1cf0520c7a9f67cd5ff6b9fa94335ea5`.

The immediate consequences for PoX of this bug being triggered are as follows:

* The total STX locked to the PoX address owned by the caller of `stack-increase`
  will be potentially higher than the amount of STX that the caller possesses.
The caller will receive PoX payouts _as if_ they had locked that much STX.  So,
the caller receives undue BTC.

* The stacking threshold is raised to account for the PoX contract's reported
  increase in STX locked.

Similar consequences are expected when a Stacker contributes to two different PoX addresses and 
at least one PoX address would benefit from auto-unlocking. This behavior has not been seen in the wild.

Furthermore, if this bug is triggered enough times, the Stacks network will crash.  This is
because of a runtime assertion in the PoX reward set calculation logic that
verifies that the total locked STX does not exceed the total liquid STX.  This
would no longer be true due to this bug.  The offending assertion is detailed
below:

```rust
pub fn get_reward_threshold_and_participation(
    pox_settings: &PoxConstants,
    addresses: &[RawRewardSetEntry],
    liquid_ustx: u128,
) -> (u128, u128) {
    let participation = addresses
        .iter()
        .fold(0, |agg, entry| agg + entry.amount_stacked);

    assert!(
        participation <= liquid_ustx,
        "CORRUPTION: More stacking participation than liquid STX"
    );
```

The `RawRewardSetEntry` data are pulled directly from the
`reward-cycle-pox-address-list` data map, and the `.amount_stacked` field is
equal to the `total-ustx` field that was erroneously set in `stack-increase`.

Considering these consequences with respect to the [draft blockchain catastrophic
failure and recovery guidelines](https://github.com/stacksgov/sips/pull/10),
this bug would be categorized as requiring an _immediate_ hard fork to rectify.
The network has not yet crashed, but it is in imminent danger of crashing and
there is insufficient time to execute a Stacker-based SIP vote as has been customary for
past hard forks.  This SIP instead proposes that this hard fork activate at the
start of the next reward cycle (#58) at the time of this writing.

There is less than one reward cycle remaining to fix this
problem, and yet a Stacker vote would require at least one complete reward cycle
to carry out a vote (not accounting for any additional time required for
sending out adequate public communications and tabulating the vote afterwards).
A subsequent hard fork to re-enable PoX would have the effect of restoring
Stacker voting.

# Specification

Given the lack of time to conduct a Stacker vote to activate this SIP, the
proposed fix in this SIP is as parsimonious and discreet as possible.  It will
execute as a set of **two hard forks**.

The first hard fork, which activates before the start of reward cycle #58, will disable PoX beginning with cycle #58.
The `pox-2` contract will be considered defunct, just as the pre-2.1 `pox`
contract is.  This hard fork would do the following:

* Prevent all stacking functions from being called in `pox-2`.  The `pox-2`
  contract would be considered defunct, like the `pox` contract is now.

* Set the minimum required block-commit memo bits to `0x07`.  All block-commits
  after the Bitcoin block activation height must have a memo value of at least
`0x07`.  This ensures that miners that do not upgrade from Stacks 2.1 will not
be able to mine in Stacks 2.2.

* Set the mainnet peer network version bits to `0x18000007`.  This ensures that follower
  nodes that do not upgrade to Stacks 2.2 will not be able to talk to Stacks
2.2 nodes.

* Set the testnet peer network version bits to `0xfacade07`.  This ensures that
  testnet follower nodes that do not upgrade to Stacks 2.2 will not be able to
talk to Stacks 2.2 nodes.

The second hard fork will instantiate a new PoX
implementation `pox-3`. The `pox-3` contract code will:

* Fix the aforementioned `stacks-increase` bug.

* Prevent Stackers from contributing to two or more PoX addresses. In particular,
    * Add a `delegated-to` field in the `stacking-state` data map, so that the 
    `stacking-state` entry for a PoX user will indicate the principal to which 
    their STX are delegated (if they are using delegated stacking).

    * Add checks to the delegated stacking public functions that validate the 
    `delegated-to` field in the Stacker's `stacking-state` map entry.

* Set the minimum required block-commit memo bits to `0x08`.  All block-commits
  after the Bitcoin block activation height must have a memo value of at least
`0x08`.  This ensures that miners that do not upgrade from Stacks 2.2 will not
be able to mine in Stacks 2.3.

* Set the mainnet peer network version bits to `0x18000008`.  This ensures that follower
  nodes that do not upgrade to Stacks 2.3 will not be able to talk to Stacks
2.3 nodes.

* Set the testnet peer network version bits to `0xfacade08`.  This ensures that
  testnet follower nodes that do not upgrade to Stacks 2.3 will not be able to
talk to Stacks 2.3 nodes.

## Alternative Approaches Considered

It is unusual that this SIP proposes two hard forks in rapid succession without 
a Stacker-based vote for activating the new rules.  The
reason for this is because PoX must first be disabled prior to the start of
reward cycle #58 in order to avert a network-wide
crash.  This cannot be delayed, but there is insufficient time to prepare `pox-3`
or even execute a Stacker-based vote before this bug must be addressed.
Therefore, the first hard fork must happen immediately.

A second hard fork is necessary to instantiate `pox-3`, which must also happen
as soon as possible (no more than one reward cycle later) in order to restore
critical functionality to the Stacks blockchain.  Because PoX would be disabled
before `pox-3` would activate, there would not exist a means of carrying out a
Stacker-based vote to upgrade the system (as had been the case in all prior
hard forks).  Beyond receiving a BTC yield, **the lack of a functioning PoX contract
prevents consensus-level SIPs from being voted upon**, which necessitates an
expedient restoration.

A less-disruptive alternative approach of repairing the
`reward-cycle-pox-address-list` data map was considered.  This would have only
required a single hard fork, and no disruption of PoX payouts (however,
`stack-increase` would have been disabled).  This approach was ultimately dropped
because of the interactions between delegated stacking and solo stacking -- it
would not be possible to retroactively compute the correct values for the
`reward-cycle-pox-address-list` data map.  This, in turn, meant that any repair
tactic would require the node to calculate the correct values for
`reward-cycle-pox-address-list` as the blocks in reward cycle #57 were
processed, which in turn meant that nodes would need to boot from genesis to
derive the correct data with which to repair the data map at the start of reward
cycle #58.  There is insufficient time to implement, test, and deploy this
approach.

# Related Work

Consensus bugs requiring immediate attention such as this
have been detected and fixed in other blockchains.  In the
absence of a means of gathering user comments on proposed fixes, the task of
activating these bugfixes has fallen to miners, exchanges, and node runners.  As
long as sufficiently many participating entities upgrade, then a chain split is
avoided and the fixed blockchain survives.  A prominent example was Bitcoin
[CVE-2010-5139](https://www.cvedetails.com/cve/CVE-2010-5139/), in which a
specially-crafted Bitcoin transaction could mint arbitrarily many BTC well above
the 21 million cap.  The [developer
response](https://bitcointalk.org/index.php?topic=823.0) was to quickly release
a patched version of Bitcoin and rally enough miners and users to upgrade.  In a
matter of hours, the canonical Bitcoin chain ceased to include any transactions
that minted too much BTC.

# Backwards Compatibility

There are no changes to the chainstate database schemas in this SIP.  Everyone
who runs a Stacks 2.1 node today will be able to run a Stacks 2.2 node off of
their existing chainstates at the start of reward cycle #58,
as well as a Stacks 2.3 node off of a Stacks 2.2 chainstate at the start of
reward cycle #59.

Stacks 2.2 nodes will not interact with Stacks 2.1 nodes on the peer network
after the Bitcoin block activation height passes.  In addition, Stacks 2.2 nodes
will ignore block-commits from Stacks 2.1 nodes.  Similar changes were made for
Stacks 2.05 and Stacks 2.1 to ensure that the new network cleanly separates from
stragglers still following the old rules.  This also applies to Stacks 2.3 nodes
and Stacks 2.2 nodes.

# Activation

This SIP shall be considered Activated if the Stacks 2.3 network is live at the
start of reward cycle #59.

The Bitcoin block activation height for the first hard fork will need to pass prior to the selection of
the PoX anchor block for reward cycle #58 (Bitcoin block height 787851).  This
SIP proposes Bitcoin block height 787651, which is 200 Bitcoin blocks prior.  In
other words, the Bitcoin block that is mined 100 blocks prior to the start of
the prepare phase for reward cycle #58 shall be the activation height.

The second hard fork will take effect 200 Bitcoin blocks prior to the start of reward cycle #59,
which is Bitcoin block height 789751.

The node software for Stacks 2.2 and 2.3 shall be merged to the `master` branch of the
reference implementation no later than three days prior to the activation
height.  This means that everyone shall have at least three days to upgrade
their Stacks 2.1 nodes to Stacks 2.2, and at least three days to upgrade from
2.2 to 2.3.

# Reference Implementation

The reference implementation of this SIP can be found in the
`feat/sip-022-pox-disable` branch of
the Stacks blockchain reference implementation.  It is available at
https://github.com/stacks-network/stacks-blockchain.

          
# Preamble

SIP Number: 016

Title: Schema Definition for Metadata for Digital Assets

Author: Friedger Müffke (mail@friedger.de), Dan Trevino (dantrevino@gmail.com)

Consideration: Technical

Type: Standard

Status: Ratified

Created: 7 November 2021

License: CC0-1.0

Sign-off: Jude Nelson (jude@stacks.org)

Layer: Traits

# Abstract

Non-fungible tokens - NFTs for short - are digital assets registered on
blockchain with unique identifiers and properties that distinguish them from
each other. SIP-009 defines the trait for how ownership of an NFT is managed.
Fungible tokens - FTs for short - are digital assets where each token can be
replaced by another token (see SIP-010). Semi-fungible tokens are digital assets
where each token has a unique identifier and is dividable into fungible parts
(see SIP-013). This SIP aims to provide a flexible standard to attach metadata
to NFTs, like descriptions or urls to digital files. The same standard is
applicable to fungible tokens.

# License and Copyright

This SIP is made available under the terms of the Creative Commons CC0 1.0
Universal license, available at
https://creativecommons.org/publicdomain/zero/1.0/ This SIP’s copyright is held
by the Stacks Open Internet Foundation.

# Introduction

Tokens are digital assets registered on blockchain through a smart contract. A
non-fungible token (NFT) is a token that is globally unique and can be
identified through its unique identifier. In blockchains with smart contracts,
including the Stacks blockchain, developers and users can use smart contracts to
register and interact with non-fungible tokens.

Some use cases of NFTs are name registration, digital art, certification, media
and entertainment, real-estate. They all require that users associate certain
content with an NFT. In general, it is helpful for the users to have a name,
sound, image that represents this content.

# Specification

Every SIP-016 compliant smart contract in the Stacks blockchain must implement
one or more functions that return a resolvable/retrievable URI referencing
metadata. The metadata provide information e.g. for displaying a digital asset
to users. This type of function is named "metadata URI functions".

Appendix A contains a list of trait functions that must meet the following
requirements for the return value. The appendix can be extended without changing
the ratification status of this SIP. Any changes to that appendix must be noted
in the changelog subsection.

## Return Value of Metadata URI Functions

The return value must be a `some` value if and only if the metadata reference an
existing token, otherwise the value must be `none`. Appendix A specifies the
exact meaning of "existing" for each function.

For existing tokens, the inner value of the return value must be a string
representing a resolvable URI.

The schema of the resolvable URI is not specified and should be a well-known
schema like `https`, `ar`, `ipfs`, `sia`. A `data` URI is also valid, however,
the length is limited by this SIP.

If a metadata URI function expects a parameter of type `uint` that identifies a
token and the resulting strings contain `{id}`, then the `{id}` part must be
replaced by the identifier in decimal format given in the function call.

The resolved data of the URI must be a JSON blob.

## JSON scheme of Metadata

The JSON blob resolved through the URI must follow the following JSON schema.

If metadata were retrieved by a function call containing a token identifier and
the string `{id}` exists in any JSON value, it MUST be replaced with the actual
token id in decimal format, by all client software that follows this standard.

```
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Token Metadata",
    "type": "object",
    "required": ["sip", "name"],
    "properties": {
        "sip": {
            "type": "number",
            "description": "SIP number that defines the JSON schema for metadata. For this SIP, the sip number must be `16`."
        },
        "name": {
            "type": "string",
            "description": "Identifies the asset which this token represents"
        },
        "description": {
            "type": "string",
            "description": "Describes the asset which this token represents"
        },
        "image": {
            "type": "string",
            "description": "A URI pointing to a resource with MIME type image/* representing the asset to which this token represents. Consider making any images at a width between 320 and 1080 pixels and aspect ratio between 1.91:1 and 4:5 inclusive. If the token represents a media file of different MIME type or of higher quality defined in property 'raw_media_file_uri', then this image should be used as preview image like a cover for music, or an low-res image."
        },
        "attributes": {
            "type": "array",
            "description": "Additional attributes of the token that are \"observable\". See section below. Values may be strings, numbers, object or arrays.",
            "items": {
                "type": "object",
                "required": ["trait_type", "value"],
                "properties": {
                    "display_type": {"type": "string"},
                    "trait_type": {"type": "string"},
                    "value": {"anyOf": [{"type": "object"}, {"type": "string"}, {"type": "number"}, {"type": "integer"}, {"type": "boolean"}, {"type": "array"}]}
                }
            }
        },
        "properties": {
            "type": "object",
            "description": "Additional other properties of the token. See section below. Values may be strings, numbers, object or arrays."
        },
        "localization": {
            "type": "object",
            "required": ["uri", "default", "locales"],
            "properties": {
                "uri": {
                    "type": "string",
                    "description": "The URI pattern to fetch localized data from. This URI should contain the substring `{locale}` which will be replaced with the appropriate locale value before sending the request. See section about localization for more rules"
                },
                "default": {
                    "type": "string",
                    "description": "The locale of the default data within the base JSON"
                },
                "locales": {
                    "type": "array",
                    "description": "The list of locales for which data is available. These locales should conform to those defined in the Unicode Common Locale Data Repository (http://cldr.unicode.org/)."
                }
            }
        },
        "image_data": {
            "type": "string",
            "description": "Raw SVG image data. Deprecated. Use `properties.image_data`."
        },
        "external_url": {
            "type": "string",
            "description": "Url to view the item on a 3rd party web site. Deprecated. Use `properties.external_url`."
        },
        "animation_url": {
            "type": "string",
            "description": "URL to a multi-media attachment for the item. Deprecated. Use `properties.animation_url`."
        }
    }
}
```

The length of string values is not restricted. Nowadays, clients should be smart
enough to deal with values of different lengths. Note, that the [sitemap
protocol](https://www.sitemaps.org/protocol.html) and many search engines
support only URLs with less than 2048 characters.

### Example

token101.json

```
{
  "sip": 16,
  "name": "Foo #101",
  "image": "ipfs://somerandomecid",
  "attributes": [
     {
      "trait_type": "hair",
      "value": "red",
    },
    {
      "trait_type": "strength",
      "display_type": "number",
      "value": 99,
    },
  ],
  "properties": {
      "collection":  "Foo Collection",
      "total_supply":  "10000"
  },
  "localization": {
      "uri": "ipfs://somerandomcid/{locale}.json",
      "default": "en",
      "locales": ["en", "pt-BR", "de"]
  }
}
```

de.json

```
{
    "sip": 16,
    "attributes: [
        {
          "trait_type": "Haare",
          "value": "rot",
        },
        {
          "trait_type": "Stärke",
          "display_type": "number",
          "value": 99,
        },
    ]
}
```

pt-BR.json

```
{
    "sip": 16,
    "attributes: [
        {
          "trait_type": "cabelos",
          "value": "vermelho",
        },
        {
          "trait_type": "força",
          "display_type": "number",
          "value": 99,
        },
    ]
}
```

### Properties

Common properties of tokens are described in appendix C. Properties of type
`object` are usually described using the following schema:

```
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "title": "Token Metadata Property",
    "type": "object",
    "required": [],
    "properties": {
        "type": {
            "type": "string",
            "description": "type of the property"
        },
        "description": {
            "type": "string",
            "description": "description of the property"
        },
        "value": {
            "type": {"oneOf": [{"type": "object"}, {"type": "string"}, {"type": "number"}, {"type": "integer"}, {"type": "boolean"} {"type: "array"}},
            "description": "value of the property"
        }
    }
}
```

Example:

```
{
    "type": "string",
    "description": "Address of custodian key holder",
    "value": "Casa Inc., P.O. Box 20575, Charleston, S.C. 29413, United States."
}
```

### Attributes

Attributes describe additional elements of tokens that are "observable", usually
represented in the associated image or digital asset of the token. In contrast,
`properties` describe elements of tokens that are more abstract and not visible
in the associated image of the token.

Images of NFTs often have a limited set of traits and each trait has a limited
number of possible values. These values are represented as attributes in the
metadata. They can be used to calculate a score for each NFT in the collection
that could define the rarity of the NFT.

An `attribute` consists of a `trait_type` defining the name of the trait, e.g.
"hair". The `value` is the value of the trait, e.g. "red". The `display_type` is
a field indicating how the trait value should be displayed, e.g. on a
marketplace. If `display_type` is omitted, then `string` is used as default
display type.

Appendix B describes the possible types and display types of attributes.

## Localization

The localized data follow the same JSON schema with property `sip` as required
and all other properties as optional.

The localized data overwrite data provided in the default metadata JSON. The
localized data can provide only partial data.

An array of localized `attributes` overwrites the whole list of default
`attributes`.

A localized properties with partial data overwrites only the provided
properties; the remaining default properties remain as default values.

# Using metadata in applications

An application like a marketplace uses metadata to present tokens to users.
Before doing so, application developers should verify whether the metadata is
compliant with their own application's guidelines, e.g. forbidding bad language
in names or unsuitable images.

We remind implementation authors that the empty string for the token URI is a
valid response. We also remind everyone that any smart contract can use the same
metadata as other NFT contracts. It is out of the scope of this standard to
define how a client may determine which smart contracts are the original,
well-known, canonical ones.

## Graphical representation

The metadata of a token contain several properties that can be used to visually
represent the token. It is recommended to consider the first defined property of
the following ordered list:

1. `image`
2. `properties.image_data`
3. `image_data`

A rich representation should use the first defined property of the following
list:

1. `properties.animation_url`
2. `animation_url`

# Out of Scope

Accessiblity of content is not covered by the standard.

Properties other than resolvability of the token URI are out of scope. This
implies that metadata might change over time (stability).

# Metadata functions

Some contracts have dedicated functions to provide some metadata directly
without resolving the token URI. This is usually necessary, if other contracts
need to use the token metadata. This SIP does not define signatures for these
functions.

Examples of contracts with metadata functions are listed below:

**Boom**

The [NFT contract for
Boom](https://explorer.stacks.co/txid/0x423d113e14791f5d60f5efbee19bbb05cf5e68d84bcec4e611f2c783b08d05aa?chain=mainnet)
implements a variation of this trait using similar naming, but returning other
types than response types.

The function signatures for metadata are:

- `(get-boom-meta () {uri: (string-ascii 35), name: (string-ascii 16),
  mime-type: (string-ascii 9)})` and
- `(get-meta? uint {series-id: uint, number: uint, name: (string-utf8 80), uri:
  (string-ascii 2048), mime-type: (string-ascii 129), hash: (buff 64)})`

**Badges**

The [badges
contract](https://explorer.stacks.co/txid/0xb874ddbb4a602c22bb5647c7a2f8bfcafbbca7c0c663a175f2270ef3665f33de?chain=mainnet)
defines metadata for nfts.

The function signatures for metadata are:

- `(get-badge-meta () {uri: (string-ascii 78111)})` and
- `(get-meta? (uint) (optional {user: principal}))`

# Backwards Compatibility

This SIP defines metadata so that metadata for existing NFTs on other
blockchains like Ethereum, Solana or WAX can be re-used for NFTs on the Stacks
blockchain.

# Related Work

NFTs are an established asset class on blockchains. Read for example
[here](https://www.ledger.com/academy/what-are-nft).

## BNS

The Blockchain Naming System uses native non-fungible tokens. It does define
metadata for a name through attachements. The schema for names owned by a person
follows the definition of (schema.org/Person)[https://schema.org/Person]. This
could be an alternative to token URIs.

## EIP 721 and 1155

Metadata for NFTs on Ethereum are defined in [EIP
721](https://eips.ethereum.org/EIPS/eip-721) and [EIP
1155](https://eips.ethereum.org/EIPS/eip-1155). The JSON schema for SIP-016 has
adopted the EIP 1155 schema with the following differences:

- substitution of `{id}` strings must use the decimal format not the hexdecimal,
  zero-padded format.

- properties of type object should use property `value` for the value, not
  property `description` as used by some EIP-1155 NFTs.

## Metaplex

The tool suite Metaplex for NFTs on Solana defines a [JSON
schema](https://docs.metaplex.com/nft-standard#uri-json-schema). The properties
`category` and `files` in Appendic C were inspired by that schema.

## Hedera

Hedera follows the same schema defined in
[H-10](https://github.com/hashgraph/hedera-improvement-proposal/blob/master/HIP/hip-10.md).

# Activation

This SIP is activated if 10 contracts are deployed that follows this
specification. This must happen before Bitcoin tip #750,000.

# Appendix A

List of trait function define in SIPs and specifications specific to these
functions

| SIP and Trait Function Name                            | Definition of "existing"                                                                        | Additional Specification for Properties                                                                                                                                                                                    | Identifier Parameter |
| ------------------------------------------------------ | ----------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------- |
| SIP-009 nft-trait.get-token-uri                        | token must be minted and not burnt                                                              | NFTs belonging to a group of tokens should use property `properties.collection` of type `string` for the collection name. <br/> Optional property `properties.id` of type `integer` describes the identifier of the token. | 1st                  |
| SIP-016 get-contract-uri                               | always                                                                                          | `properties.items` of type array can be used to provide the metadata of all tokens belonging to the collection                                                                                                             | X                    |
| SIP-010 ft-trait.get-token-uri                         | always                                                                                          | The required property `decimals` of type `integer` must be the same number as `get-decimals`.                                                                                                                              | X                    |
| SIP-013 sip013-semi-fungible-token-trait.get-token-uri | token must be minted and not burnt, no requirements on the number of fungible part of the token |                                                                                                                                                                                                                            | 1st                  |

# Appendix B

Attribute types

| Type    | Display types                                | Additional Properties | Comment                  |
| ------- | -------------------------------------------- | --------------------- | ------------------------ |
| Numeric | `number`, `boost_percentage`, `boost_number` | `max_value`           |                          |
| Date    | `date`                                       |                       | As unix timestamp in UTC |
| String  | empty                                        |                       |                          |

# Appendic C

Common Properties with predefined types.

| Name                            | Type      | Description                                                                                                                                                                                                                                                                                                                                                         |
| ------------------------------- | --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `animation_url`                 | `string`  | url to a multi-media attachment for the item. Application might use this to display the token in a richer way than the image of the `image` property. Application might support media types like GLTF, GLB, WEBM, MP4, M4V, OGV, and OGG, MP3, WAV, and OGA as well as HTML. The query `?ext={file_extension}` can be used to provide information on the file type. |
| `artist_name`                   | `string`  | name of the artist, mainly used as attribution.                                                                                                                                                                                                                                                                                                                     |
| `category`                      | `string`  | category of the associated media file, e.g. `image`, `video`, `audio`, `vr`, `html`.                                                                                                                                                                                                                                                                                |
| `collection`                    | `string`  | collection name the token belongs to. See also Appendix A.                                                                                                                                                                                                                                                                                                          |
| `collection_image`              | `string`  | url to an image representing the collection.                                                                                                                                                                                                                                                                                                                        |
| `created`                       | `integer` | creation date of the token in unix timestamp                                                                                                                                                                                                                                                                                                                        |
| `creators`                      | `array`   | list of creators and their shares, represented as `{address: string, share: integer}`. Shares are represented as percentage. The sum of shares of all creators must add up to 100. Shares can be used to define royalties.                                                                                                                                          |
| `decimals`                      | `integer` | number of decimals. See also Appendix A.                                                                                                                                                                                                                                                                                                                            |
| `external_url`                  | `string`  | url that will view the token on an external site                                                                                                                                                                                                                                                                                                                    |
| `files`                         | `array`   | list of all associated files, represented as `{uri: string, type: string, signature: string, signature_type: string}`.                                                                                                                                                                                                                                              |
| `id`                            | `integer` | identifier for NFTs. See also Appendix A.                                                                                                                                                                                                                                                                                                                           |
| `image_data`                    | `string`  | raw SVG image data.                                                                                                                                                                                                                                                                                                                                                 |
| `ip_document_uri`               | `string`  | link to document about intellectual property (IP) rights                                                                                                                                                                                                                                                                                                            |
| `raw_media_file_signature`      | `string`  | signature of the media file represented by the token                                                                                                                                                                                                                                                                                                                |
| `raw_media_file_signature_type` | `string`  | signature type of the media represented by the token, e.g. SHA-256                                                                                                                                                                                                                                                                                                  |
| `raw_media_file_type`           | `string`  | MIME type of the media represented by the token                                                                                                                                                                                                                                                                                                                     |
| `raw_media_file_uri`            | `string`  | uri of the media represented by the token                                                                                                                                                                                                                                                                                                                           |
| `seed`                          | `string`  | a string representing of the uniqueness of the NFT, like a DNA. The seed is usually stored on-chain, but it might be contained in this metadata for convenience.                                                                                                                                                                                                                              |
| `symbol`                        | `string`  | token symbol                                                                                                                                                                                                                                                                                                                                                        |
| `total_supply`                  | `integer` | number of total supply, e.g. minted tokens                                                                                                                                                                                                                                                                                                                          |

          
# Preamble

SIP Number: 006

Title: Clarity Execution Cost Assessment

Author: Aaron Blankstein <aaron@blockstack.com>, Reed Rosenbluth <reed@blockstack.com>

Consideration: Technical

Type: Consensus

Status: Ratified

Created: 19 October 2019

License: BSD 2-Clause

Sign-off: Jude Nelson <jude@stacks.org>, Technical Steering Committee Chair

Discussions-To: https://github.com/stacksgov/sips

# Abstract

This document describes the measured costs and asymptotic costs
assessed for the execution of Clarity code. This will not specify the
_constants_ associated with those asymptotic cost functions. Those
constants will necessarily be measured via benchmark harnesses and
regression analyses. Furthermore, the _analysis_ cost associated with
this code will not be covered by this proposal.

The asymptotic cost functions for Clarity functions are modifiable
via an on-chain voting mechanism. This enables new native functions to
be added to the language over time.

This document also describes the memory limit imposed during contract
execution, and the memory model for enforcing that limit.

# Introduction

Execution cost of a block of Clarity code is broken into 5 categories:

1. Runtime cost: captures the number of cycles that a single
   processor would require to process the Clarity block. This is a
   _unitless_ metric, so it will not correspond directly to cycles,
   but rather is meant to provide a basis for comparison between
   different Clarity code blocks.
2. Data write count: captures the number of independent writes
   performed on the underlying data store (see SIP-004).
3. Data read count: captures the number of independent reads
   performed on the underlying data store.
4. Data write length: the number of bytes written to the underlying
   data store.
5. Data read length: the number of bytes read from the underlying
   data store.

Importantly, these costs are used to set a _block limit_ for each
block.  When it comes to selecting transactions for inclusion in a
block, miners are free to make their own choices based on transaction
fees, however, blocks may not exceed the _block limit_. If they do so,
the block is considered invalid by the network --- none of the block's
transactions will be materialized and the leader forfeits all rewards
from the block.

## Static versus Dynamic Cost Assessment

Tracking the execution cost of a contract may be done either dynamically
or statically. Dynamic cost assessment involves tracking, at the VM level,
the various metrics as a contract is executed. Static cost assessment is
performed via analysis of the contract source code, and is inherently
a more pessimistic accounting of the execution cost: list operations
are charged according to the _maximum_ size of the list (per the type
annotations and inferences from the source code) and branching statements
are charged according to the _maximum_ branch cost (per metric tracked, i.e.,
if one branch performs 1 write and has a runtime cost of 1, and another
branch performs 0 writes and has a runtime cost of 2, the whole statement
will be assessed as having a maximum of 1 write and runtime cost of 2).

# Specification

## Costs of Common Operations

### Variable Lookup

Looking up variables in Clarity incurs a non-constant cost -- the stack
depth _and_ the length of the variable name affect this cost. However,
variable names in Clarity have bounded length -- 128 characters. Therefore,
the cost assessed for variable lookups may safely be constant with respect
to name length.

The stack depth affects the lookup cost because the variable must be
checked for in each context on the stack.

The cost model of Clarity depends on a copy-on-read semantic for
objects. This allows operations like appends, matches, wrapping/unwrapping,
to be constant cost, but it requires that variable lookups be charged for
copies.

Cost Function:

```
a*X+b*Y+c
```

a, b, and c are constants \
X := stack depth \
Y := variable size

### Function Lookup

Looking up a function in Clarity incurs a constant cost with respect
to name length (for the same reason as variable lookup). However,
because functions may only be defined in the top-level contract
context, stack depth does not affect function lookup.

Cost Function:

```
a
```

a is a constant.

### Name Binding

The cost of binding a name in Clarity -- in either a local or the contract
context is _constant_ with respect to the length of the name:

```
binding_cost = a
```

a is a constant

### Function Application

Function application in Clarity incurs a cost in addition to the
cost of executing the function's body. This cost is the cost of
binding the arguments to their passed values, and the cost of
ensuring that those arguments are of the correct type. Type checks
and argument binding are _linear_ in the size of the arguments.

The cost of applying a function is:


```
(a*X+b) + costEval(body)
```

a and b are constants \
X := the cumulative size of the argument types \
`costEval(body)` := the cost of executing the body of the function

### contract-call Transactions

User-signed transactions for contract-calls are charged for the
application of the function, as well as the loading of the contract
data. This charge is the same as a normal contract-call. _However_,
contract principals that are supplied as trait arguments must be
checked by the runtime system to ensure that they validly implement
the trait. The cost of this check is:

```
read_count = 2
read_length = trait_size + contract_size
runtime_cost = a*(contract_size) + b*(trait_size) + c
```

This check needs to read the trait, and then validate that the supplied
contract fulfills that trait by reading the contract in, and checking
the method signatures. This check must be performed for each such
trait parameter.

### Type Parsing

Parsing a type in Clarity incurs a linear cost in the size of the
AST describing the type:

```
type_parsing_cost(X) = (a*X+b)
```

a, b, are constants \
X := the number of elements in the type description AST

The type description AST is the tree of Clarity language elements used
for describing the type, e.g.:

* `(list 1 uint)` - this AST has four elements: `list`, `1`, `uint`
  and the parentheses containing them.
* `(response bool int)` - this AST has four elements: `response`, `bool`, `int`
  and the parentheses containing them.
* `int` - this AST is just one component.

### Function Definition

Defining a function in Clarity incurs an execution cost at the
time of contract publishing (unrelated to any analysis). This
is the cost of _parsing_ the function's signature, which is linear
in the length of the type signatures, and linear in the length of the
function name and argument names.

```
binding_cost + sum(a + type_parsing_cost(Y) for Y in ARG_TYPES)
```

`type_parsing_cost(Y)` := the cost of parsing argument Y \
ARG_TYPES := the function definition's argument type signatures \
a is a constant associated with the binding of argument types

### Contract Storage Cost

Storing a contract incurs both a runtime cost as well as storage costs. Both of
these are _linear_ the size of the contract AST.

```
WRITE_LENGTH = a*X+b
RUNTIME_COST = c*X+d
```

a, b, c, and d, are constants.

## Initial Native Function Costs

These are the initial set values for native function costs, however, these
can be changed as described below in the [Cost Upgrades](#cost-upgrades)
section of this document.

### Data, Token, Contract-Calls

#### Data Lookup Costs

Fetching data from the datastore requires hashing the key to be looked up.
That cost is linear in the key size:

```
data_hash_cost(X) = a*X+b
```

X := size of the key

#### Data Fetching Costs

Fetching data from the datastore incurs a runtime cost, in addition to
any costs associated with MARF accesses (which are simply counted as the
integer number of times the MARF is accessed). That runtime cost
is _linear_ in the size of the fetched value (due to parsing).

```
read_data_cost = a*X+b
```

X := size of the fetched value.

#### Data Writing Costs

Writing data to the datastore incurs a runtime cost, in addition to
any costs associated with MARF writes (which are simply counted as the
integer number of times the MARF is written). That runtime cost
is _linear_ in the size of the written value (due to data serialization).

```
write_data_cost = a*X+b
```

X := size of the stored value.

#### contract-call

Contract calls incur the cost of a normal function lookup and
application, plus the cost of loading that contract into memory from
the data store (which is linear in the size of the called contract).

```
RUNTIME_COST: (a*Y+b) + func_lookup_apply_eval(X)
READ_LENGTH: Y
```

a and b are constants \
Y := called contract size \
`func_lookup_apply_eval(X)` := the cost of looking up, applying, and
evaluating the body of the function


Note that contract-calls that use _trait_ definitions for dynamic dispatch
are _not_ charged at a different cost rate. Instead, there is a cost for
looking up the trait variable (assessed as a variable lookup), and the cost
of validating any supplied trait implementors is assessed during a transaction's
argument validation.

#### map-get

```
RUNTIME_COST: data_hash_cost(X+Y) + read_data_cost(Z)
READ_LENGTH:  Z
```

X := size of the map's _key_ tuple \
Y := the length of the map's name \
Z := the size of the map's _value_ tuple


#### contract-map-get

```
RUNTIME_COST: data_hash_cost(X) + read_data_cost(Z)
READ_LENGTH:  Z
```

X := size of the map's _key_ tuple \
Z := the size of the map's _value_ tuple

#### map-set

```
RUNTIME_COST: data_hash_cost(X+Y) + write_data_cost(Z)
WRITE_LENGTH:  Z
```

X := size of the map's _key_ tuple \
Y := the length of the map's name \
Z := the size of the map's _value_ tuple

#### map-insert

```
RUNTIME_COST: data_hash_cost(X+Y) + write_data_cost(Z)
WRITE_LENGTH:  Z
```

X := size of the map's _key_ tuple \
Y := the length of the map's name \
Z := the size of the map's _value_ tuple

#### map-delete

```
RUNTIME_COST: data_hash_cost(X+Y) + write_data_cost(1)
WRITE_LENGTH:  1
```

X := size of the map's _key_ tuple \
Y := the length of the map's name \
Y := the length of the map's name

#### var-get

```
RUNTIME_COST: data_hash_cost(1) + read_data_cost(Y)
READ_LENGTH: Y
```

Y := the size of the variable's _value_ type

#### var-set

```
RUNTIME_COST: data_hash_cost(1) + write_data_cost(Y)
WRITE_LENGTH: Y
```

Y := the size of the variable's _value_ type

#### nft-mint

```
RUNTIME_COST: data_hash_cost(Y) + write_data_cost(a) + b
WRITE_LENGTH: a
```

Y := size of the NFT type \
a is a constant: the size of a token owner \
b is a constant cost (for tracking the asset in the assetmap)

#### nft-get-owner

```
RUNTIME_COST: data_hash_cost(Y) + read_data_cost(a)
READ_LENGTH: a
```

Y := size of the NFT type \
a is a constant: the size of a token owner


#### nft-transfer

```
RUNTIME_COST: data_hash_cost(Y) + write_data_cost(a) + write_data_cost(a) + b
READ_LENGTH: a
WRITE_LENGTH: a
```

Y := size of the NFT type \
a is a constant: the size of a token owner \
b is a constant cost (for tracking the asset in the assetmap)

#### ft-mint
 
Minting a token is a constant-time operation that performs a constant
number of reads and writes (to check the total supply of tokens and
incremement).

```
RUNTIME: a
READ_LENGTH: b
WRITE_LENGTH: c
```
a, b, and c are all constants.

#### ft-transfer

Transfering a token is a constant-time operation that performs a constant
number of reads and writes (to check the token balances).

```
RUNTIME: a
READ_LENGTH: b
WRITE_LENGTH: c
```
a, b, and c are all constants.

#### ft-get-balance

Getting a token balance is a constant-time operation that performs a
constant number of reads.

```
RUNTIME: a
READ_LENGTH: b
```
a and b are constants.

#### get-block-info

```
RUNTIME: a
READ_LENGTH: b
```

a and b are constants.

## Control-Flow and Context Manipulation

### let

In addition to the cost of evaluating the body expressions of a `let`,
the cost of a `let` expression has a constant cost, plus
the cost of binding each variable in the new context (similar
to the cost of function evaluation, without the cost of type checks).


```
a + b * Y + costEval(body) + costEval(bindings)
```

a and b are constants \
Y := the number of let arguments \
`costEval(body)` := the cost of executing the body of the let \
`costEval(bindings)` := the cost of evaluating the value of each let binding

### if

```
a + costEval(condition) + costEval(chosenBranch)
```

a is a constant \
`costEval(condition)` := the cost of evaluating the if condition \
`costEval(chosenBranch)` := the cost of evaluating the chosen branch

if computed during _static analysis_, the chosen branch cost is the
`max` of the two possible branches.

### asserts!

```
a + costEval(condition) + costEval(throwBranch)
```

a is a constant \
`costEval(condition)` := the cost of evaluating the asserts condition \
`costEval(throwBranch)` := the cost of evaluating the throw branch in
the event that condition is false

if computed during _static analysis_, the thrown branch cost is always
included.

## List and Buffer iteration
### append

The cost of appending an item to a list is the cost of checking the
type of the added item, plus some fixed cost.

```
a + b * X
```

a and b is a constant \
X := the size of the list _entry_ type

### concat

The cost of concatting two lists or buffers is linear in
the size of the two sequences:

```
a + b * (X+Y)
```

a and b are constants \
X := the size of the right-hand iterable \
Y := the size of the left-hand iterable

### as-max-len?

The cost of evaluating an `as-max-len?` function is constant (the function
is performing a constant-time length check)

### map

The cost of mapping a list is the cost of the function lookup,
and the cost of each iterated function application

```
a + func_lookup_cost(F) + L * apply_eval_cost(F, i)
```

a is a constant \
`func_lookup_cost(F)` := the cost of looking up the function name F \
`apply_eval_cost(F, i)` := the cost of applying and evaluating the body of F on type i \
`i` := the list _item_ type \
`L` := the list length 

If computed during _static analysis_, L is the maximum length of the list
as specified by it's type.

### filter

The cost of filtering a list is the cost of the function lookup,
and the cost of each iterated filter application

```
a + func_lookup_cost(F) + L * apply_eval_cost(F, i)
```

a is a constant \
`func_lookup_cost(F)` := the cost of looking up the function name F \
`apply_eval_cost(F, i)` := the cost of applying and evaluating the body of F on type i \
`i` := the list _item_ type \
`L` := the list length

If computed during _static analysis_, L is the maximum length of the list
as specified by it's type.

### fold


The cost of folding a list is the cost of the function lookup,
and the cost of each iterated application

```
a + func_lookup_cost(F) + (L) * apply_eval_cost(F, i, j)
```

a is a constant \
`func_lookup_cost(F)` := the cost of looking up the function name F \
`apply_eval_cost(F, i, j)` := the cost of applying and evaluating the body of F on types i, j \
`j` := the accumulator type \
`i` := the list _item_ type \
`L` := the list length 

If computed during _static analysis_, L is the maximum length of the list
as specified by it's type.

### len

The cost of getting a list length is constant, because Clarity lists
store their lengths.

### list

The cost of constructing a new list is linear -- Clarity ensures that
each item in the list is of a matching type.

```
a*X+b
```

a and b are constants \
X := the total size of all arguments to the list constructor

### tuple

The cost of constructing a new tuple is `O(nlogn)` with respect to the number of
keys in the tuple (because tuples are represented as BTrees).

```
a*(X*log(X)) + b
```

a and b are constants \
X := the number of keys in the tuple

### get

Reading from a tuple is `O(nlogn)` with respect to the number of
keys in the tuple (because tuples are represented as BTrees).

```
a*(X*log(X)) + b
```

a and b are constants \
X := the number of keys in the tuple

## Option/Response Operations

### match

Match imposes a constant cost for evaluating the match, a cost for checking
that the match-bound name does not _shadow_ a previous variable. The
total cost of execution is:

```
a + evalCost(chosenBranch) + cost(lookupVariable)
```

where a is a constant, and `chosenBranch` is whichever branch
is chosen by the match. In static analysis, this will be:
`max(branch1, branch2)` 

### is-some, is-none, is-error, is-okay

These check functions all have constant cost.

### unwrap, unwrap-err, unwrap-panic, unwrap-err-panic, try!

These functions all have constant cost.

## Arithmetic and Logic Operations

### Variadic operators

The variadic operators (`+`,`-`,`/`,`*`, `and`, `or`) all have costs linear
in the _number_ of arguments supplied

```
(a*X+b)
```

X := the number of arguments

### Binary/Unary operators

The binary and unary operators:

```
>
>=
<
<=
mod
pow
xor
not
to-int
to-uint
```

all have constant cost, because their inputs are all of fixed sizes.

### Hashing functions

The hashing functions have linear runtime costs: the larger the value being
hashed, the longer the hashing function takes.

```
(a*X+b)
```

X := the size of the input.


## Memory Model and Limits

Clarity contract execution imposes a maximum memory usage limit for applications.
For any given Clarity value, the memory usage of that value is counted using
the _size_ of the Clarity value.

Memory is consumed by the following variable bindings:

* `let` - each value bound in the `let` consumes that amount of memory
    during the execution of the `let` block.
* `match` - the bound value in a `match` statement consumes memory during
    the execution of the `match` branch.
* function arguments - each bound value consumes memory during the execution
    of the function. this includes user-defined functions _as well as_ native
    functions.

Additionally, functions that perform _context changes_ also consume memory,
though they consume a constant amount:

* `as-contract`
* `at-block`

### Type signature size

Types in Clarity may be described using type signatures. For example,
`(tuple (a int) (b int))` describes a tuple with two keys `a` and `b`
of type `int`. These type descriptions are used by the Clarity analysis
passes to check the type correctness of Clarity code. Clarity type signatures
have varying size, e.g., the signature `int` is smaller than the signature for a
list of integers.

The size of a Clarity value is defined as follows:

```
type_size(x) :=
  if x = 
     int        => 16
    uint        => 16
    bool        => 1
    principal   => 148
    (buff y)    => 4 + y
    (some y)    => 1 + size(y)
    (ok y)      => 1 + size(y)
    (err y)     => 1 + size(y)
    (list ...)  => 4 + sum(size(z) for z in list)
    (tuple ...) => 1 + 2*(count(entries)) 
                     + sum(size(z) for each value z in tuple)
```

### Contract Memory Consumption

Contract execution requires loading the contract's program state in
memory. That program state counts towards the memory limit when
executed via a programmatic `contract-call!` or invoked by a
contract-call transaction.

The memory consumed by a contract is equal to:

```
a + b*contract_length + sum(size(x) for each constant x defined in the contract)
```

That is, a contract consumes memory which is linear in the contract's
length _plus_ the amount of memory consumed by any constants defined
using `define-constant`.

### Database Writes

While data stored in the database itself does _not_ count against the
memory limit, supporting public function abort/commit behavior requires
holding a write log in memory during the processing of a transaction.

Operations that write data to the data store therefore consume memory
_until the transaction completes_, and the write log is written to the
database. The amount of memory consumed by operations on persisted data
types is defined as:

* `data-var`: the size of the stored data var's value.
* `map`: the size of stored key + the size of the stored value.
* `nft`: the size of the NFT key
* `ft`: the size of a Clarity uint value.

## Cost Upgrades

In order to enable the addition of new native functions to the Clarity
language, there is a mechanism for voting on and changing a function's
_cost-assessment function_ (the asymptotic function that describes its
complexity and is used to calculate its cost). 

New functions can be introduced to Clarity by first being implemented
_in_ Clarity and published in a contract. This is necessary to avoid
a hard fork of the blockchain and ensure the availability of the function
across the network.

If it is decided that a published function should be a part of the Clarity
language, it can then be re-implemented as a native function in Rust and
included in a future release of Clarity. Nodes running this upgraded VM
would instead use this new native implementation of the function when they
encounter Clarity code that references it via `contract-call?`.

The new native function is likely faster and more efficient than the
prior Clarity implementation, so a new cost-assessment function may need
to be chosen for its evaluation. Until a new cost-assessment function is
agreed upon, the cost of the Clarity implementation will continue to be used.

### Voting on the Cost of Clarity Functions

New and more accurate cost-assessment functions can be agreed upon as
follows:

1. A user formulates a cost-assessment function and publishes it in a
Clarity contract as a `define-read-only` function.
2. The user proposes the cost-assessment function by calling the
`submit-proposal` function in the **Clarity Cost Voting Contract**.
3. Voting on the proposed function ensues via the voting functions in
the **Clarity Cost Voting Contract**, and the function is either
selected as a replacement for the existing cost-assessment function,
or ignored.

#### Publishing a Cost-Assessment Function

Cost-assessment functions to be included in proposals can be published
in Clarity contracts. They must be written precisely as follows:

1. The function must be `read-only`.
2. The function must return a tuple with the keys `runtime`, `write_length`,
`write_count`, `read_count`, and `read_length` (the execution cost measurement
[categories](#measurements-for-execution-cost)).
3. The values of the returned tuple must be Clarity expressions representing
the asymptotic cost functions. These expressions must be limited to **arithmetic operations**.
4. Variables used in these expressions must be listed as arguments to the Clarity
function.
5. Constant factors can be set directly in the code.

For example, suppose we have a function that implements a sorting algorithm:

```lisp
(define-public (quick-sort (input (list 1024 int))) ... )
```

The cost-assessment function should have _one_ argument, corresponding to the size
of the `input` field, e.g.

```lisp
(define-read-only (quick-sort-cost (n uint))
  {
    runtime: (* n (log n)),
    write_length: 0,
    write_count: 0,
    read_count: 0,
    read_length: 0,
  })
```

Here's another example where the cost function contains constant factors:

```lisp
(define-read-only (quick-sort-cost (n uint))
   {
     runtime: (+ 30 (* 2 n (log n))),
     write_length: 0,
     write_count: 0,
     read_count: 0,
     read_length: 0
   })
```

#### Making a Cost-Assessment Function Proposal

Stacks holders can propose cost-assessment functions by calling the
`submit-proposal` function in the **Clarity Cost Voting Contract**.

```Lisp
(define-public (submit-proposal
    (function-contract principal)
    (function-name (string-ascii 128))
    (cost-function-contract principal)
    (cost-function-name (string-ascii 128))
    ...
)
```

Description of `submit-proposal` arguments:
- `function-contract`: the principal of the contract that defines
the function for which a cost is being proposed
- `function-name`: the name of the function for which a cost is being proposed
- `cost-function-contract-principal`: the principal of the contract that defines
the cost function being proposed
- `cost-function-name`: the name of the cost-function being proposed

If submitting a proposal for a native function included in Clarity at boot,
provide the principal of the boot costs contract for the `function-contract`
argument, and the name of the corresponding cost function for the `function-name`
argument.

This function will return a response containing the proposal ID, if successful,
and an error otherwise.

Usage:
```Lisp
(contract-call?
    .cost-voting-contract
    "submit-proposal"
    .function-contract
    "function-name"
    .new-cost-function-contract
    "new-cost-function-name"
)
```

#### Viewing a Proposal

To view cost-assessment function proposals, you can use the 
`get-proposal` function in the **Clarity Cost Voting Contract**:

```Lisp
(define-read-only (get-proposal (proposal-id uint)) ... )
```

This function takes a `proposal-id` and returns a response containing the proposal
data tuple, if a proposal with the supplied ID exists, or an error code. Proposal
data tuples contain information about the state of the proposal, and take the
following form:

```Lisp
{
    cost-function-contract: principal,
    cost-function-name: (string-ascii 128),
    function-contract: principal,
    function-name: (string-ascii 128),
    expiration-block-height: uint
}
```

Usage:
```Lisp
(contract-call? .cost-voting-contract "get-proposal" 123)
```

### Voting

#### Stacks Holders

Stacks holders can vote for cost-assessment function proposals by calling the
**Clarity Cost Voting Contract's** `vote-proposal` function. The `vote-proposal`
function takes two arguments, `proposal-id` and `amount`. `proposal-id` is the ID
of the proposal being voted for. `amount` is the amount of STX the voter would like
to vote *with*. The amount of STX you include is the number of votes you are casting.

Calling the `vote` function authorizes the contract to transfer STX out
of the caller's address, and into the address of the contract. The equivalent
amount of `cost-vote-tokens` will be distributed to the voter, representing the
amount of STX they have staked in the voting contract.

STX staked for voting can be withdrawn from the voting contract by the voter with
the `withdraw-votes` function. If staked STX are withdrawn prior to confirmation,
they will not be counted as votes.

Upon withdrawal, the voter permits the contract to reclaim allocated `CFV` tokens,
and will receive the equivalent amount of their staked STX tokens.

**Voting example**
```Lisp
(contract-call? .cost-voting-contract "vote-proposal" 123 10000)
```

The `vote-proposal` function will return a successful response if the STX were staked
for voting, or an error if the staking failed.

**Withdrawal example**
```Lisp
(contract-call? .cost-voting-contract "withdraw-votes" 123 10000)
```

Like the `vote-proposal` function, the `withdraw-votes` function expects a `proposal-id` and
an `amount`, letting the voter withdraw some or all of their staked STX. This function
will return a successful response if the STX were withdrawn, and an error otherwise.

#### Miner Veto

Miners can vote *against* (veto) cost-function proposals by creating a transaction that
calls the **Clarity Cost Voting Contract's** `veto` function and mining
a block that includes this transaction. The `veto` function won't count
the veto if the block wasn't mined by the node that signed that transaction.
In other words, miners must **commit** their veto with their mining power.

Usage:
```Lisp
(contract-call? .cost-voting-contract "veto" 123)
```

This function will return a successful response if the veto was counted, or
an error if the veto failed.

### Confirming the Result of a Vote

In order for a cost-function proposal to get successfully voted in, it must be
**confirmed**. Confirmation is a two step process, involving calling the `confirm-votes`
function _before_ the proposal has expired to confirm the vote threshold was met,
and calling the `confirm-miners` function _after_ to confirm that the proposal wasn't vetoed
by miners.

#### Confirm Votes

Any stacks holder can call the `confirm-votes` function in the **Clarity
Cost Voting Contract** to attempt confirmation. `confirm-votes` will return a
success response and become **vote confirmed** if the following criteria are met.

1. The proposal must receive votes representing 20% of the liquid supply of STX.
This is calculated like such:
   
```lisp
(>= (/ (* votes u100) stx-liquid-supply) u20)
```

2. The proposal must not be expired, meaning its `expiration-height` must
not have been reached.
   
Usage:
```Lisp
(contract-call? .cost-voting-contract "confirm-votes" 123)
```

#### Confirm Miners

Like `confirm-votes`, any stacks holder can call the `confirm-miners` function.
`confirm-miners` will return a success response and the proposal will become
**miner confirmed** if the following criteria are met:

1. The number of vetos is less than 500.
2. There have been less than 10 proposals already confirmed in the current block.
3. The proposal has expired.

Usage:
```Lisp
(contract-call? .cost-voting-contract "confirm-miners" 123)
```

# Related Work

This section will be expanded upon after this SIP is ratified.

# Backwards Compatibility

All Stacks accounts from Stacks 1.0 will be imported into Stacks 2.0 when it
launches.  The state of the Stacks 1.0 chain will be snapshotted at Bitcoin
block 665750.

# Activation

At least 20 miners must register a name in the `.miner` namespace in Stacks 1.0.
Once the 20th miner has registered, the state of Stacks 1.0 will be snapshotted.
300 Bitcoin blocks later (Bitcoin block 666050), the Stacks 2.0 blockchain will launch.  Stacks 2.0
implements this SIP.

# Reference Implementations

Implemented in Rust.  See https://github.com/blockstack/stacks-blockchain.


          
# Preamble

SIP Number: 019

Title: Notifications for Token Metadata Updates

Author: Rafael Cárdenas (rafael@hiro.so), Matthew Little (matt@hiro.so)

Consideration: Technical

Type: Standard

Status: Ratified

Created: 17 May 2022

License: GPL-3.0

Sign-off: Jude Nelson (jude@stacks.org), Aaron Blankstein (aaron@hiro.so), Marvin Janssen (http://github.com/MarvinJanssen)

Layer: Traits

# Abstract

As the use of tokens (fungible and non-fungible) has grown in popularity, Stacks developers have
found novel ways to define and use metadata to describe them. This rich data is commonly cached and
indexed for future use in applications such as marketplaces, statistics aggregators, and developer
tools like the [Stacks Blockchain API](https://github.com/hirosystems/stacks-blockchain-api).

Occasionally, however, this metadata needs to change for a number of reasons: artwork reveals, media
storage migrations, branding updates, etc. As of today, these changes do not have a standardized way
of being propagated through the network for indexers to refresh their cache, so the display of stale
metadata is a very common problem.

This SIP aims to define a simple mechanism for developers to notify the Stacks network when metadata
for a token has changed, so interested parties can refresh their cache and display up-to-date
information in their applications.

# Introduction

Smart contracts that declare NFTs, FTs and SFTs conform to a standard set of traits used to describe
each token (see [SIP-009](../sip-009/sip-009-nft-standard.md),
[SIP-010](../sip-010/sip-010-fungible-token-standard.md) and
[SIP-013](https://github.com/stacksgov/sips/blob/main/sips/sip-013/sip-013-semi-fungible-token-standard.md)).
One of these traits is `get-token-uri`, which should return a URI string that resolves to a token's
metadata usually in the form of a JSON file. There is currently no defined structure for this data,
and it is not considered to be immutable.

To illustrate a common use of `get-token-uri`, we'll look at the
[`SPSCWDV3RKV5ZRN1FQD84YE1NQFEDJ9R1F4DYQ11.newyorkcitycoin-token-v2`](https://explorer.stacks.co/txid/0x969192220b1c478ef9d18d1cd413d7c79fe02937a9b33af63d441bd5519d1715?chain=mainnet)
contract which declares the NewYorkCityCoin fungible token.

At the time of writing, the value returned by this contract for `get-token-uri` is the string:
```
"https://cdn.citycoins.co/metadata/newyorkcitycoin.json"
```
When this URI is resolved, it returns a JSON file with the following metadata:
```json
{
  "name": "NewYorkCityCoin",
  "description": "A CityCoin for New York City, ticker is NYC, Stack it to earn Stacks (STX)",
  "image": "https://cdn.citycoins.co/logos/newyorkcitycoin.png"
}
```
Even though the URI string is fixed, this file lives off-chain so it is conceivable that its
contents could change at any point in the future. Additionally, this contract includes a way for its
owners to change this URI via a `var-set` function call:

```clarity
(define-data-var tokenUri (optional (string-utf8 256)) (some u"https://cdn.citycoins.co/metadata/newyorkcitycoin.json"))

;; set token URI to new value, only accessible by Auth
(define-public (set-token-uri (newUri (optional (string-utf8 256))))
  (begin
    (asserts! (is-authorized-auth) ERR_UNAUTHORIZED)
    (ok (var-set tokenUri newUri))
  )
)
```

This setup is very flexible for administrators, but it creates a complex problem for metadata
indexers which now need to figure out if (and when) they should re-index token contracts to avoid
displaying stale metadata in their applications.


## Metadata staleness

Within the Stacks ecosystem, there are a number of applications that need to index token metadata
and struggle with specific challenges caused by changed metadata. For example:

* An NFT marketplace, which needs to display a token's artwork for users to view.
  * Presenting a token's icon correctly is difficult given that the `get-token-uri` on-chain
    variable could change, the off-chain JSON file could change, and/or the image served by the URL
    could change.
* A [blockchain API](https://github.com/hirosystems/stacks-blockchain-api), which needs to serve FT
metadata to return account balances correctly.
  * Wallets require the on-chain decimals value in order to correctly send and receive tokens.
    Critical balance draining is possible when this property is zero at contract launch but updated
    later.

For indexing, developers usually run and maintain a background process that listens for new token
contracts deployed to the blockchain so they can immediately call on their metadata to save the
results. This works for new contracts, but it is insufficient for old ones that may change their
metadata after it has been processed.

To avoid staleness, some indexers resort to a cron-like periodic refresh of all tracked contracts,
but while this may work for individual applications, it does not provide a consistent experience for
Stacks users that may interact with different metadata-aware systems with different refresh periods.
This workaround also adds unnecessary network traffic and creates extra strain on public Stacks
nodes due to aggressively polling contract-read RPC endpoints.

## Metadata update notifications

To solve this problem reliably, contract administrators need a way to notify the network when they
have made changes to the metadata so any indexers may then perform a refresh just for that contract.

The proposed mechanism for these notifications leverages the [`print` Clarity
language function](https://docs.stacks.co/write-smart-contracts/language-functions#print). When
used, its output is bundled inside an event of type `contract_event`:

```json
{
  "type": "contract_event",
  "contract_event": {
    "contract_identifier": "<emitter contract>",
    "topic": "print",
    "value": "<print output>"
  }
}
```

This event is then attached to a transaction object and broadcasted when the same transaction is
included in a block or microblock.

This SIP proposes a standard message structure (similar to a notification payload) that would be
used through `print`. Existing metadata indexers would receive this event through the [Stacks node
event-emitter
interface](https://github.com/stacks-network/stacks-blockchain/blob/master/docs/event-dispatcher.md#post-new_block),
parse and validate its contents, and refresh any contracts that were updated. `print` was also
selected for the following reasons:

1. There is precedent for the use of `print` notifications in the Stacks ecosystem: the BNS
contract, for example, uses it to notify the network when a change to a name or its zonefile has
occurred. The PoX-2 contract for Stacks 2.1 will make heavy use of it to record stacking state
changes across addresses. This SIP aims to continue this trend.
1. For chain indexers, consuming it is practically free if they already process transactions. This
would enable, for example, a notification to be clearly displayed in the Stacks Explorer alongside
its transaction.
1. Adding a `print` notification to a function's Clarity code also serves as self-explanatory
   documentation.
1. If there is a new notification use case in the future, a newer SIP can propose an additional
   `print` structure and indexers would be quick to adopt these if they need to. See [Notification
   structure reusability](#notification-structure-reusability).

# Specification

Notification messages for each token class are specified below. Token metadata update notifications
must be made via a contract call transaction to the [deployed reference
contract](https://explorer.stacks.co/txid/0xe92af2ea5c11e2e6fde4d31fd394de888070efff23bffad04465c549543abfa2?chain=mainnet)
or from a call to `print` within any other contract, including the token contract itself.

## Fungible Tokens

When a contract needs to notify the network that metadata has changed for a **Fungible Token**, it
shall call `print` with a tuple with the following structure:

```clarity
{ notification: "token-metadata-update", payload: { token-class: "ft", contract-id: <token contract id> }}
```

| Key                   | Value                                                                  |
|-----------------------|------------------------------------------------------------------------|
| `notification`        | The string `"token-metadata-update"`                                   |
| `payload.token-class` | The string `"ft"`                                                      |
| `payload.contract-id` | The contract id (principal) of the contract that declared the token    |
| `payload.update-mode` | _[optional]_ Metadata update mode (see section below)                  |
| `payload.ttl`         | _[optional]_ Time-to-live for `payload.update-mode: dynamic`           |

## Non-Fungible Tokens

When a contract needs to notify the network that metadata has changed for a **Non-Fungible Token**,
it shall call `print` with a tuple with the following structure:

```clarity
{ notification: "token-metadata-update", payload: { token-class: "nft", token-ids: (list u100, u101), contract-id: <token contract id> }}
```

| Key                   | Value                                                                  |
|-----------------------|------------------------------------------------------------------------|
| `notification`        | The string `"token-metadata-update"`                                   |
| `payload.token-class` | The string `"nft"`                                                     |
| `payload.contract-id` | The contract id (principal) of the contract that declared the tokens   |
| `payload.token-ids`   | _[optional]_ A list with the uint token ids that need to be refreshed  |
| `payload.update-mode` | _[optional]_ Metadata update mode (see section below)                  |
| `payload.ttl`         | _[optional]_ Time-to-live for `payload.update-mode: dynamic`           |

If a notification does not contain a value for `payload.token-ids`, it means it is requesting an
update for all tokens.

## Semi-Fungible Tokens

When a contract needs to notify the network that metadata has changed for a **Semi-Fungible Token**,
it shall call `print` with a tuple with the following structure:

```clarity
{ notification: "token-metadata-update", payload: { token-class: "sft", token-ids: (list u100, u101), contract-id: <token contract id> }}
```

| Key                   | Value                                                                  |
|-----------------------|------------------------------------------------------------------------|
| `notification`        | The string `"token-metadata-update"`                                   |
| `payload.token-class` | The string `"sft"`                                                     |
| `payload.contract-id` | The contract id (principal) of the contract that declared the tokens   |
| `payload.token-ids`   | A list with the uint token ids that need to be refreshed               |
| `payload.update-mode` | _[optional]_ Metadata update mode (see section below)                  |
| `payload.ttl`         | _[optional]_ Time-to-live for `payload.update-mode: dynamic`           |

Notifications for SFTs must include a value for `payload.token-ids`.

## Metadata update modes

Applications may use tokens for very different purposes. Some of these could require none or very
few metadata updates ever (e.g. digital artwork that never changes except for reveals), while others
could need to alter it several times a day (e.g. NFTs for in-game items that are traded and modded
continuously).

This use-case variety also affects how developers decide to host their metadata JSON files. For
example, they could choose to use IPFS for low-frequency updates and finality, versus Amazon S3 for
high-frequency off-chain updates.

In order to allow creators and app developers to specify how token metadata should be treated by
indexers, notifications support an optional `payload.update-mode` key that may contain one of the
following values:

* `standard`: The new metadata will be valid until the next notification comes.

    This is the default mode if none is specified.
* `frozen`: This token's metadata will never change again, ever.

    Indexers should ignore new notifications for this token, even if valid.
* `dynamic`: The new metadata is expected to change very quickly and many times in the future (even
  off-chain).

    Indexers should not expect to receive explicit notifications for each of these changes and
should consider refreshing this token's metadata frequently. Token developers may suggest a
reasonable amount of time between refreshes by adding an estimated value (defined in seconds) to the
`payload.ttl` notification property.

## Considerations for metadata indexers

For a token metadata update notification to be considered valid by metadata indexers, it must meet
the following requirements:

1. Its payload structure should be correct whether it is updating a [FT](#fungible-tokens), an
   [NFT](#non-fungible-tokens) or an [SFT](#semi-fungible-tokens).
1. Either the `contract_identifier` field of the contract event must be equal to the
   `payload.contract-id` (i.e., the event was produced by the contract that owns the metadata) or
   the transaction's `tx-sender` principal should match the principal contained in the
   notification's `payload.contract-id` (i.e., the STX address that sent the transaction which emits
   the notification should match the owner of the token contract being updated).

Notifications that do not meet these requirements must be ignored.

### Other implications

* Notifications can come at any point in time and are persistent in the Stacks blockchain.
  * When performing a local sync to the chain tip, old notifications for old metadata updates could
    not necessarily have a distinct effect in metadata responses when processed in the present.
* Multiple notifications for the same tokens will not necessarily correspond to multiple metadata
  updates.
  * Refreshing a token's metadata should be an idempotent operation. Repeated refreshes should not
    create distinct records in the internal metadata database.
  * To prevent slow performance and guard against any Denial of Service attack attempts, contract
    call rate limiting should be implemented locally.
* Notifications can be delayed and out of order.
  * A notification transaction's timestamp should not be considered to be the time when the token
    metadata was actually updated.

Given these constraints the notifications this SIP proposes should be taken as _hints_ to metadata
indexers. Metadata indexers are not obliged to follow them.

## Notification structure reusability

Even though establishing a generalized smart contract notification standard is out of scope for this
SIP, the proposed `print` message structure was designed for reusability by future SIPs that wish to
standardize other events.

For example, developers could vary the `notification` and `payload` values to notify the network
when an NFT collection has been fully minted or another important milestone is reached.

# Related work

An alternative considered for token metadata update notifications is for them to be transmitted via
an off-chain notification service that indexer developers may subscribe to, such as:

* An official mailing list
* A forum post
* An authoritative API service

While these channels would have several advantages like being simpler to update, faster to
propagate, and easier to moderate, they have key disadvantages that make them inadequate for this
SIP's intended use:

1. They introduce a third party dependency
    * An off-chain notification service would most likely be maintained by centralized entities
      unrelated to the Stacks ecosystem. As such, they could modify the channel, its reach, or its
      rules at any time while affecting the entire network.
    * Accepting third party solutions would invite developers to use many different hinting service
      APIs and implementations, defeating the standardization purpose of this SIP. Moving
      notifications to the blockchain establishes a canonical way to store and access them.
    * Even if a decentralized off-chain third-party solution is found, it could still add a layer
      of friction for developer adoption.
1. They are not future proof
    * If the selected off-chain service changes at any point, a migration to another notification
      channel will be much more difficult once the Stacks ecosystem has more token applications and
      metadata indexers.

# Backwards compatibility

Developers who need to emit metadata update notifications for tokens declared in older contracts
(that were deployed before this notification standard was established) could do so by either calling
the contract described in [Reference Implementations](#reference-implementations) or by first
deploying a new separate contract containing a public function that prints this notification and
then calling it to have it emitted.

# Activation

This SIP will be activated when the following conditions are met:

1. At least 10 unique contracts have had metadata updates triggered via contract-call transactions
   that print the proposed notification payload.
1. At least 3 metadata indexers (like the Stacks Blockchain API or an NFT marketplace) start
   listening for and reacting to the emitted notifications.

If the Stacks blockchain reaches block height 170000 and the above has not happened, this SIP will
be considered rejected.

# Reference implementations

A [reference contract](./token-metadata-update-notify.clar) has been deployed to mainnet as
[`SP1H6HY2ZPSFPZF6HBNADAYKQ2FJN75GHVV95YZQ.token-metadata-update-notify`](https://explorer.stacks.co/txid/0xe92af2ea5c11e2e6fde4d31fd394de888070efff23bffad04465c549543abfa2?chain=mainnet).
It demonstrates how to send notifications for each token class and it is available for developers to
use for refreshing any existing or future token contract. If the SIP evolves to require a change to
this contract pre-activation, a new one will be deployed and noted here.

```clarity
;; token-metadata-update-notify
;;
;; Use this contract to notify token metadata indexers that an NFT or FT needs its metadata
;; refreshed.

(use-trait nft-trait 'SP2PABAF9FTAJYNFZH93XENAJ8FVY99RRM50D2JG9.nft-trait.nft-trait)
(use-trait ft-trait 'SP3FBR2AGK5H9QBDH3EEN6DF8EK8JY7RX8QJ5SVTE.sip-010-trait-ft-standard.sip-010-trait)

;; Refresh the metadata for one or more NFTs from a specific contract.
(define-public (nft-metadata-update-notify (contract <nft-trait>) (token-ids (list 100 uint)))
  (ok (print
    {
      notification: "token-metadata-update",
      payload: {
        contract-id: contract,
        token-class: "nft",
        token-ids: token-ids
      }
    })))

;; Refresh the metadata for a FT from a specific contract
(define-public (ft-metadata-update-notify (contract <ft-trait>))
  (ok (print
    {
      notification: "token-metadata-update",
      payload: {
        contract-id: contract,
        token-class: "ft"
      }
    })))
```

The [Stacks Blockchain API](https://github.com/hirosystems/stacks-blockchain-api) will also add
compatibility for this standard while this SIP is being considered to demonstrate how indexers can
listen for and react to these notifications.

          
# Preamble

SIP Number: 000

Title: Stacks Improvement Proposal Process

Author: Jude Nelson <jude@stacks.org>, Ken Liao <yukanliao@gmail.com>

Consideration: Governance 

Type: Meta 

Status: Ratified 

Created: 2020-06-23 

License: BSD-2-Clause 

Sign-off: Jude Nelson <jude@stacks.org>, Technical Steering Committee Chair

Discussions-To: https://github.com/stacksgov/sips 

# Abstract

A Stacks Improvement Proposal (SIP) is a design document that provides
information to the greater Stacks ecosystem's participants concerning the design
of the Stacks blockchain and its ongoing operation. Each SIP shall provide a
clear and concise description of features, processes, and/or standards for the
Stacks blockchain and its operators to adopt, with sufficient details provided
such that a reasonable practitioner may use the document to create an
independent but compatible implementation of the proposed improvement.

SIPs are the canonical medium by which new features are proposed and described,
and by which input from the Stacks ecosystem participants is collected. The SIP
Ratification Process is also described in this document, and provides the means
by which SIPs may be proposed, vetted, edited, accepted, rejected, implemented,
and finally incorporated into the Stacks blockchain's design, governance, and
operational procedures. The set of SIPs that have been ratified shall
sufficiently describe the design, governance, and operationalization of the
Stacks blockchain, as well as the means by which future changes to its official
design, implementation, operation, and governance may be incorporated.

# License and Copyright

This SIP is made available under the terms of the BSD-2-Clause license,
available at https://opensource.org/licenses/BSD-2-Clause.  This SIP’s copyright
is held by the Stacks Open Internet Foundation.

# Introduction

Blockchains are unique among distributed systems in that they also
happen to encode a social contract. By running a blockchain node, a user
implicitly agrees to be bound to the social contract's terms embedded within the
blockchain's software. These social contracts are elaborate constructions that
contain not only technical terms (e.g. "a block may be at most 1MB"), but also
economic terms (e.g. "only 21 million tokens may exist") and social terms (e.g.
"no money can leave this account" or "this transaction type was supported
before, but will now be ignored by the system") which the user agrees to uphold
by running a blockchain node.

It stands to reason that the Stacks blockchain is made of more than just
software; it is also made of the people who run it. As such, the act of
developing and managing the Stacks blockchain network includes the act of
helping its people coordinate and agree on what the blockchain is and what it
should do. To this end, this document proposes a process by which the Stacks
blockchain's users can conduct themselves to be the stewards of the blockchain
network in perpetuity.

The goals of this process are to ensure that anyone may submit a SIP in good
faith, that each SIP will receive fair and speedy good-faith consideration by
other people with the relevant expertise, and that any discussions and
decision-making on each SIP's ratification shall happen in public. To achieve
these ends, this document proposes a standard way of presenting a Stacks
Improvement Proposal (SIP), and a standard way of ratifying one.

Each SIP document contains all of the information needed to propose a
non-trivial change to the way in which the Stacks blockchain operates. This
includes both technical considerations, as well as operational and governance
considerations. This document proposes a formal document structure based on both
request-for-comments (RFC) practices in the Internet Engineering Task Force
(IETF), as well as existing blockchain networks.

SIPs must be ratified in order to be incorporated into the definition of what
the Stacks blockchain is, what it does, and how it operates. This document
proposes a ratification process based on existing governance processes from
existing open source projects (including Python, Bitcoin, Ethereum, and Zcash),
and makes provisions for creating and staffing various roles that people must
take on to carry out ratification (e.g. committees, editors, working groups and
so on).

This document uses the word “users” to refer specifically to people who
participate in the greater Stacks ecosystem.  This includes, but is not limited
to, people who mine blocks, people who contribute code, people who run nodes,
people who develop applications that rely on the Stacks blockchain, people who
use such applications, people involved in the project governance, and people
involved in operating software deployments.

# Specification

Each SIP shall adhere to the same general formatting and shall be ratified
through the processes described by this document.

## SIP Format

All SIPs shall be formatted as markdown files. Each section shall be
annotated as a 2nd-level header (e.g. `##`). Subsections may be added with
lower-level headers.

Each SIP shall contain the following sections, in the given order:

- _Preamble_. This section shall provide fields useful for categorizing the SIP.
  The required fields in all cases shall be:
    - _SIP Number_. Each SIP receives a unique number once it has been accepted
      for consideration for ratification (see below). This number is assigned to
      a SIP; its author does not provide it.
    - _Title_. A concise description of the SIP, no more than 20 words long.
    - _Author_. A list of names and email addresses of the SIP's author(s).
    - _Consideration_. What class of SIP this is (see below).
    - _Type_. The SIP track for consideration (see below).
    - _Status_. This SIP's point in the SIP workflow (see below).
    - _Created_. The ISO 8601 date when this SIP was created.
    - _License_. The content license for the SIP (see below for permitted
      licenses).
    - _Sign-off_. The list of relevant persons and their titles who have worked to
      ratify the SIP. This field is not filled in entirely until ratification,
      but is incrementally filled in as the SIP progresses through the ratification
      process.
- Additional SIP fields, which are sometimes required, include:
    - _Layer_. The logical layer of the Stacks blockchain affected. Must be one
    - of the following:
        - _Consensus (soft fork)_. For backwards-compatible proposals for
          transaction-processing.
        - _Consensus (hard fork)_. For backwards-incompatible proposals for
          transaction-processing.
        - _Peer Services_. For proposals to the peer-to-peer network protocol
          stack.
        - _API/RPC_. For proposals to the Stacks blockchain's official
          programmatic interfaces.
        - _Traits_. For proposals for new standardized Clarity trait definitions.
        - _Applications_. For proposals for standardized application protocols
          that interface with the Stacks blockchain.
    - _Discussions-To_. A mailing list where ongoing discussion of the SIP takes
      place.
    - _Comments-Summary_. The comments summary tone.
    - _Comments-URI_. A link to the Stacks blockchain wiki for comments.
    - _License-Code_. Abbreviation for code under a different license than the SIP
      proposal.
    - _Post-History_. Dates of posting the SIP to the Stacks mailing list, or a
      link to a thread with the mailing list.
    - _Requires_. A list of SIPs that must be implemented prior to this SIP.
    - _Replaces_. A list of SIPs that this SIP replaces.
    - _Superceded-By_. A list of SIPs that replace this SIP.

- _Abstract_. This section shall provide a high-level summary of the proposed
  improvement. It shall not exceed 5000 words.
- _Copyright_. This section shall provide the copyright license that governs the
  use of the SIP content. It must be one of the approved set of licenses (see
below).
- _Introduction_. This section shall provide a high-level summary of the
  problem(s) that this SIP proposes to solve, as well as a high-level
description of how the proposal solves them. This section shall emphasize its
novel contributions, and briefly describe how they address the problem(s). Any
motivational arguments and example problems and solutions belong in this
section.
- _Specification_. This section shall provide the detailed technical
  specification. It may include code snippets, diagrams, performance
evaluations, and other supplemental data to justify particular design decisions.
However, a copy of all external supplemental data (such as links to research
papers) must be included with the SIP, and must be made available under an
approved copyright license.
- _Related Work_. This section shall summarize alternative solutions that address
  the same or similar problems, and briefly describe why they are not adequate
solutions. This section may reference alternative solutions in other blockchain
projects, in research papers from academia and industry, other open-source
projects, and so on. This section must be accompanied by a bibliography of
sufficient detail such that someone reading the SIP can find and evaluate the
related works.
- _Backwards Compatibility_. This section shall address any
  backwards-incompatiblity concerns that may arise with the implementation of
this SIP, as well as describe (or reference) technical mitigations for breaking
changes. This section may be left blank for non-technical SIPs.
- _Activation_. This section shall describe the timeline, falsifiable criteria,
  and process for activating the SIP once it is ratified. This applies to both
technical and non-technical SIPs.  This section is used to unambiguously
determine whether or not the SIP has been accepted by the Stacks users once it
has been submitted for ratification (see below).
- _Reference Implementations_. This section shall include one or more references
  to one or more production-quality implementations of the SIP, if applicable.
This section is only informative — the SIP ratification process is independent
of any engineering processes (or other processes) that would be followed to
produce implementations.  If a particular implementation process is desired,
then a detailed description of the process must be included in the Activation
section.  This section may be updated after a SIP is ratified in order to
include an up-to-date listing of any implementations or embodiments of the SIP. 

Additional sections may be included as appropriate.

### Supplemental Materials

A SIP may include any supplemental materials as
appropriate (within reason), but all materials must have an open format
unencumbered by legal restrictions. For example, an LibreOffice `.odp`
slide-deck file may be submitted as supplementary material, but not a Keynote
`.key` file.

When submitting the SIP, supplementary materials must be present within the same
directory, and must be named as `SIP-XXXX-YYY.ext`, where:

- `XXXX` is the SIP number,
- `YYY` is the serial number of the file, starting with 1,
- `.ext` is the file extension.

## SIP Types

The types of SIPs are as follows:

- _Consensus_. This SIP type means that all Stacks blockchain implementations
  would need to adopt this SIP to remain compatible with one another. If this is
the SIP type, then the SIP preamble must have the Layer field set to either
_Consensus (soft fork)_ or _Consensus (hard fork)_.
- _Standard_. This SIP type means that the proposed change affects one or more
  implementations, but does not affect network consensus. If this is the SIP
type, then the SIP preamble must have the Layer field set to indicate which
aspect(s) of the Stacks blockchain are affected by the proposal.
- _Operation_. This SIP type means that the proposal concerns the operation of the
  Stacks blockchain -- in particular, it concerns node operators and miners.
The difference between this SIP type and the Standard type is that this type
does not change any existing protocols.
- _Meta_. This SIP type means that the proposal concerns the SIP ratification
  process. Such a SIP is a proposal to change the way SIPs are handled.
- _Informational_. This is a SIP type that provides useful information, but does
  not require any action to be taken on the part of any user.

New types of SIPs may be created with the ratification of a Meta-type SIP under
the governance consideration (see below). SIP types may not be removed.

## SIP Considerations

A SIP's consideration determines the particular steps needed to ratify the SIP
and incorporate it into the Stacks blockchain. Different SIP considerations have
different criteria for ratification. A SIP can have more than one consideration,
since a SIP may need to be vetted by different users with different domains of
expertise.


- _Technical_. The SIP is technical in nature, and must be vetted by users with
  the relevant technical expertise.
- _Economic_. The SIP concerns the blockchain's token economics. This not only
  includes the STX token, but also any on-chain tokens created within smart
contracts. SIPs that are concerned with fundraising methods, grants, bounties,
and so on also belong in this SIP track.
- _Governance_. The SIP concerns the governance of the Stacks blockchain,
  including the SIP process. This includes amendments to the SIP Ratification
Process, as well as structural considerations such as the creation (or removal)
of various committees, editorial bodies, and formally recognized special
interest groups. In addition, governance SIPs may propose changes to the way by
which committee members are selected.
- _Ethics_. This SIP concerns the behaviors of office-holders in the SIP
  Ratification Process that can affect its widespread adoption.  Such SIPs
describe what behaviors shall be deemed acceptable, and which behaviors shall be
considered harmful to this end (including any remediation or loss of privileges
that misbehavior may entail).  SIPs that propose formalizations of ethics like
codes of conduct, procedures for conflict resolution, criteria for involvement
in governance, and so on would belong in this SIP consideration.
- _Diversity_. This SIP concerns proposals to grow the set of users, with an
  emphasis on including users who are traditionally not involved with
open-source software projects. SIPs that are concerned with evangelism,
advertising, outreach, and so on must have this consideration.

Each SIP consideration shall have a dedicated Advisory Board that ultimately
vets SIPs under their consideration for possible ratification in a timely
fashion (see below).  New considerations may be created via the ratification of
a Meta-type SIP under the governance consideration.

## SIP Workflow

As a SIP is considered for ratification, it passes through multiple statuses as
determined by one or more committees (see next section). A SIP may have exactly
one of the following statuses at any given time:

- _Draft_. The SIP is still being prepared for formal submission. It does not yet
  have a SIP number.
- _Accepted_. The SIP text is sufficiently complete that it constitutes a
  well-formed SIP, and is of sufficient quality that it may be considered for
ratification. A SIP receives a SIP number when it is moved into the Accepted
state by SIP Editors.
- _Recommended_. The people responsible for vetting the SIPs under the
  consideration(s) in which they have expertise have agreed that this SIP should
be implemented. A SIP must be Accepted before it can be Recommended.
- _Activation-In-Progress_.  The SIP has been tentatively approved by the Steering
  Committee for ratification.  However, not all of the criteria for ratification
have been met according to the SIP’s Activation section.  For example, the
Activation section might require miners to vote on activating the SIPs’
implementations, which would occur after the SIP has been transferred into
Activation-In-Progress status but before it is transferred to Ratified status.
- _Ratified._ The SIP has been activated according to the procedures described in
  its Activation section.  Once ratified, a SIP remains ratified in perpetuity,
but a subsequent SIP may supersede it. If the SIP is a Consensus-type SIP, and
then all Stacks blockchain implementations must implement it. A SIP must be
Recommended before it can be Ratified. Moving a SIP into this state may be done
retroactively, once the SIP has been activated according to the terms in its
Activation section.
- _Rejected_. The SIP does not meet at least one of the criteria for ratification
  in its current form. A SIP can become Rejected from any state, except
Ratified.  If a SIP is moved to the Rejected state, then it may be re-submitted
as a Draft.
- _Obsolete_. The SIP is deprecated, but its candidacy for ratification has not
  been officially withdrawn (e.g. it may warrant further discussion).  An
Obsolete SIP may not be ratified, and will ultimately be Withdrawn.
- _Replaced_. The SIP has been superseded by a different SIP.  Its preamble must
  have a Superseded-By field. A Replaced SIP may not be ratified, nor may it be
re-submitted as a Draft-status SIP.  It must be transitioned to a Withdrawn
state once the SIP(s) that replace it have been processed.
- _Withdrawn_. The SIP's authors have ceased working on the SIP. A Withdrawn SIP
  may not be ratified, and may not be re-submitted as a Draft.  It must be
re-assigned a SIP number if taken up again.
    

The act of ratifying a SIP is the act of transitioning it to the Ratified status
-- that is, moving it from Draft to Accepted, from Accepted to Recommended, and
Recommended to Activation-In-Progress, and from Activation-In-Progress to
Ratified, all without the SIP being transitioned to Rejected, Obsolete,
Replaced, or Withdrawn status.  A SIP's current status is recorded in its Status
field in its preamble.

## SIP Committees

The act of deciding the status of a SIP is handled by a set of designated
committees. These committees are composed of users who dedicate their time and
expertise to curate the blockchain, ratifying SIPs on behalf of the rest of the
ecosystem’s users.

There are three types of committee:

- _Steering Committee (SC)_. The roles of the SC are to select Recommended-status
  SIPs to be activated, to determine whether or not a SIP has been activated and
thus ratified, and to formally recognize Consideration Advisory Boards (see
below).
- _Consideration Advisory Boards_. The roles of the Consideration Advisory Boards
  are to provide expert feedback on SIPs that have been moved to Accepted status
in a timely manner, and to transition SIPs to Recommended status if they meet
the Board's consideration criteria, and Rejected status otherwise. 
- _SIP Editors_. The role of the SIP Editors is to identify SIPs in the Draft
  status that can be transitioned to Accepted status. A SIP editor must be able
to vet a SIP to ensure that it is well-formed, that it follows the ratification
workflow faithfully, and that it does not overlap with any already-Accepted SIPs
or SIPs that have since become Recommended or Ratified.
    
Any user may serve on a committee. However, all Stacks committee members must
abide by the SIP Code of Conduct and must have a history of adhering to it.
Failure to adhere to the Code of Conduct shall be grounds for immediate removal
from a committee, and a prohibition against serving on any future committees.

### Compensation

Compensation for carrying out committee duties is outside of the scope of this
document.  This document does not create a provision for compensation for
committee participation, but it does not forbid it either.

### Steering Committee Duties

The Steering Committee's overarching duty is to oversee the evolution of the
Stacks blockchain’s design, operation, and governance, in a way that is
technically sound and feasible, according to the rules and procedures described
in this document. The SC shall be guided by and held accountable by the greater
community of users, and shall make all decisions with the advice of the relevant
Consideration Advisory Boards. 

The SC’s role is that of a steward.  The SC shall select SIPs for ratification
based on how well they serve the greater good of the Stacks users.  Given the
nature of blockchains, the SC's particular responsibilities pertaining to
upgrading the blockchain network are meant to ensure that upgrades happen in a
backwards-compatible fashion if at all possible. While this means that more
radical SIPs may be rejected or may spend a long amount of time in Recommended
status, it also minimizes the chances of an upgrade leading to widespread
disruption (the minimization of which itself serves the greater good).

#### Membership

The initial Steering Committee shall be comprised of at least three members:
two from the Stacks Open Internet Foundation, and one
from the greater Stacks blockchain community (independent of the Stacks
Foundation).

A provisional Steering Committee will be appointed by the Stacks Open Internet Foundation Board
before the launch of the Stacks blockchain’s mainnet (see the "Activation" section).
Once this SIP activates, the Stacks Open Internet Foundation shall select its
representatives in a manner of their choosing within 90 days after activation.
The committee may be expanded later to include more seats.  Once this SIP
activates, the provisional SC will consult with the community to
ratify a SIP that implements a voting procedure whereby
Stacks community members can select the individual who will serve on the
community SC seat.

#### Qualifications

Members of this committee must have deep domain expertise
pertinent to blockchain development, and must have excellent written
communication skills. It is highly recommended that members should have authored
at least one ratified technical-consideration SIP before joining this committee.

#### Responsibilities

The Steering Committee shall be responsible for the following
tasks.

##### Recognizing Consideration Advisory Boards.

The members of the Steering Committee
must bear in mind that they are not infallible, and that they do not know everything
there is to know about what is best for the broader user community. To the
greatest extent practical, the SC shall create and foster the development of
Consideration Advisory Boards in order make informed decisions on subjects that
in which they may not be experts.

Any group of users can form an unofficial working group to help provide feedback
to SIPs, but the SC shall have the power to recognize such groups formally as a
Consideration Advisory Board via at least a two-thirds majority vote. The SC
shall simultaneously recognize one of it’s member to serve as the interim
chairperson while the Advisory Board forms. A SC member cannot normally serve on
a Consideration Advisory Board concurrently with serving on the SC, unless
granted a limited exception by a unanimous vote by the SC (e.g. in order to
address the Board’s business while a suitable chairperson is found).  Formally
recognizing Consideration Advisory Boards shall occur in Public Meetings (see
below) no more than once per quarter.

Once recognized, Consideration Advisory Boards may not be dissolved or
dismissed, unless there are no Accepted or Recommended SIPs that request their
consideration. If this is the case, then the SC may vote to rescind recognition
of a Consideration Advisory Board with a two-thirds majority at one of its
Public Meetings.

In order to identify users who would form a Consideration Advisory Board, users
should organize into an unofficial working group and submit a SIP to petition
that SC recognize the working group as a Consideration Advisory Board.  This
petition must take the form of a Meta-type SIP, and may be used to select the
initial chairperson and define the Board's domain(s) of expertise, bylaws,
membership, meeting procedures, communication channels, and so on, independent
of the SC. The SC would only be able to ratify or reject the SIP.

The SC shall maintain a public index of all Consideration Advisory Boards that
are active, including contact information for the Board and a summary of what
kinds of expertise the Board can offer. This index is meant to be used by SIP
authors to help route their SIPs towards the appropriate reviewers before being
taken up by the SC.

##### Voting on Technical SIPs

The Steering Committee shall select Recommended SIPs
for ratification by moving them to Activation-In-Progress status.  All
technical-consideration SIPs shall require an 80% vote. If it is a
Consensus-type SIP for a hard fork, then a unanimous vote shall be required. If
a SIP is voted on and is not moved to Activation-in-Progress, then it shall be
moved to Rejected status, and the SC shall provide a detailed explanation as to
why they made their decision (see below).

##### Voting on Non-technical SIPs

Not all SIPs are technical in nature. All
non-technical SIPs shall require only a two-thirds majority vote to transition
it to Activation-In-Progress status. The SC members must provide a public
explanation for the way it voted as supplementary materials with the ratified
non-technical SIP (see below).  If the SC votes to move a non-technical SIP to
Activation-In-Progress status, but does not receive the requisite number of
votes, then the SIP shall be transferred to Rejected status, and the SC shall
provide a detailed explanation as to why they made their decision (see below).

##### Overseeing SIP Activation and Ratification

Once a SIP is in Activation-In-Progress status,
the SC shall be responsible for overseeing the procedures and criteria in the
SIP’s Activation section.  The Activation section of a SIP can be thought of as
an “instruction manual” and/or “checklist” for the SC to follow to determine if
the SIP has been accepted by the Stacks users.  The SC shall strictly adhere to
the process set forth in the Activation section.  If the procedure and/or
criteria of the Activation section cannot be met, then the SC may transfer the
SIP to Rejected status and ask the authors to re-submit the SIP with an updated
Activation section.

Once all criteria have been unambiguously meet and all activation procedures
have been followed, the SC shall transition the SIP to Ratified status.  The SC
shall keep a log and provide a record of the steps they took in following a
SIP’s Activation section once the SIP is in Activation-In-Progress status, and
publish them alongside the Ratified SIP as supplemental material.

Due to the hands-on nature of the Activation section, the SC may deem it
appropriate to reject a SIP solely on the quality of its Activation section.
Reasonable grounds for rejection include, but are not limited to, ambiguous
instructions, insufficiently-informative activation criteria, too much work on
the SC members’ parts, the lack of a prescribed activation timeout, and so on.

Before the Stacks mainnet launches, the SC shall ratify a SIP that, when
activated according to the procedures outlined in its Activation section, will
allow Stacks blockchain miners to signal their preferences for the activation of
particular SIPs within the blocks that they mine. This will enable the greater
Stacks community of users to have the final say as to which SIPs activate and
become ratified.

##### Feedback on Recommended SIPs

The Steering Committee shall give a full, fair,
public, and timely evaluation to each SIP transitioned to Recommended status by
Consideration Advisory Boards. A SIP shall only be considered by the SC if the
Consideration Advisory Board chairpeople for each of the SIP's considerations
have signed-off on the SIP (by indicating as such on the SIP's preamble). 

The SC may transition a SIP to Rejected status if it disagrees with the
Consideration Advisory Boards' recommendation. The SC may transition a SIP to
Obsolete status if it finds that the SIP no longer addresses a relevant concern.
It may transition the SIP to a Replaced status if it considers a similar,
alternative SIP that is more likely to succeed. In all cases, the SC shall
ensure that a SIP does not remain in Recommended status for an unreasonable
amount of time.

The SC shall maintain a public record of all feedbacks provided for each SIP it
reviews.

If a SIP is moved to Rejected, Obsolete, or Replaced status, the SIP authors may
appeal the process by re-submitting it in Draft status once the feedback has
been addressed.  The appealed SIP must cite the SC’s feedback as supplemental
material, so that SIP Editors and Consideration Advisory Boards are able to
verify that the feedback has, in fact, been addressed.

##### Public Meetings

The Steering Committee shall hold and record regular public
meetings at least once per month. The SC may decide the items of business for
these meetings at its sole discretion, but it shall prioritize business
pertaining to the ratification of SIPs, the recognition of Consideration
Advisory Boards, and the needs of all outstanding committees. That said, any
user may join these meetings as an observer, and the SC shall make a good-faith
effort to address public comments from observers as time permits.

The SC shall appoint up to two dedicated moderators from the user community for
its engineering meetings, who shall vet questions and commentary from observers
in advance (possibly well before the meeting begins). If there is more than one
moderator, then the moderators may take turns. In addition, the SC shall appoint
a dedicated note-taker to record the minutes of the meetings. All of these
appointees shall be eligible to receive a fixed, regular bounty for their work.

### Consideration Advisory Board Duties

There is an Advisory Board for each SIP consideration, with a designated
chairperson responsible for maintaining copies of all discussion and feedback on
the SIPs under consideration.

#### Membership

All Consideration Advisory Boards begin their life as unofficial
working groups of users who wish to review inbound SIPs according to their
collective expertise.  If they wish to be recognized as an official
Consideration Advisory Board, they shall submit a SIP to the Steering Committee
per the procedure described in the Steering Committee’s duties.  Each
Consideration Advisory Board shall be formally created by the SC with a
designated member serving as its first interim chairperson. After this, the
Consideration Advisory Board may adopt its own bylaws for selecting members and
chairpeople. However, members should have domain expertise relevant to the
consideration.

#### Members

Members shall serve on their respective Consideration Advisory Boards so long as
they are in good standing with the SIP Code of Conduct and in accordance to the
individual Board’s bylaws.  A user may serve on at most three Consideration
Advisory Boards concurrently.

#### Qualifications

Each Consideration Advisory Board member shall have sufficient
domain expertise to provide the Steering Committee with feedback pertaining to a
SIP's consideration. Members shall possess excellent written communication
skills.

#### Responsibilities

Each Consideration Advisory Board shall be responsible for the
following.

##### Chairperson

Each Consideration Advisory Board shall appoint a chairperson, who
shall serve as the point of contact between the rest of the Board and the
Steering Committee. If the chairperson becomes unresponsive, the SC may ask the
Board to appoint a new chairperson (alternatively, the Board may appoint a new
chairperson on its own and inform the SC).  The chairperson shall be responsible
for maintaining the Board’s public list of members’ names and contact
information as a supplementary document to the SIP that the SC ratified to
recognize the Board.

##### Consideration Track

Each Consideration Advisory Board shall provide a clear and
concise description of what expertise it can offer, so that SIP authors may
solicit it with confidence that it will be helpful. The chairperson shall make
this description available to the Steering Committee and to the SIP Editors, so
that both committees can help SIP authors ensure that they receive the most
appropriate feedback.

The description shall be provided and updated by the chairperson to the SC so
that the SC can provide a public index of all considerations a SIP may possess.

##### Feedback

Feedback to SIP Authors Each Consideration Advisory Board shall provide a full,
fair, public, and timely evaluation of any Accepted-status SIP that lists the
Board's consideration in its preamble. The Board may decide to move each SIP to
a Recommended status or a Rejected status based on whether or not the Board
believes that the SIP is feasible, practical, and beneficial to the greater
Stacks ecosystem.

Any feedback created shall be made public. It is the responsibility of the Board
to store and publish all feedbacks for the SIPs it reviews. It shall forward
copies of this feedback to both the SIP authors.

##### Consultation with the Steering Committee

The Steering Committee may need to
follow up with the Consideration Advisory Board in order to clarify its position
or solicit its advice on a particular SIP. For example, the SC may determine
that a Recommended SIP needs to be considered by one or more additional Boards
that have not yet been consulted by the SIP authors.

The Board shall respond to the SC's request for advice in a timely manner, and
shall prioritize feedback on SIPs that are under consideration for ratification.

### SIP Editor Duties

By far the largest committee in the SIP process is the SIP Editor Committee.
The SIP Editors are responsible for maintaining the "inbound funnel" for SIPs
from the greater Stacks community. SIP Editors ensure that all inbound SIPs are
well-formed, relevant, and do not duplicate prior work (including rejected
SIPs).

#### Membership

Anyone may become a SIP Editor by recommendation from an existing SIP
Editor, subject to the “Recruitment” section below.

#### Qualifications

A SIP Editor must demonstrate proficiency in the SIP process and
formatting requirements. A candidate SIP Editor must demonstrate to an existing
SIP Editor that they can independently vet SIPs.

#### Responsibilities

SIP Editors are concerned with shepherding SIPs from Draft
status to Accepted status, and for mentoring community members who want to get
involved with the SIP processes (as applicable).

##### Getting Users Started

SIP Editors should be open and welcoming towards
enthusiastic users who want to help improve the greater Stacks ecosystem. As
such, SIP Editors should encourage users to submit SIPs if they have good ideas
that may be worth implementing.

In addition, SIP Editors should respond to public requests for help from
community members who want to submit a SIP. They may point them towards this
document, or towards other supplemental documents and tools to help them get
started.

##### Feedback

When a SIP is submitted in Draft status, a SIP Editor that takes the
SIP into consideration should provide fair and full feedback on how to make the
SIP ready for its transition to Accepted status. 

To do this, the SIP Editor should:

- Verify that the SIP is well-formed according to the criteria in this document
- Verify that the SIP has not been proposed before
- Verify as best that they can that the SIP is original work
- Verify that the SIP is appropriate for its type and consideration
- Recommend additional Considerations if appropriate
- Ensure that the text is clear, concise, and grammatically-correct English
- Ensure that there are appropriate avenues for discussion of the SIP listed in
  the preamble.

The SIP Editor does not need to provide public feedback to the SIP authors, but
should add their name(s) to the Signed-off field in the SIP preamble once the
SIP is ready to be Accepted.

##### Acceptance

Once a SIP is moved to Accepted, the SIP Editor shall assign it the
smallest positive number not currently used to identify any other SIP. Once that
number is known, the SIP Editor shall set the SIP's status to Accepted, set the
number, and commit the SIP to the SIP repository in order to make it visible to
other SIP Editors and to the Consideration Advisory Boards.

##### Recruitment

Each SIP Editor must list their name and contact information in an
easy-to-find location in the SIP repository, as well list of each SIP Editor
they recommended.  In so doing, the SIP Editors shall curate an “invite tree”
that shows which Editors recommended which other Editors.

A SIP Editor may recommend another user to be a SIP Editor no more than once per
month, and only if they have faithfully moved at least one SIP to Accepted
status in the last quarter.  If a SIP Editor does not participate in editing a
SIP for a full year and a day, then they may be removed from the SIP Editor
list.  The SC may remove a SIP Editor (and some or all of the users he or she
recommended) if they find that the SIP Editor has violated the SIP Code of
Conduct.

Newly-Accepted SIPs, new SIP Editor recruitment, and SIP Editor retirement shall
be submitted as pull requests by SIP Editors to the SIP repository.

## SIP Workflow

The lifecycle of a SIP is summarized in the flow-chart below:

```
    ------------------
    |     Draft      |  <-------------------------. Revise and resubmit
    ------------------                            |
           |                             --------------------
    Submit to SIP Editor ------------->  |     Rejected     |
           |                             --------------------
           |                                      ^
           V                                      |
    ------------------                            |
    |   Accepted     | -------------------------/ | /--------------------------------.
    ------------------                            |                                  |
           |                             --------------------                        |
    Review by Consideration ---------->  |     Rejected     |                        | 
    Advisory Board(s)                    --------------------                        |
           |                                      ^                                  |
           V                                      |                                  |
    -------------------------                     |                                  |
    |      Recommended       | -----------------/ | /------------------------------->|
    -------------------------                     |                                  |
           |                              --------------------                       |
    Vote by the Steering    ----------->  |    Rejected      |                       |
    Committee for activation              --------------------                       |
           |                                      ^                                  |
           V                                      |                                  |
    --------------------------                    |                                  |
    | Activation-in-Progress | -----------------/ | /------------------------------->|
    --------------------------                    |                                  |
           |                             ---------------------                       |
    All activation  ------------------>  |     Rejected      |                       |
    criteria are met       |             ---------------------  ------------------   |
           |               |----------------------------------> |    Obsolete    |   |
           V               |      ---------------------         ------------------   |
    ------------------     *--->  |     Replaced      | --------------->|<-----------*
    |   Ratified     |            ---------------------                 | 
    ------------------                                                  V
                                                                -------------------
                                                                |    Withdrawn    |
                                                                ------------------- 
```

When a SIP is transitioned to Rejected, it is not deleted, but is preserved in
the SIP repository so that it can be referenced as related or prior work by
other SIPs. Once a SIP is Rejected, it may be re-submitted as a Draft at a later
date. SIP Editors may decide how often to re-consider rejected SIPs as an
anti-spam measure, but the Steering Committee and Consideration Advisory Boards
may opt to independently re-consider rejected SIPs at their own discretion.

Once a SIP has been moved to Ratified status, the only changes that may be made
to it are fixing errata and adding supplementary materials.  Substantial changes
to the SIP's body should be done as a separate SIP.

## Public Venues for Conducting Business

The canonical set of SIPs in all state shall be recorded in the same medium that
the canonical copy of this SIP is.  Right now, this is in the Github repository
`https://github.com/stacksgov/sips`, but may be changed before this SIP is
ratified.  New SIPs, edits to SIPs, comments on SIPs, and so on shall be
conducted through Github's facilities for the time being.

In addition, individual committees may set up and use public mailing lists for
conducting business.  The Stacks Open Internet Foundation shall provide a means
for doing so.  Any discussions on the mailing lists that lead to non-trivial
contributions to SIPs should be referenced by these SIPs as supplemental
material.

### Github-specific Considerations

All SIPs shall be submitted as pull requests, and all SIP edits (including status
updates) shall be submitted as pull requests.  The SC, or one or more
individuals or entities appointed by the SC, shall be responsible for merging
pull requests to the main branch.

## SIP Copyright & Licensing

Each SIP must identify at least one acceptable license in its preamble. Source
code in the SIP can be licensed differently than the text. SIPs whose reference
implementation(s) touch existing reference implementation(s) must use the same
license as the existing implementation(s) in order to be considered. Below is a
list of recommended licenses.

- BSD-2-Clause: OSI-approved BSD 2-clause license
- BSD-3-Clause: OSI-approved BSD 3-clause license
- CC0-1.0: Creative Commons CC0 1.0 Universal
- GNU-All-Permissive: GNU All-Permissive License
- GPL-2.0+: GNU General Public License (GPL), version 2 or newer
- LGPL-2.1+: GNU Lesser General Public License (LGPL), version 2.1 or newer

# Related Work

The governance process proposed in this SIP is inspired by the Python PEP
process [1], the Bitcoin BIP2 process [2], the Ethereum Improvement Proposal [3]
processes, the Zcash governance process [4], and the Debian GNU/Linux
distribution governance process [5].  This SIP describes a governance process
where top-level decision-making power is vested in a committee of elected
representatives, which distinguishes it from Debian (which has a single elected
project leader), Python (which has a benevolent dictator for life), and Bitcoin
and ZCash (which vest all decision ratification power solely in the blockchain
miners).  The reason for a top-level steering committee is to ensure that
decision-making power is not vested in a single individual, but also to ensure
that the individuals responsible for decisions are accountable to the community
that elects them (as opposed to only those who have the means to participate
in mining).  This SIP differs from Ethereum's governance
process in that the top-level decision-making body (the "Core Devs" in Ethereum,
and the Steering Committee in Stacks) is not only technically proficient to evaluate
SIPs, but also held accountable through an official governance
process.

[1] https://www.python.org/dev/peps/pep-0001/

[2] https://github.com/bitcoin/bips/blob/master/bip-0002.mediawiki

[3] https://eips.ethereum.org/

[4] https://www.zfnd.org/governance/

[5] https://debian-handbook.info/browse/stable/sect.debian-internals.html

# Activation

This SIP activates once following tasks have been carried out:

- The provisional Steering Committee must be appointed by the Stacks Open Internet
  Foundation Board.
- Mailing lists for the initial committees must be created.
- The initial Consideration Advisory Boards must be formed, if there is interest
  in doing so before this SIP activates.
- A public, online SIP repository must be created to hold all non-Draft SIPs, their edit
  histories, and their feedbacks.
- A directory of Consideration Advisory Boards must be established (e.g. within
  the SIP repository).
- A SIP Code of Conduct should be added as a supplemental document
- The Stacks blockchain mainnet must launch.

# Reference Implementation

Not applicable.


          
# Preamble

SIP Number: 008

Title: Clarity Parsing and Analysis Cost Assessment

Author: Aaron Blankstein <aaron@blockstack.com>

Consideration: Technical

Type: Consensus

Status: Ratified

Created: 5 March 2020

License: BSD 2-Clause

Sign-off: Jude Nelson <jude@stacks.org>, Technical Steering Committee Chair

Discussions-To: https://github.com/stacksgov/sips

# Abstract

This document describes the measured costs and asymptotic costs
assessed for parsing Clarity code into an abstract syntax tree (AST)
and the static analysis of that Clarity code (type-checking and
read-only enforcement). This will not specify the _constants_
associated with those asymptotic cost functions. Those constants will
necessarily be measured via benchmark harnesses and regression
analyses.

# Introduction

The cost of analyzing Clarity code is measured using the same 5 categories
described in SIP-006 for the measurement of execution costs:

1. Runtime cost: captures the number of cycles that a single
   processor would require to process the Clarity block. This is a
   _unitless_ metric, so it will not correspond directly to cycles,
   but rather is meant to provide a basis for comparison between
   different Clarity code blocks.
2. Data write count: captures the number of independent writes
   performed on the underlying data store (see SIP-004).
3. Data read count: captures the number of independent reads
   performed on the underlying data store.
4. Data write length: the number of bytes written to the underlying
   data store.
5. Data read length: the number of bytes read from the underlying
   data store.

Importantly, these costs are used to set a _block limit_ for each
block.  When it comes to selecting transactions for inclusion in a
block, miners are free to make their own choices based on transaction
fees, however, blocks may not exceed the _block limit_. If they do so,
the block is considered invalid by the network --- none of the block's
transactions will be materialized and the leader forfeits all rewards
from the block.

Costs for static analysis are assessed during the _type check_ pass.
The read-only and trait-checking passes perform work which is strictly
less than the work performed during type checking, and therefore, the
cost assessment can safely fold any costs that would be incurred during
those passes into the type checking pass.

# Specification

## Common Analysis Metrics and Costs

### AST Parsing

The Clarity parser has a runtime that is linear with respect to the Clarity
program length.

```
a*X+b
```

where a and b are constants, and

X := the program length in bytes

### Dependency cycle detection

Clarity performs cycle detection for intra-contract dependencies (e.g.,
functions that depend on one another). This detection is linear in the
number of dependency edges in the smart contract:

```
a*X+b
```

where a and b are constants, and
X := the total number of dependency edges in the smart contract

Dependency edges are created anytime a top-level definition refers 
to another top-level definition.

### Type signature size

Types in Clarity may be described using type signatures. For example,
`(tuple (a int) (b int))` describes a tuple with two keys `a` and `b`
of type `int`. These type descriptions are used by the Clarity analysis
passes to check the type correctness of Clarity code. Clarity type signatures
have varying size, e.g., the signature `int` is smaller than the signature for a
list of integers.

The signature size of a Clarity type is defined as follows:

```
type_signature_size(x) :=
  if x = 
     int      => 1
    uint      => 1
    bool      => 1
    principal => 1
    buffer    => 2
    optional  => 1 + type_signature_size(entry_type)
    response  => 1 + type_signature_size(ok_type) + type_signature_size(err_type)
    list      => 2 + type_signature_size(entry_type)
    tuple     => 1 + 2*(count(entries)) 
                   + sum(type_signature_size for each entry)
                   + sum(len(key_name) for each entry)
```

### Type annotation

Each node in a Clarity contract's AST is annotated with the type value
for that node during the type checking analysis pass.

The runtime cost of type annotation is:

```
a + b*X
```

where a and b are constants, and X is the type signature size of the
type being annotated.

### Variable lookup

Looking up variables during static analysis incurs a non-constant cost -- the stack
depth _and_ the length of the variable name affect this cost. However,
variable names in Clarity have bounded length -- 128 characters. Therefore,
the cost assessed for variable lookups may safely be constant with respect
to name length.

The stack depth affects the lookup cost because the variable must be
checked for in each context on the stack.

Cost Function:

```
a*X+b*Y+c
```

where a, b, and c are constants,
X := stack depth
Y := the type size of the looked up variable

### Function Lookup

Looking up a function incurs a constant cost with respect
to name length (for the same reason as variable lookup). However,
because functions may only be defined in the top-level contract
context, stack depth does not affect function lookup.

Cost Function:

```
a*X + b
```

where a and b are constants,
X := the sum of the type sizes for the function signature (each argument's type size, as well
    as the function's return type)

### Name Binding

The cost of binding a name in Clarity -- in either a local or the contract
context is _constant_ with respect to the length of the name, but linear in
the size of the type signature.

```
binding_cost = a + b*X
```

where a and b are constants, and
X := the size of the bound type signature

### Type check cost

The cost of a static type check is _linear_ in the size of the type signature:

```
type_check_cost(expected, actual) :=
  a + b*X
```

where a and b are constants, and

X := `max(type_signature_size(expected), type_signature_size(actual))`

### Function Application

Static analysis of a function application in Clarity requires
type checking the function's expected arguments against the
supplied types.

The cost of applying a function is:


```
a + sum(type_check_cost(expected, actual) for each argument)
```

where a is a constant.

This is also the _entire_ cost of type analysis for most function calls
(e.g., intra-contract function calls, most simple native functions). 

### Iterating the AST

Static analysis iterates over the entire program's AST in the type checker,
the trait checker, and in the read-only checker. This cost is assessed
as a constant cost for each node visited in the AST during the type
checking pass.

## Special Function Costs

Some functions require additional work from the static analysis system.

### Functions on sequences (e.g., map, filter, fold)

Functions on sequences need to perform an additional check that the
supplied type is a list or buffer before performing the normal
argument type checking. This cost is assessed as:

```
a
```

where a is a constant.

### Functions on options/responses

Similarly to the functions on sequences, option/response functions
must perform a simple check to see if the supplied input is an option or
response before performing additional argument type checking. This cost is
assessed as:

```
a
```

### Data functions (ft balance checks, nft lookups, map-get?, ...)

Static checks on intra-contract data functions do not require database lookups
(unlike the runtime costs of these functions). Rather, these functions
incur normal type lookup (i.e., fetching the type of an NFT, data map, or data var)
and type checking costs.

### get

Checking a tuple _get_ requires accessing the tuple's signature
for the specific field. This has runtime cost:

```
a*log(N) + b
```
where a and b are constants, and

N := the number of fields in the tuple type

### tuple

Constructing a tuple requires building the tuple's BTree for
accessing fields. This has runtime cost:


```
a*N*log(N) + b
```
where a and b are constants, and

N := the number of fields in the tuple type

### use-trait

Importing a trait imposes two kinds of costs on the analysis.
First, the import requires a database read. Second, the imported
trait is included in the static analysis output -- this increases
the total storage usage and write length of the static analysis.

The costs are defined as:

```
read_count = 1
write_count = 0
runtime = a*X+b
write_length = c*X+d
read_length = c*X+d
```

where a, b, c, and d are constants, and

X := the total type size of the trait (i.e., the sum of the
    type sizes of each function signature).

### contract-call?

Checking a contract call requires a database lookup to inspect
the function signature of a prior smart contract.

The costs are defined as:

```
read_count = 1
read_length = a*X+b
runtime = c*X+d
```

where a, b, c, and d are constants, and

X := the total type size of the function signature

### let

Let bindings require the static analysis system to iterate over
each let binding and ensure that they are syntactically correct.

This imposes a runtime cost:

```
a*X + b
```
where a and b are constants, and

X := the number of entries in the let binding.

# Related Work

This section will be expanded upon after this SIP is ratified.

# Backwards Compatibility

Not applicable.

# Activation

At least 20 miners must register a name in the `.miner` namespace in Stacks 1.0.
Once the 20th miner has registered, the state of Stacks 1.0 will be snapshotted.
300 Bitcoin blocks later (Bitcoin block 666050), the Stacks 2.0 blockchain will launch.  Stacks 2.0
implements this SIP.

# Reference Implementations

Implemented in Rust.  See https://github.com/blockstack/stacks-blockchain.


          
# Preamble

SIP number: 003

Title: Stacks P2P Network

Author: Jude Nelson <jude@stacks.org>

Consideration: Technical

Type: Standard

Status: Ratified

Created: 28 February 2019

License: BSD 2-Clause

Sign-off: Jude Nelson <jude@stacks.org>, Technical Steering Committee Chair

Discussions-To: https://github.com/stacksgov/sips

# Abstract

This SIP describes the design of the Stacks peer network, used for relaying
blocks, transactions, and routing information.  The document describes both the
overall protocol design and rationale, and provides descriptions of each
message's wire format (where applicable).

# License and Copyright

This SIP is made available under the terms of the BSD-2-Clause license,
available at https://opensource.org/licenses/BSD-2-Clause.  This SIP's copyright
is held by the Stacks Open Internet Foundation.

# Introduction

The Stacks blockchain implements a peer-to-peer _reachability network_ in order
to ensure that each Stacks peer has a full copy of all blocks committed to on
the burn chain, and all unconfirmed transactions.  A full replica of the chain
state is necessary for user security -- users must be able to determine what
their account states are in order to know that any transactions they send from
them are valid as they are sent.  In addition, a full replica of all chain state
is desirable from a reliability perspective -- as long as there exists one
available replica, then it will be possible for new peers to bootstrap
themselves from it and determine the current state of the chain.  As such, the
network protocol is designed to help peers build full replicas while remaining
resilient to disruptions and partitions.

The Stacks peer network is designed with the following design goals in mind:

* **Ease of reimplementation**.  The rules for encoding and decoding messages
  are meant to be as simple as possible to facilitate implementing ancilliary software
that depends on talking to the peer network.  Sacrificing a little bit of space
efficiency is acceptable if it makes encoding and decoding simpler.

* **Unstructured reachability**.  The peer network's routing algorithm
  prioritizes building a _random_ peer graph such that there are many
_distinct_ paths between any two peers.  A random (unstructured) graph is
preferred to a structured graph (like a DHT) in order to maximize the number of next-hop
(neighbor) peers that a given peer will consider in its frontier.  When choosing neighbors, a peer
will prefer to maximize the number of _distinct_ autonomous systems represented
in its frontier in order to help keep as many networks on the Internet connected
to the Stacks peer network.

# Specification

The following subsections describe the data structures and protocols for the
Stacks peer network.  In particular, this document discusses _only_ the peer
network message structure and protocols.  It does _not_ document the structure
of Stacks transactions and blocks.  These structures are defined in SIP 005.

## Encoding Conventions

This section explains how this document will describe the Stacks messages, and
explains the conventions used to encode Stacks messages as a sequence of bytes.

All Stacks network messages are composed of _scalars_, _byte buffers_ of fixed
length, _vectors_ of variable length, and _typed containers_ of variable length.

A scalar is a number represented by 1, 2, 4, or 8 bytes, and is unsigned.
Scalars requiring 2, 4, and 8 bytes are encoded in network byte order (i.e. big-endian).

Byte buffers have known length and are transmitted as-is.

Vectors are encoded as length-prefixed arrays.  The first 4 bytes of a vector
are a scalar that encodes the vector's length.  As such, a vector may not have
more than 2^32 - 1 items.  Vectors are recursively defined in terms of other 
scalars, byte buffers, vectors, and typed containers.

A typed container is encoded as a 1-byte type identifier, followed by zero or
more encoded structures.  Typed containers are used in practice to encode
type variants, such as types of message payloads or types of transactions.
Typed containers are recursively-defined in terms of other scalars, byte
buffers, vectors, and type containers.  Unlike a vector, there is no length
field for a typed container -- the parser will begin consuming the container's
items immediately following the 1-byte type identifier.

**Example**

Consider the following message definitions:

```
// a byte buffer
pub struct SomeBytes([u8; 10]);

pub struct ExampleMessagePayload {
   pub aa: u16,
   pub bytes: SomeBytes
}

// will encode to a typed container
pub enum PayloadVariant {
   Foo(ExampleMessagePayload),
   Bar(u32)
}

pub const FOO_MESSAGE_TYPE_ID: u8 = 0x00;
pub const BAR_MESSAGE_TYPE_ID: u8 = 0x01;

// top-level message that will encode to a sequence of bytes
pub struct ExampleMessage {
   pub a: u8,
   pub b: u16,
   pub c: u32,
   pub d: u64,
   pub e: Vec<u64>,
   pub payload: PayloadVariant,
   pub payload_list: Vec<PayloadVariant>
}
```

Consider the following instantiation of an `ExampleMessage` on a little-endian
machine (such as an Intel x86):

```
let msg = ExampleMessage {
   a: 0x80,
   b: 0x9091,
   c: 0xa0a1a2a3,
   d: 0xb0b1b2b3b4b5b6b7,
   e: vec![0xc0c1c2c3c4c5c6c7, 0xd0d1d2d3d4d5d6d7, 0xe0e1e2e3e4e5e6e7],
   payload: PayloadVariant::Foo(
      ExampleMessagePayload {
         aa: 0xaabb,
         bytes: SomeBytes([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
      }
   ),
   payload_list: vec![
      PayloadVariant::Foo(
         ExampleMessagePayload {
            aa: 0xccdd,
            bytes: SomeBytes([0xa, 0xb, 0xc, 0xd, 0xe, 0xf, 0x10, 0x11, 0x12, 0x13])
         }
      ),
      PayloadVariant::Bar(0x00112233)
   ]
};
```

This message would serialize to the following bytes.  Note that each line represents 
a separate record to improve readability.

```
80                               # msg.a
90 91                            # msg.b in network byte order
a0 a1 a2 a3                      # msg.c in network byte order
b0 b1 b2 b3 b4 b5 b6 b7          # msg.d in network byte order
00 00 00 03                      # length of message.e, in network byte order (note that vector lengths are always 4 bytes)
c0 c1 c2 c3 c4 c5 c6 c7          # msg.e[0] in network byte order
d0 d1 d2 d3 d4 d5 d6 d7          # msg.e[1] in network byte order
e0 e1 e2 e3 e4 e5 e6 e7          # msg.e[2] in network byte order
00                               # PayloadVariant::Foo type ID
aa bb                            # msg.payload.aa, where msg.payload is PayloadVariant::Foo(ExampleMessagePayload)
00 01 02 03 04 05 06 07 08 09    # msg.payload.bytes, where msg.payload is PayloadVariant::Foo(ExampleMessagePayload)
00 00 00 02                      # length of msg.payload_list, in network byte order
00                               # PayloadVariant::Foo type ID
cc dd                            # msg.payload_list[0].aa, where msg.payload_list[0] is a PayloadVariant::Foo(ExampleMessagePayload)
0a 0b 0c 0d 0e 0f 10 11 12 13    # msg.payload_list[0].bytes, where msg.payload_list[0] is a PayloadVariant::Foo(ExampleMessagePayload)
01                               # PayloadVariant::Bar type ID
00 11 22 33                      # msg.payload_list[1].0, where msg.payload_list[1] is a PayloadVariant::Bar(u32) in network byte order
```

## Byte Buffer Types

The following byte buffers are used within Stacks peer messsages:

```
pub struct MessageSignature([u8; 65]);
```

This is a fixed-length container for storing a recoverable secp256k1
signature.  The first byte is the recovery code; the next 32 bytes are the `r`
parameter, and the last 32 bytes are the `s` parameter.  Because there are up to
two valid signature values for a secp256k1 curve, only the signature with the _lower_
value for `s` will be accepted.

```
pub struct PeerAddress([u8; 16]);
```

This is a fixed-length container for an IPv4 or an IPv6 address.

```
pub struct Txid([u8; 32]);
```

This is a container for a transaction ID.  Transaction IDs are 32-byte
cryptographic hashes.

```
pub struct BurnchainHeaderHash([u8; 32]);
```

This is a container for the hash of a burn chain block header, encoded as a
32-byte cryptographic hash.

```
pub struct BlockHeaderHash([u8; 32]);
```

This is a container for the hash of a Stacks block or a Stacks microblock header
hash, encoded as a 32-byte cryptographic hash.

```
pub struct Secp256k1PublicKey([u8; 33]);
```

This is a compressed secp256k1 public key, as used in Bitcoin and other
cryptocurrencies.

```
pub struct DoubleSha256([u8; 32]);
```

This is a SHA256 hash applied twice to some data.

```
pub struct Sha512Trunc256([u8; 32]);
```

This is a container for a SHA512/256 hash.

```
pub struct TrieHash([u8; 32]);
```

This is a container for a MARF merkle hash (see SIP-004).

```
pub struct UrlString(Vec<u8>);
```

This is a container for an ASCII string that encodes a URL.  It is encoded as
follows:
* A 1-byte length prefix
* The string's bytes, as-is.

### Common Data Structures

This section details common data structures used in multiple messages.

**Neighbor Address**

The network address of a Stacks peer is encoded as follows:

```
pub struct NeighborAddress {
    /// The IPv4 or IPv6 address of this peer
    pub addrbytes: PeerAddress,

    /// The port this peer listens on
    pub port: u16,

    /// The RIPEMD160-SHA256 hash of the node's public key.
    /// If this structure is used to advertise knowledge of another peer,
    /// then this field _may_ be used as a hint to tell the receiver which
    /// public key to expect when it establishes a connection.
    pub public_key_hash: Hash160
}
```

**Relay Data**

Messages in the network preserve the order of peers that send them.  This
information is encoded as follows:

```
pub struct RelayData {
    /// The peer that relayed a message
    pub peer: NeighborAddress,

    /// The sequence number of that message (see the Preamble structure below)
    pub seq: u32,
}
```

## Messages

All Stacks messages have three components:

* A fixed-length **preamble** which describes some metadata about the peer's view of the
  network.

* A variable-length but bound-sized **relayers** vector which describes the order of peers that
  relayed a message.

* A variable-length **payload**, which encodes a specific peer message as a
  typed container.

All Stacks messages are represented as:

```
pub struct StacksMessage {
    pub preamble: Preamble,
    pub relayers: Vec<RelayData>,
    pub payload: StacksMessageType
}
```

The preamble has the following fields.  Descriptions of each field are provided
in-line.

```
pub struct Preamble {
    /// A 4-byte scalar to encode the semantic version of this software.
    /// The only valid value is 0x15000000 (i.e. version 21.0.0.0).
    pub peer_version: u32,

    /// A 4-byte scalar to encode which network this peer belongs to.
    /// Valid values are:
    ///   0x15000000 -- this is "mainnet"
    ///   0x15000001 -- this is "testnet"
    pub network_id: u32,

    /// A 4-byte scalar to encode the message sequence number. A peer will
    /// maintain a sequence number for each neighbor it talks to, and will
    /// increment it each time it sends a new message (wrapping around if
    /// necessary).
    pub seq: u32,

    /// This is the height of the last burn chain block this peer processed.
    /// If the peer is all caught up, this is the height of the burn chain tip.
    pub burn_block_height: u64,

    /// This is the burn block hash calculated at the burn_block_height above.
    /// It uniquely identifies a burn chain block.
    pub burn_header_hash: BurnchainHeaderHash,

    /// This is the height of the last stable block height -- i.e. the largest
    /// block height at which a block can be considered stable in the burn
    /// chain history.  In Bitcoin, this is at least 7 blocks behind block_height.
    pub stable_burn_block_height: u64,

    /// This is the hash of the last stable block's header.
    pub stable_burn_header_hash: BurnchainHeaderHash,

    /// This is a pointer to additional data that follows the payload.
    /// This is a reserved field; for now, it should all be 0's.
    pub additional_data: u32,

    /// This is a signature over the entire message (preamble and payload).
    /// When generating this value, the signature bytes below must all be 0's.
    pub signature: MessageSignature;

    /// This is the length of the message payload.
    pub payload_len: u32;
}
```

A payload is a typed container, and may be any of the following enumerated
types:

```
pub enum StacksMessageType {
    Handshake(HandshakeData),
    HandshakeAccept(HandshakeAcceptData),
    HandshakeReject,
    GetNeighbors,
    Neighbors(NeighborsData),
    GetBlocksInv(GetBlocksData),
    BlocksInv(BlocksInvData),
    GetPoxInv(GetPoxInv),
    PoxInv(PoxInvData),
    BlocksAvailable(BlocksAvailableData),
    MicroblocksAvailable(MicroblocksAvailableData),
    Blocks(BlocksData),
    Microblocks(MicroblocksData),
    Transaction(StacksTransaction),
    Nack(NackData),
    Ping,
    Pong
}
```

## Payloads

**Handshake**

Type identifier: 0

Structure:

```
pub struct HandshakeData {
    /// Address of the peer sending the handshake
    pub addrbytes: PeerAddress,
    pub port: u16,

    /// Bit field of services this peer offers.
    /// Supported bits:
    /// -- SERVICE_RELAY = 0x0001 -- must be set if the node relays messages
    ///                              for other nodes.
    pub services: u16,

    /// This peer's public key
    pub node_public_key: Secp256k1PublicKey,

    /// Burn chain block height at which this key will expire
    pub expire_block_height: u64,

    /// HTTP(S) URL to where this peer's block data can be fetched
    pub data_url: UrlString
}
```

**HandshakeAccept**

Type identifier: 1

Structure:

```
pub struct HandshakeAcceptData {
    /// The remote peer's handshake data
    pub handshake: HandshakeData,

    /// Maximum number of seconds the recipient peer expects this peer
    /// to wait between sending messages before the recipient will declare
    /// this peer as dead.
    pub heartbeat_interval: u32,
}
```

**HandshakeReject**

Type identifier: 2

Structure: [empty]

**GetNeighbors**

Type identifier: 3

Structure: [empty]

**Neighbors**

Type identifier: 4

Structure:

```
pub struct NeighborsData {
    /// List of neighbor addresses and public key hints.
    /// This vector will be at most 128 elements long.
    pub neighbors: Vec<NeighborAddress>
}
```

**GetBlocksInv**

Type identifier: 5

Structure:

```
pub struct GetBlocksInv {
    /// The consensus hash at the start of the requested reward cycle block range
    pub consensus_hash: ConsensusHash, 
    /// The number of blocks after to this consensus hash, including the block
    /// that corresponds to this consensus hash.
    pub num_blocks: u16 
}
```

Notes:

* Expected reply is a `BlocksInv`.
* `consensus_hash` must correspond to a (burnchain) block at the start of a PoX reward cycle
* `num_blocks` cannot be more than the PoX reward cycle length (see SIP-007).

**BlocksInv**

Type identifier: 6

Structure:

```
pub struct BlocksInvData {
    /// Number of bits represented in the bit vector below.
    /// Represents the number of blocks in this inventory.
    pub bitlen: u16,

    /// A bit vector of which blocks this peer has.  bitvec[i]
    /// represents the availability of the next 8*i blocks, where
    /// bitvec[i] & 0x01 represents the availability of the (8*i)th block, and
    /// bitvec[i] & 0x80 represents the availability of the (8*i+7)th block.
    /// Each bit corresponds to a sortition on the burn chain, and will be set
    /// if this peer has the winning block data
    pub block_bitvec: Vec<u8>,

    /// A bit vector for which confirmed microblock streams this peer has.
    /// The ith bit represents the presence/absence of the ith confirmed
    /// microblock stream.  It is in 1-to-1 correspondance with block_bitvec.
    pub microblocks_bitvec: Vec<u8>
}
```

Notes:

* `BlocksInvData.bitlen` will never exceed 4096
* `BlocksInvData.block_bitvec` will have length `ceil(BlocksInvData.bitlen / 8)`
* `BlocksInvData.microblocks_bitvec` will have length `ceil(BlocksInvData.bitlen / 8)`

**GetPoxInv**

Type identifier: 7

Structure:

```
pub struct GetPoxInv {
    /// The consensus hash at the _beginning_ of the requested reward cycle range
    pub consensus_hash: ConsensusHash,
    /// The number of reward cycles to request (number of bits to expect)
    pub num_cycles: u16
}
```

Notes:

* Expected reply is a `PoxInv`
* `num_cycles` cannot be more than 4096

**PoxInv**

Type identifier: 8

Structure:

```
pub struct PoxInvData {
    /// Number of reward cycles encoded
    pub bitlen: u16,
    /// Bit vector representing the remote node's PoX vector.
    /// A bit will be `1` if the node is certain about the status of the 
    /// reward cycle's PoX anchor block (it either cannot exist, or the
    /// node has a copy), or `0` if the node is uncertain (i.e. it may exist
    /// but the node does not have a copy if it does).
    pub pox_bitvec: Vec<u8>
}
```

Notes:
* `bitlen` should be at most `num_cycles` from the corresponding `GetPoxInv`.

**BlocksAvailable**

Type identifier: 9

Structure:

```
pub struct BlocksAvailableData {
    /// List of blocks available
    pub available: Vec<(ConsensusHash, BurnchainHeaderHash)>,
}
```

Notes:

* Each entry in `available` corresponds to the availability of an anchored
  Stacks block from the sender.
* `BlocksAvailableData.available.len()` will never exceed 32.
* Each `ConsensusHash` in `BlocksAvailableData.available` must be the consensus
  hash calculated by the sender for the burn chain block identified by
`BurnchainHeaderHash`.

**MicroblocksAvailable**

Type identifier: 10

Structure:

```
// Same as BlocksAvailable
```

Notes:

* Each entry in `available` corresponds to the availability of a confirmed
  microblock stream from the sender.
* The same rules and limits apply to the `available` list as in
  `BlocksAvailable`.
   
**Blocks**

Type identifier: 11

Structure:

```
pub struct BlocksData {
    /// A list of blocks pushed, paired with the consensus hashes of the
    /// burnchain blocks that selected them
    pub blocks: Vec<(ConsensusHash, StacksBlock)>
}

pub struct StacksBlock {
   /// Omitted for brevity; see SIP 005
}
```

**Microblocks**

Type identifier: 12

Structure:

```
pub struct MicroblocksData {
    /// "Index" hash of the StacksBlock that produced these microblocks.
    /// This is the hash of both the consensus hash of the burn chain block
    /// operations that selected the StacksBlock, as well as the StacksBlock's
    /// hash itself.
    pub index_anchor_hash: StacksBlockId,
    /// A contiguous sequence of microblocks.
    pub microblocks: Vec<StacksMicroblock>
}

pub struct StacksMicroblock {
   /// Omited for brevity; see SIP 005
}
```

**Transaction**

Type identifier: 13

Structure:

```
pub struct StacksTransaction {
   /// Omitted for brevity; see SIP 005
}
```

**Nack**

Type identifier: 14

Structure:

```
pub struct NackData {
   /// Numeric error code to describe what went wrong
   pub error_code: u32
}
```

**Ping**

Type identifier: 15

Structure:

```
pub struct PingData {
   /// Random number
   nonce: u32
}
```

**Pong**

Type identifier: 16

Structure:

```
pub struct PongData {
   /// Random number
   nonce: u32
}
```

**NatPunchRequest**

Type identifier: 17

Structure:

```
/// a 4-byte nonce unique to this request
u32
```

**NatPunchReply**

Type identifier: 18

Structure:

```
pub struct NatPunchData {
   /// The public IP address, as reported by the remote peer
   pub addrbytes: PeerAddress,
   /// The public port
   pub port: u16,
   /// The nonce from the paired NatPunchRequest
   pub nonce: u32,
}
```

Notes:
* The `nonce` field in a `PongData` should match the `nonce` field sent by the
  corresponding `Ping`.


## Protocol Description

This section describes the algorithms that make up the Stacks peer-to-peer
network.  In these descriptions, there is a distinct **sender peer** and a
distinct **receiver peer**.

### Network Overview

The Stacks peer network has a dedicated _control plane_ and _data plane_.  They
listen on different ports, use different encodings, and fulfill different roles.

The control-plane is implemented via sending messages using the encoding
described above.  It is concerned with the following tasks:
* Identifying and connecting with other Stacks peer nodes
* Crawling the peer graph to discover a diverse set of neighbors
* Discovering peers' data-plane endpoints
* Synchronizing block and microblock inventories with other peers.

The data-plane is implemented via HTTP(S), and is concerned with both fetching
and relaying blocks, microblocks, and transactions.

Each Stacks node implements the control-plane protocol in order to help other
nodes discover where they can fetch blocks.  However, Stacks nodes do _not_ need
to implement the data plane.  They can instead offload some or all of this responsibility to
other Stacks nodes, Gaia hubs, and vanilla HTTP servers.  The reason for this is
to **preserve compatibility with existing Web infrastructure** like cloud
storage and CDNs for doing the "heavy lifting" for propagating the blockchain
state.

### Creating a Control-Plane Message

All control-plane messages start with a `Preamble`.  This allows peers to identify other peers
who (1) have an up-to-date view of the underlying burn chain, and (2) are part
of the same fork set.  In addition, the `Preamble` allows peers to authenticate
incoming messages and verify that they are not stale.

All control-plane messages are signed with the node's session private key using ECDSA on the
secp256k1 curve.  To sign a `StacksMessage`, a peer uses the following algorithm:

1. Serialize the `payload` to a byte string.
2. Set the `preamble.payload_len` field to the length of the `payload` byte string
3. Set the `preamble.seq` field to be the number of messages sent to
   this peer so far.
4. Set the `preamble.signature` field to all 0's
5. Serialize the `preamble` to a byte string.
6. Calculate the SHA512/256 over the `preamble` and `payload` byte strings
7. Calculate the recoverable secp256k1 signature from the SHA256

### Receiving a Control-Plane Message

Because all control-plane messages start with a fixed-length `Preamble`, a peer receives a
message by first receiving the `Preamble`'s bytes and decoding it.  If the bytes
decode successfully, the peer _then_ receives the serialized payload, using the
`payload_len` field in the `Preamble` to determine how much data to read.  To
avoid memory exhaustion, **the payload may not be more than 32 megabytes**.

Once the preamble and payload message bytes are loaded, the receiver peer
verifies the message as follows:

1. Calculate the SHA256 hash over the serialized `preamble` and the payload
   bytes
2. Extract the recoverable signature from `preamble.signature`
3. Verify the signature against the sender peer's public key.
4. Verify that the `seq` field of the payload is greater than any
   previously-seen `seq` value for this peer.
5. Parse the payload typed container bytes into a `Payload`

### Error Handling

If anything goes wrong when communicating with a peer, the receiver may reply
with a `Nack` message with an appropriate error code.  Depending on the error,
the sender should try again, close the socket and re-establish the connection, or
drop the peer from its neighbor set altogether.  In particular, if a peer
receives an _invalid_ message from a sender, the peer should blacklist the remote
peer for a time (i.e. ignore any future messages from it).

Different aspects of the control-plane protocol will reply with different error codes to
convey exactly what went wrong.  However, in all cases, if the preamble is
well-formed but identifies a different network ID, a version field
with a different major version than the local peer, or different stable
burn header hash values, then both the sender and receiver peers should blacklist each other.

Because peers process the burn chain up to its chain tip, it is possible for
peers to temporarily be on different fork sets (i.e. they will have different
burn header hashes for the given chain tip, but will have the same values for
the stable burn header hashes at each other's `stable_block_height`'s).
In this case, both peers should take it as a hint to first check
that their view of the burn chain is consistent (if they have not done so
recently).  They may otherwise process and react to each other's messages
without penalty.

Peers are expected to be both parsimonious and expedient in their communication.
If a remote peer sends too many valid messages too quickly, the peer
may throttle or blacklist the remote peer.  If a remote peer
is sending data too slowly, the recipient may terminate the connection in order
to free resources for serving more-active peers.

### Connecting to a Peer's Control Plane

Connecting to a peer's control-plane is done in a single round as follows:

1.  The sender peer creates a `Handshake` message with its address, services,
    and public key and sends it to the receiver.
2.  The receiver replies with a `HandshakeAccept` with its public key and
    services.

On success, the sender adds the receiver to its frontier set.  The receiver may
do so as well, but this is not required.

If the receiver is unable to process the `Handshake`, the receiver should
reply with a `HandshakeReject` and temporarily blacklist the sender for a time.
Different implementations may have different considerations for what constitutes
an invalid `Handshake` request.  A `HandshakeReject` response should be used
only to indicate that the sender peer will be blacklisted.  If the `Handshake`
request cannot be processed for a _recoverable_ reason, then the receiver should
reply with a `Nack` with the appropriate error code to tell the sender to try
again.

When executing a handshake, a peer should _not_ include any other peers in the
`relayers` vector except for itself.  The `relayers` field will be ignored.

### Learning the public IP address

Before the peer can participate in the control plane, it must know its
publicly-routable IP address so it can exchange it with its remote neighbors.
This is necessary, since other neighbors-of-neighbors will learn this peer's
public IP address from its remote neighbors, and thus must have a publicly-routable
address if they are going to handshake with it.

The peer may take an operator-given public IP address.  If no public IP address
is given, the peer will learn the IP address using the `NatPunchRequest` and
`NatPunchReply` messages as follows:

1. The peer sends a `NatPunchRequest` to a randomly-chosen initial neighbor it has
   already handshaked with.  It uses a random nonce value.
2. The remote neighbor replies with a (signed) `NatPunchReply` message, with its
   `addrbytes` and `port` set to what it believes the public IP is (based on the
   underlying socket's peer address).
3. Upon receipt of the `NatPunchReply`, the peer will have confirmed its public
   IP address, and will send it in all future `HandshakeAccept` messages.  It
   will periodically re-learn its IP address, if it was not given by the
   operator.

Because the peer's initial neighbors are chosen by the operator as being
sufficiently trustworthy to supply network information for network walks, it is
reasonable to assume that they can also be trusted to tell a bootstrapping peer
its public IP address.

### Checking a Peer's Liveness

A sender peer can check that a peer is still alive by sending it a `Ping`
message on the control-plane.  The receiver should reply with a `Pong` message.  Both the sender
and receiver peers would update their metrics for measuring each other's
responsiveness, but they do _not_ alter any information about each other's
public keys and expirations.

Peers will ping each other periodically this way to prove that they are still alive.
This reduces the likelihood that they will be removed from each other's
frontiers (see below).

### Exchanging Neighbors

Peers exchange knowledge about their neighbors on the control-plane as follows:

1. The sender peer creates a `GetNeighbors` message and sends it to the
   receiver.
2. The receiver chooses up to 128 neighbors it knows about and replies to the
   sender with them as a `Neighbors` message.  It provides the hashes of their session public keys (if
known) as a hint to the sender, which the sender may use to further
authenticate future neighbors.
3. The sender sends `Handshake` messages to a subset of the replied neighbors,
   prioritizing neighbors that are not known to the sender or have not been
recently contacted.
4. The various neighbors contacted reply either `HandshakeAccept`,
   `HandshakeReject`, or `Nack` messages.  The sender updates its frontier with
knowledge gained from the `HandshakeAccept` messages.

On success, the sender peer adds zero or more of the replied peer addresses to
its frontier set.  The receiver and its contacted neighbors do nothing but
update their metrics for the sender.

If the sender receives an invalid `Neighbors` reply with more than 128
addresses, the sender should blacklist the receiver.

The sender is under no obligation to trust the public key hashes in the
`Neighbors` request.  However, if the sender trusts the receiver, then they can
be used as hints on the expected public keys if the sender subsequently
attempts to connect with these neighbors.  Deciding which nodes to trust
with replying true neighbor information is a peer-specific configuration option.

The receiver may reply with a `Nack` if it does not wish to divulge its
neighbors.  In such case, the sender should not ask this receiver for neighbors
again for a time.

### Requesting Blocks on the Data-Plane

Peers exchange blocks in a 2-process protocol:  the sender first queries the
receiver for the blocks it has via the control-plane,
and then queries individual blocks and microblocks on the data-plane.

On the control-plane, the sender builds up a locally-cached inventory of which
blocks the receiver has.  To do so, the sender and receiver execute a two-phase
protocol to synchronize the sender's view of the receiver's block inventroy.
First, the sender downloads the receiver's knowledge of PoX reward cycles,
encoded as a bit vector where a `1` in the _ith_ position means that the
receiver is certain about the status of the PoX anchor block in the _ith_ reward
cycle (i.e. it either does not exist, or it does exist and the receiver has a
copy).  It is `0` otherwise -- i.e. it may exist, but the receiver does not have
a copy.

To synchronize the PoX anchor block knowledge, the sender and receiver do the following:

1.  The sender creates a `GetPoxInv` message for the range of PoX reward cycles
    it wants, and sends it to the receiver.
2.  If the receiver recognizes the consensus hash in the `GetPoxInv` message, it
    means that the receiver agrees on all PoX state that the sender does, up to the
burn chain block that this consensus hash represents (note that the consensus
hash must correspond to a burn chain block at the start of a reward cycle).  The receiver replies with
a `PoxInv` with its knowledge of all reward cycles at and after the reward cycle
identified by that consensus hash.
3.  The sender and receiver continue to execute this protocol until the receiver
    shares all of its PoX reward cycle knowledge, or it encounters a consensus
hash from the sender that it does not recognize.  If the latter happens, the
receiver shall reply with a `Nack` with the appropriate error code.

Once the sender has downloaded the PoX anchor block knowledge from the receiver,
it proceeds to fetch an inventory of all block and microblock knowledge from the
receiver for all PoX reward cycles that it agrees with the receiver on.  That
is, it will fetch block and microblock inventory data for all reward cycles in
which the sender and receiver both have a `1` or both have a `0` in the _ith_ bit position,
starting from the first-ever reward cycle, and up to either the lowest reward cycle in
which they do not agree (or the end of the PoX vector, whichever comes first).
They proceed as follows:

1.  The sender creates a `GetBlocksInv` message for reward cycle _i_,
    and sends it to the receiver.
2.  If the receiver has processed the range of blocks represented by the `GetBlocksInv` 
    block range -- i.e. it recognizes the consensus hash in `GetBlocksInv` as
the start of a reward cycle -- then the receiver creates a `BlocksInv` message and replies
with it.  The receiver's inventory bit vectors may be _shorter_ than the
requested range if the request refers to blocks at the burn chain tip.  The
receiver sets the _ith_ bit in the blocks inventory if it has the corresponding
block, and sets the _ith_ bit in the microblocks inventory if it has the
corresponding _confirmed_ microblock stream.
3.  The sender repeats the process for reward cycle _i+1_, so long as both it
    and the receiver are both certain about the PoX anchor block for reward
cycle _i+1_, or both are uncertain.  If this is not true, then the sender stops
downloading block and microblock inventory from the receiver, and will assume
that any blocks in or after this reward cycle are unavailable from the receiver.

The receiver peer may reply with a `PoxInv` or `BlocksInv` with as few
inventory bits as it wants, but it must reply with at
least one inventory bit.  If the receiver does not do so,
the sender should terminate the connection to the receiver and refrain from
contacting it for a time.

While synchronizing the receiver's block inventory, the sender will fetch blocks and microblocks
on the data-plane once it knows that the receiver has them.
To do so, the sender and receiver do the following:

1.  The sender looks up the `data_url` from the receiver's `HandshakeAccept` message
    and issues a HTTP GET request for each anchored block marked as present in
the inventory.
2.  The receiver replies to each HTTP GET request with the anchored blocks.
3.  Once the sender has received a parent and child anchor block, it will ask
    for the microblock stream confirmed by the _child_ by asking for the
microblocks that the child confirms.  It uses the _index hash_ of the child
anchored block to do so, which itself authenticates the last hash in the
confirmed microblock stream.
4.  The receiver replies to each HTTP GET request with the confirmed microblock
    streams.
5.  As blocks and microblock streams arrive, the sender processes them to build
    up its view of the chain.

When the sender receives a block or microblock stream, it validates them against
the burn chain state.  It ensures that the block hashes to a block-commit
message that won sortition (see SIP-001 and SIP-007), and it ensures that a confirmed
microblock stream connects a known parent and child anchored block.  This means
that the sender **does not need to trust the receiver** to validate block data
-- it can receive block data from any HTTP endpoint on the web.

The receiver should reply blocks and confirmed microblock streams if it had
previously announced their availability in a `BlocksInv` message.
If the sender receives no data (i.e. a HTTP 404)
for blocks the receiver claimed to have, or if the sender receives invalid data or 
an incomplete microblock stream, then the sender disconnects from the receiver
and blacklists it on the control-plane.

The sender may not be contracting a peer node when it fetches blocks and
microblocks -- the receiver may send the URL to a Gaia hub in its
`HandshakeAcceptData`'s `data_url` field.  In doing so, the receiver can direct
the sender to fetch blocks and microblocks from a well-provisioned,
always-online network endpoint that is more reliable than the receiver node.

Blocks and microblocks are downloaded incrementally by reward cycle.
As the sender requests and receives blocks and microblocks for reward cycle _i_,
it learns the anchor block for reward cycle _i+1_ (if it exists at all), and
will only then be able to determine the true sequence of consensus hashes for
reward cycle _i+1_.  As nodes do this, their PoX knowledge my change -- i.e.
they will become certain of the presences of PoX anchor blocks that they had
previously been uncertain of.  As such, nodes periodically re-download each
other's PoX inventory vectors, and if they have changed -- i.e. the _ith_ bit flipped
from a `0` to a `1` -- the block and microblock inventory state representing blocks and
microblocks in or after reward cycle _i_ will be dropped and re-downloaded.

### Announcing New Data

In addition to synchronizing inventories, peers announce to one another
when a new block or confirmed microblock stream is available.  If peer A has
crawled peer B's inventories, and peer A downloads or is forwarded a block or
confirmed microblock stream that peer B does not have, then peer A will send a
`BlocksAvailable` (or `MicroblocksAvailable`) message to peer B to inform it
that it can fetch the data from peer A's data plane.  When peer B receives one
of these messages, it updates its copy of peer A's inventory and proceeds to
fetch the blocks and microblocks from peer A.  If peer A serves invalid data, or
returns a HTTP 404, then peer B disconnects from peer A (since this indicates
that peer A is misbehaving).

Peers do not forward blocks or confirmed microblocks to one another.  Instead,
they only announce that they are available.  This minimizes the aggregate
network bandwidth required to propagate a block -- a block is only downloaded
by the peers that need it.

Unconfirmed microblocks and transactions are always forwarded to other peers in order to
ensure that the whole peer network quickly has a full copy.  This helps maximize
the number of transactions that can be included in a leader's microblock stream.

### Choosing Neighbors

The core design principle of the Stacks peer network control-plane is to maximize the entropy
of the peer graph.  Doing so helps ensure that the network's connectivity
avoids depending too much on a small number of popular peers and network edges.
While this may slow down message propagation relative to more structured peer graphs,
the _lack of_ structure is the key to making the Stacks peer network
resilient.

This principle is realized through a randomized neighbor selection algorithm.
This algorithm curates the peer's outbound connections to other peers; inbound
connections are handled separately.

The neighbor selection algorithm is designed to be able to address the following
concerns:

* It helps a peer discover possible "choke points" in the network, and devise
  alternative paths around them.
* It helps a peer detect network disruptions (in particular, BGP prefix hijacks) --
observed as a sets of peers with the same network prefix suddenly not relaying
messages, or sets of paths through particular IP blocks no longer being taken.
* It helps a peer discover the "jurisdictional path" its messages could travel through, 
which helps a peer route around hostile networks that would delay, block, or
track the messages.

To achieve this, the Stacks peer network control-plane is structured as a K-regular random graph,
where _any_ peer may be chosen as a peer's neighbor.  The network forms
a _reachability_ network, with the intention of being "maximally difficult" for a
network adversary to disrupt by way of censoring individual nodes and network
hops.  A random graph topology is suitable for this,
since the possibility that any peer may be a neighbor means that the only way to
cut off a peer from the network is to ensure it never discovers another honest
peer.

To choose their neighbors in the peer graph, peers maintain two views of the network:

* The **frontier** -- the set of peers that have either sent a message to this
  peer or have responded to a request at some point in the past.
The size of the frontier is significantly larger than
K.  Peer records in the frontier set may expire and may be stale, but are only
evicted when the space is needed.  The **fresh frontier set** is the subset of
the frontier set that have been successfully contacted in the past _L_ seconds.

* The **neighbor set** -- the set of K peers that the peer will announce as its
  neighbors when asked.  The neighbor set is a randomized subset of the frontier.
Unlike the frontier set, the peer continuously refreshes knowledge of the state
of the neighbor sets' blocks and transactions in order to form a transaction and
block relay network.

Using these views of the network, the peers execute a link-state routing
protocol whereby each peer determines each of its neighbors' neighbors,
and in doing so, builds up a partial view of the routing graph made up of
recently-visited nodes.  Peers execute a route recording protocol whereby each 
message is structured to record the _path_ it took
through the graph's nodes.  This enables a peer to determine how often other peers
in its frontier, as well as the network links between them, are responsible for
relaying messages.  This knowledge, in turn, is used to help the peer seek out
new neighbors and neighbor links to avoid depending on popular peers and 
links too heavily.

**Discovering Other Peers**

To construct a K-regular random graph topology, peers execute a modified Metropolis-Hastings
random graph walk with delayed acceptance (MHRWDA) [1] to decide which peers belong to
their neighbor set and to grow their frontiers.

A peer keeps track of which peers are neighbors of which other peers, and in
doing so, is able to calculate the degree of each peer as the number of that
peer's neighbors that report the peer in question as a neighbor.  Given a currently-visited
peer _P_, a neighboring peer _N_ is walked to with probability proportional to
the ratio of their degrees.  The exact formula is adapted from Algorithm 2 in
[1].

Once established, a peer tries to keep its neighbor set stable as long as the
neighbors are live.  It does so by periodically pinging and re-handshaking with
its K neighbors in order to establish a minimum time between contacts.
As it communicates with neighbors, it will measure the health of each neighbor by measuring how often
it responds to a query.  A peer will probabilistically evict a peer from its
neighbor set if its response rate drops too low, where the probability of
eviction is proportional both to the peer's perceived uptime and to the peer's
recent downtime.

**Curating a Frontier**

In addition to finding neighbors, a peer curates a frontier set to (1) maintain knowledge
of backup peers to contact in case a significant portion of their neighbors goes
offline, and (2) to make inferences about the global connectivity of the peer
graph.  A peer can't crawl each and every other peer in the
frontier set (this would be too expensive), but a peer can infer over time which
nodes and edges are likely to be online by examining its fresh frontier set.

The frontier set grows whenever new neighbors are discovered, but it is not
infinitely large.  Frontier nodes are stored in a bound-sized hash table on disk.  A neighbor
inserted deterministically into the frontier set by hashing its address with a
peer-specific secret and the values `0` through `7` in order to identify eight
slots into which its address can be inserted.  If any of the resulting slots are
empty, the peer is added to the frontier.

As more peers are discovered, it becomes possible that a newly-discovered peer cannot be inserted
determinstically.  This will become more likely than not to happen once the
frontier set has `8 * sqrt(F)` slots full, where `F` is the maximum size of
the frontier (due to the birthday paradox).  In such cases, a random existing peer in one of the slots is
chosen for possible eviction, but only if it is offline.  The peer will attempt
to handshake with the existing peer before evicting it, and if it responds with
a `HandshakeAccept`, the new node is discarded and no eviction takes place.

Insertion and deletion are deterministic (and in deletion's case, predicated on
a failure to ping) in order to prevent malicious remote peers from filling up
the frontier set with junk without first acquiring the requisite IP addresses
and learning the victim's peer-specific secret nonce.
The handshake-then-evict test is in place also to
prevent peers with a longer uptime from being easily replaced by short-lived peers.

**Mapping the Peer Network**

The Stacks protocol includes a route recording mechanism for peers to probe network paths.
This is used to measure how frequently peers and connections are used in the peer
graph.  This information is encoded in the `relayers` vector in each message.

When relaying data, the relaying peer must re-sign the message preamble and update its
sequence number to match each recipient peer's expectations on what the signature 
and message sequence will be.  In addition, the relaying peer appends the
upstream peer's address and previous sequence number in the
message's `relayers` vector.  Because the `relayers` vector grows each time a
message is forwarded, the peer uses it to determine the message's time-to-live:
if the `relayers` vector becomes too long, the message is dropped.

A peer that relays messages _must_ include itself at the end of the
`relayers` vector when it forwards a message.
If it does not do so, a correct downstream peer can detect this by checking that
the upstream peer inserted its previously-announced address (i.e. the IP
address, port, and public key it sent in its `HandshakeData`).  If a relaying
peer does not update the `relayers` vector correctly, a downstream peer should
close the connection and possibly throttle the peer (blacklisting should not
be used since this can happen for benign reasons -- for example, a node on a
laptop may change IP addresses between a suspend/resume cycle).  Nevertheless,
it is important that the `relayers` vector remains complete in order to detect and resist routing
disruptions in the Internet.

Not all peers are relaying peers -- only peers that set the `SERVICE_RELAY`
bit in their handshakes are required to relay messages and list themselves in the `relayers` vector.
Peers that do not do this may nevertheless _originate_ an unsolicited `BlocksData`,
`MicroblocksData`, or `Transaction` message.  However, its `relayers` vector _must_ be
empty.  This option is available to protect the privacy of the originating peer, since
(1) network attackers seeking to disrupt the chain could do
so by attacking block and microblock originators, and
(2) network attackers seeking to go after Stacks users could do so if they knew
or controlled the IP address of the victim's peer.  The fact that network
adversaries can be expected to harass originators who advertise their network
addresses serves to discourage relaying peers from stripping the
`relayers` vector from messsages, lest they become the target of an attack.

A peer may transition between being a relaying peer and a non-relaying peer by
closing a connection and re-establishing it with a new handshake.  A peer that
violates the protocol by advertising their `SERVICE_RELAY` bit and not
updating the `relayers` vector should be blacklisted by downstream
peers.

A peer must not forward messages with invalid `relayers` vectors.  In
particular, if a peer detects that its address (specicifically, it's public key
hash) is present in the `relayers` vector, or if the vector contains a cycle,
then the message _must_ be dropped.  In addition, a peer that receives a message
from an upstream peer without the `SERVICE_RELAY` bit set that includes a
`relayers` vector _must_ drop the message.

**Promoting Route Diversity**

The peer network employs two heuristics to help prevent choke points from
arising:

* Considering the AS-degree:  the graph walk algorithm will consider a peer's
connectivity to different _autonomous systems_
(ASs) when considering adding it to the neighbor set.

* Sending data in rarest-AS-first order:  the relay algorithm will probabilistically
  rank its neighbors in order by how rare their AS is in the fresh frontier set.

When building up its K neighbors, a peer has the opportunity to select neighbors
based on how popular their ASs are.  To do this, the peer crawl N > K neighbors, and then
randomly disconnect from N - K of them.  The probability that a peer will
be removed is proportional to (1) how popular its AS
is in the N neighbors, and (2) how unhealthy it is out of the neighbors in the
same AS.  The peer will first select an AS to prune, and then select a neighbor
within that AS.  This helps ensure that a relayed messasge is likely to be
forwarded to many different ASs quickly.

To forward messages to as many different ASs as possible, the peer will
probabilistically prioritize neighbors to receive a forwarded message based on how _rare_
their AS is in the fresh frontier set.  This forwarding heuristic is
meant to ensure that a message quickly reaches many different networks in the
Internet.

The rarest-AS-first heuristic is implemented as follows:

1. The peer builds a table `N[AS]` that maps its fresh frontier set's ASs to the list of peers
   contained within.  `len(N[AS])` is the number of fresh frontier peers in `AS`, and 
   `sum(len(N[AS]))` for all `AS` is `K`.
2. The peer assigns each neighbor a probability of being selected to receive the
   message next.  The probability depends on `len(N[AS])`, where `AS` is the
   autonomous system ID the peer resides in.  The probability that a peer is
   selected to receive the message is proportional to `1 - (len(N[AS]) + 1) / K`.
3.  The peer selects a neighbor according to the distribution, forwards the message to it, and
    removes the neighbor from consideration for this message.  The peer repeats step 2 until all neighbors have
    been sent the message.

A full empirical evaluation on the effectiveness of these heuristics at encouraging
route diversity will be carried out before this SIP is accepted.

**Proposal for Miner-Assisted Peer Discovery**

Stacks miners are already incentivized to maintain good connectivity with one
another and with the peer network in order to ensure that they work on the
canonical fork.  As such, a correct miner may, in the future, help the
control-plane network remain connected by broadcasting the root of a Merkle tree of a set
of "reputable" peers that are known by the miner to be well-connected, e.g. by
writing it to its block's coinbase payload.  Other
peers in the peer network would include these reputable nodes in their frontiers
by default.

A peer ultimately makes its own decisions on who its neighbors are, but by 
default, a peer selects a miner-recommended peer only if over 75% of the mining power recommends
the peer for a long-ish interval (on the order of weeks).  The 75% threshold
follows from selfish mining -- the Stacks blockchain prevents selfish mining as
long as at least 75% of the hash power is honest.  If over 75% of the mining
power recommends a peer, then the peer has been recommended through an honest
process and may be presumed "safe" to include in the frontier set.

A recommended peer would not be evicted from the frontier set unless it could
not be contacted, or unless overridden by a local configuration option.

### Forwarding Data

The Stacks peer network propagates blocks, microblocks, and
transactions by flooding them.  In particular, a peer can send other peers
an unsolicited `BlocksAvailable`, `MicroblocksAvailable`, `BlocksData`, `MicroblocksData`,
and `Transaction` message.

If the message has not been seen before by the peer, and the data is valid, then the peer
forwards it to a subset of its neighbors (excluding the one that sent the data). 
If it has seen the data before, it does not forward
it.  The process for determining whether or not a block or transaction is valid
is discussed in SIP 005.  However, at a high level, the following
policies hold:

* A `StacksBlock` can only be valid if it corresponds to block commit
  transaction on the burn chain that won sortition.  A peer may cache a
`StacksBlock` if it determines that it has not yet processed the sortition that
makes it valid.  A `StacksBlock` is never forwarded by the recipient;
instead, the recipient peer sends a `BlocksAvailable` message to its neighbors.
* A `StacksMicroblock` can only be valid if it corresponds to a valid
  `StacksBlock` or a previously-accepted `StacksMicroblock`.  A peer may cache a
`StacksMicroblock` if it determines that a yet-to-arrive `StacksBlock` or
`StacksMicroblock` could make it valid in the near-term, but if the
`StacksMicroblock`'s parent `StacksBlock` is unknown, the
`StacksMicroblock` will _not_ be forwarded.
* A `Transaction` can only be valid if it encodes a legal state transition on
  top of the peer's currently-known canonical Stacks blockchain tip. 
A peer will _neither_ cache _nor_ relay a `Transaction` message if it cannot 
determine that it is valid.

#### Client Peers

Messages can be forwarded to both outbound connections to other neighbors and to inbound
connections from clients -- i.e. remote peers that have this peer as a next-hop
neighbor.  Per the above text, outbound neighbors are selected as the
next message recipients based on how rare their AS is in the frontier.

Inbound peers are handled separately.  In particular, a peer does not crawl
remote inbound connections, nor does it synchronize their peers' block inventories.
Inbound peers tend to be un-routable peers, such as those running behind NATs on
private, home networks.  However, such peers can still send
unsolicited blocks, microblocks, and transactions to publicly-routable
peers, and those publicly-routable peers will need to forward them to both its
outbound neighbors as well as its own inbound peers.  To do the latter, 
a peer will selectively forward data to its inbound peers in a way that is
expected to minimize the number of _duplicate_ messages the other peers will
receive.

To do this, each peer uses the `relayers` vector in each message
it receives from an inbound peer to keep track of which peers have forwarded 
the same messages.  It will then choose inbound peers to receive a forwarded message
based on how _infrequently_ the inbound recipient has sent duplicate messages.

The intuition is that if an inbound peer forwards many messages 
that this peer has already seen, then it is likely that the inbound per is also 
connected to a (unknown) peer that is already able to forward it data.
That is, if peer B has an inbound connectino to peer A, and
peer A observes that peer B sends it messeges that it has already seen recently,
then peer A can infer that there exists an unknown peer C that is forwarding
messages to peer B before peer A can do so.  Therefore, when selecting inbound
peers to receive a message, peer A can de-prioritize peer B based on the
expectation that peer B will be serviced by unknown peer C.

To make these deductions, each peer maintains a short-lived (i.e. 10 minutes)
set of recently-seen message digests, as well as the list of which peers have sent 
each message.  Then, when selecting inbound peers to receive a message, the peer
calculates for each inbound peer a "duplicate rank" equal to the number of times
it sent an already-seen message.  The peer then samples the inbound peers
proportional to `1 - duplicate_rank / num_messages_seen`.

It is predicted that there will be more NAT'ed peers than public peers.
Therefore, when forwarding a message, a peer will select more inbound peers
(e.g. 16) than outbound peers (e.g. 8) when forwarding a new message.

# Related Work

This section will be expanded upon after this SIP is ratified.

[1] See https://arxiv.org/abs/1204.4140 for details on the MHRWDA algorithm.
[2] https://stuff.mit.edu/people/medard/rls.pdf

# Backwards Compatibility

Not applicable

# Activation

At least 20 miners must register a name in the `.miner` namespace in Stacks 1.0.
Once the 20th miner has registered, the state of Stacks 1.0 will be snapshotted.
300 Bitcoin blocks later, the Stacks 2.0 blockchain will launch.  With Stacks
2.0 will come the Clarity VM.

# Reference Implementations

Implemented in Rust.  See https://github.com/blockstack/stacks-blockchain.

The neighbor set size K is set to 16.  The frontier set size
is set to hold 2^24 peers (with evictions becoming likely after insertions once
it has 32768 entries).


          
# Preamble

SIP Number: 020

Title: Bitwise Operations in Clarity

Authors: Cyle Witruk <https://github.com/cylewitruk>, Brice Dobry
<https://github.com/obycode>

Consideration: Technical, Governance

Type: Consensus

Status: Ratified

Created: 12 November 2022

License: CC0-1.0

Sign-off: Jason Schrader <jason@joinfreehold.com>, Jesse Wiley <jesse@stacks.org>, Jude Nelson <jude@stacks.org>

Layer: Consensus (hard fork)

Discussions-To: https://github.com/stacksgov/sips

# Abstract

This SIP adds bitwise operations to the Clarity language which could simplify
the implementation of smart contracts that require manipulation of bits. The
changes include the addition of the following operations:

- Bitwise Xor (`bit-xor`)
- Bitwise And (`bit-and`)
- Bitwise Or (`bit-or`)
- Bitwise Not (`bit-not`)
- Binary Left Shift (`bit-shift-left`)
- Binary Right Shift (`bit-shift-right`)

# License and Copyright

This SIP is made available under the terms of the Creative Commons CC0 1.0
Universal license, available at
https://creativecommons.org/publicdomain/zero/1.0/ This SIP's copyright is held
by the Stacks Open Internet Foundation.

# Introduction

Bitwise operations are common in other programming languages. Common algorithms,
including many used in encryption, or the ability to set and check flags in a
bit field for example, would be much more difficult to implement without the use
of these operations. When executing a contract using these operations, the
common hardware on which miners and nodes are likely to be running can all
perform these operations very efficiently -- these are typically single cycle
operations. Note that the addition of these bitwise operations commits the
Clarity VM to using 2's complement to represent integers.

# Specification

## Bitwise Xor (`bit-xor`)

`(bit-xor i1 i2...)`

- **Inputs:** int, ... | uint, ...
- **Output:** int | uint

Returns the result of bitwise exclusive or'ing a variable number of integer
inputs.

| Bit in i1 | Bit in i2 | Bit in Output |
| --------- | --------- | ------------- |
| 0         | 0         | 0             |
| 0         | 1         | 1             |
| 1         | 0         | 1             |
| 1         | 1         | 0             |

The following example uses only 8 bits to make it easier to follow. Actual
Clarity values are 128 bits.

```
(bit-xor 17 30) ;; Return 15
;; Binary representation:
;; i1 (17):     00010001
;; i2 (30):     00011110
;; Output (15): 00001111
```

### Examples

```
(bit-xor 1 2) ;; Returns 3
(bit-xor 120 280) ;; Returns 352
(bit-xor -128 64) ;; Returns -64
(bit-xor u24 u4) ;; Returns u28
(bit-xor 1 2 4 -1) ;; Returns -8
```

## Bitwise And (`bit-and`)

`(bit-and i1 i2...)`

- **Inputs:** int, ... | uint, ...
- **Output:** int | uint

Returns the result of bitwise and'ing a variable number of integer inputs.

| Bit in i1 | Bit in i2 | Bit in Output |
| --------- | --------- | ------------- |
| 0         | 0         | 0             |
| 0         | 1         | 0             |
| 1         | 0         | 0             |
| 1         | 1         | 1             |

The following example uses only 8 bits to make it easier to follow. Actual
Clarity values are 128 bits.

```
(bit-and 17 30) ;; Return 16
;; Binary representation:
;; i1 (17):     00010001
;; i2 (30):     00011110
;; Output (16): 00010000
```

### Examples

```
(bit-and 24 16) ;; Returns 16
(bit-and 28 24 -1) ;; Returns 24
(bit-and u24 u16) ;; Returns u16
(bit-and -128 -64) ;; Returns -128
(bit-and 28 24 -1) ;; Returns 24
```

## Bitwise Or (`bit-or`)

`(bit-or i1 i2...)`

- **Inputs:** int, ... | uint, ...
- **Outputs:** int | uint

Returns the result of bitwise inclusive or'ing a variable number of integer
inputs.

| Bit in i1 | Bit in i2 | Bit in Output |
| --------- | --------- | ------------- |
| 0         | 0         | 0             |
| 0         | 1         | 1             |
| 1         | 0         | 1             |
| 1         | 1         | 1             |

The following example uses only 8 bits to make it easier to follow. Actual
Clarity values are 128 bits.

```
(bit-or 17 30) ;; Return 31
;; Binary representation:
;; i1 (17):     00010001
;; i2 (30):     00011110
;; Output (31): 00011111
```

### Examples

```
(bit-or 4 8) ;; Returns 12
(bit-or 1 2 4) ;; Returns 7
(bit-or 64 -32 -16) ;; Returns -16
(bit-or u2 u4 u32) ;; Returns u38
```

## Bitwise Not (`bit-not`)

`(bit-not i1)`

- **Inputs:** int | uint
- **Output:** int | uint

Returns the one's compliment (sometimes also called the bitwise compliment or
not operator) of `i1`, effectively reversing the bits in `i1`.

In other words, every bit that is `1` in `ì1` will be `0` in the result.
Conversely, every bit that is `0` in `i1` will be `1` in the result.

| Bit in i1 | Bit in Output |
| --------- | ------------- |
| 0         | 1             |
| 1         | 0             |

The following example uses only 8 bits to make it easier to follow. Actual
Clarity values are 128 bits.

```
(bit-not u41) ;; Return u214
;; Binary representation:
;; i1 (41):      00101001
;; Output (214): 11010110
```

### Examples

```
(bit-not 3) ;; Returns -4
(bit-not u128) ;; Returns u340282366920938463463374607431768211327
(bit-not 128) ;; Returns -129
(bit-not -128) ;; Returns 127
```

## Bitwise Left Shift (`bit-shift-left`)

`(bit-shift-left i1 shamt)`

- **Inputs:** int, uint | uint, uint
- **Outputs:** int | uint

Shifts all bits in `i1` to the left by the number of places specified in `shamt`
modulo 128 (the bit width of Clarity integers). New bits are filled with zeros.

Note that there is a deliberate choice made to ignore arithmetic overflow for
this operation. In use cases where overflow should be detected, developers
should use `*`, `/`, and `pow` instead of the shift operators.

The following example uses only 8 bits to make it easier to follow. Actual
Clarity values are 128 bits.

```
(bit-shift-left 6 u3) ;; Return 48
;; Binary representation:
;; Input  (6):  00000110
;; Output (48): 00110000
```

### Examples

```
(bit-shift-left 16 u2) ;; Returns 64
(bit-shift-left -64 u1) ;; Returns -128
(bit-shift-left u4 u2) ;; Returns u16
(bit-shift-left 123 u9999999999) ;; Returns -170141183460469231731687303715884105728 (== 123 bit-shift-left 127)
(bit-shift-left u123 u9999999999) ;; Returns u170141183460469231731687303715884105728 (== u123 bit-shift-left 127)
(bit-shift-left -1 u7) ;; Returns -128
(bit-shift-left -1 u128) ;; Returns -1
```

## Bitwise Right Shift (`bit-shift-right`)

`(bit-shift-right i1 shamt)`

- **Inputs:** int, uint | uint, uint
- **Output:** int | uint

Shifts all the bits in `i1` to the right by the number of places specified in
`shamt` modulo 128 (the bit width of Clarity integers). When `i1` is a `uint`
(unsigned), new bits are filled with zeros. When `i1` is an `int` (signed), the
sign is preserved, meaning that new bits are filled with the value of the
previous sign-bit.

Note that there is a deliberate choice made to ignore arithmetic overflow for
this operation. In use cases where overflow should be detected, developers
should use `*`, `/`, and `pow` instead of the shift operators.

The following example uses only 8 bits to make it easier to follow. Actual
Clarity values are 128 bits.

```
(bit-shift-right u170 u1) ;; Return u85
;; Binary representation:
;; Input  (u170): 10101010
;; Output (u85):  01010101
```

### Examples

```
(bit-shift-right 2 u1) ;; Returns 1
(bit-shift-right 128 u2) ;; Returns 32
(bit-shift-right -64 u1) ;; Returns -32
(bit-shift-right u128 u2) ;; Returns u32
(bit-shift-right 123 u9999999999) ;; Returns 0
(bit-shift-right u123 u9999999999) ;; Returns u0
(bit-shift-right -128 u7) ;; Returns -1
(bit-shift-right -256 u1) ;; Returns -128
(bit-shift-right 5 u2) ;; Returns 1
(bit-shift-right -5 u2) ;; Returns -2
```

# Related work

Not applicable

# Backwards Compatibility

Because this SIP introduces new Clarity operators, it is a consensus-breaking
change. A contract that uses one of these new operators would be invalid before
this SIP is activated, and valid after it is activated.

# Activation

This SIP will be a rider on SIP-015. It will be considered activated if and only
if SIP-015 (and Stacks 2.1) is activated.

# Reference Implementations

- https://github.com/stacks-network/stacks-blockchain/pull/3389
- See also discussions in
  https://github.com/stacks-network/stacks-blockchain/pull/3382

          
# Preamble

SIP Number: 015

Title: Stacks Upgrade of Proof-of-Transfer and Clarity

Authors:
    Aaron Blankstein <aaron@hiro.so>,
    Mike Cohen <mjoecohen@gmail.com>,
    Greg Coppola <greg@hiro.so>,
    Brice Dobry <brice@hiro.so>,
    Hero Gamer <herogamerthesht572@gmail.com>
    Matthew Little <matthew@blockstack.com>,
    Jenny Mith <jenny@stacks.org>,
    Jude Nelson <jude@stacks.org>,
    Pavitthra Pandurangan <pavitthra@hiro.so>,
    Jason Schrader <jason@joinfreehold.com>,
    Rena Shah <rena@trustmachines.co>,
    Hank Stoever <hank@mechanism.so>,
    Igor Sylvester <igor@trustmachines.co>,
    Jesse Wiley <jw@stacks.org>,

Consideration: Technical, Governance, Economics

Type: Consensus

Status: Ratified

Created: 1 December 2021

License: BSD 2-Clause

Sign-off: Brice Dobry <brice@hiro.so>, Jason Schrader <jason@joinfreehold.com>,
          MattyTokenomics, Jude Nelson <jude@stacks.org>

Discussions-To: https://github.com/stacksgov/sips

# Abstract

This SIP proposes a set of updates to three major areas of the Stacks blockchain:

* **Stacking**, the Proof-of-Transfer (PoX) consensus algorithm as implemented
in the Stacks chain and originally proposed in [SIP-007](./sip-007-stacking-consensus.md). 
The proposed changes improve the user and developer experience of participating
in Stacking, and add support for new behaviors and PoX reward address types.

* **Clarity**, the smart contract language supported on the Stacks blockchain.
The proposed changes fix bugs and add new native functions and global variables
that improve its support for interacting with off-chain services and
blockchains, especially Bitcoin.

* **Block Validation**, the procedure by which blocks are determined to be acceptable to the Stacks
  blockchain.  The proposed changes address bugs in the implementation that are
not specified by any prior SIP, but which cannot be changed without a
coordinated network-wide upgrade.  In addition, the proposed changes address
potential consensus challenges for PoX-related chain reorganizations which were
not known at the time SIP-007 was written.  Finally, new variations of existing
transactions are proposed to better support Stacking, to support
multiple Clarity versions, and to support decentralized mining pools.

In addition to proposing these changes, this SIP also outlines an approach for
implementing these changes in the Stacks blockchain.

Because this is a breaking change, there must be a vote from the relevant
stakeholders to activate this SIP.  This vote is slated to take place
during reward cycles 46 and 47.  This
window is estimated to begin **starting November 10, 2022** and **ending
December 8, 2022**.

# Introduction

This SIP condenses lessons learned from running the Stacks blockchain in
production to date.  Since the system was launched in January 2021, several
shortcomings have been discovered which can only be addressed through a
coordinated, backwards-compatible network upgrade.  This SIP enumerates these
shortcomings and proposes solutions.

Broadly speaking, the shortcomings of the system fall into one of three
categories: Stacking, the Clarity language, and block validation.

Concerning Stacking, the current PoX implementation in the Stacks blockchain has several
shortcomings for developers and users:

1. After unlocking, Stackers **must** wait one cycle before Stacking
   again. This is an implementation consequence, not a requirement of
   the algorithm itself.

2. If users fail to qualify during a cycle, their funds remain locked
   for the cycle regardless.

3. PoX cannot support advanced behaviors that could be used for
   different kinds of smart contracts: changing the PoX reward
   address, increasing or decreasing lock amounts, etc. These features
   would enable new kinds of applications that build on top of PoX.

4. PoX will automatically sunset after a pre-defined number of Bitcoin blocks
   pass, regardless of how popular or useful it is.

This SIP proposes the creation of a new `pox-2` smart contract to implement
Stacking which addresses the above problems.  A migration procedure is presented
for transitioning the system from `pox` to `pox-2`.

At the same time, it was discovered over the lifetime of the Stacks blockchain
that there are a few shortcomings in the block validation logic that
negatively impact the user experience and miner experience.  In particular:

1. Stacking as proposed in SIP-007 has a design flaw: If it
is ever the case that a PoX anchor block is missing, and yet somehow
manages to achieve 80% or more confirmations during the prepare phase,
then the subsequent arrival of that anchor block will cause a deep
chain reorganization. It does not matter how many future blocks get
mined--- if the anchor block is later revealed, it will invalidate all
of the blocks that did not build on it. While mining and confirming a
hidden anchor block is very costly, it is possible.

2. An on-burnchain transaction mined in Bitcoin block `N` will only be mined 
in the Stacks blockchain if Bitcoin block `N+1` selects a Stacks block.
Even then, the transaction only materializes in Stacks forks which include this
Stacks block.  This is brittle in the face of both orphan Stacks blocks (which
will never be canonical) and "flash blocks" -- quickly-mined
Bitcoin blocks that contain no block-commits.

3. The Stacks blockchain calculates the probability of a miner winning a block race as
   proportional to the minimum of their last block-commit's Bitcoin spend, and
the median of their last six block-commits' spends.  The last six block-commits
today only include block-commits that arrived in their intended Bitcoin blocks.
This unfairly punishes miners whose block-commits, through no fault of the
miner's, are late to be mined on the Bitcoin chain.

4. Today, there are multiple ways for a Stacks miner to process a transaction
   but be forced to drop it from the block they are building because its
inclusion would invalidate the block.  While the current implementation employs
various measures to mitigate the impact of this shortcoming, a "proper" solution
would be to make it so that this simply never happens -- miners should get paid
for _all_ of the work they do, _even if_ the transaction is invalid.

This SIP proposes fixing these four problems.  Regarding the first, the SIP proposes organizing 
PoX anchor blocks into a forkable history, whose canonical fork is determined
via Nakamoto consensus.  The canonical Stacks fork must contain all canonical
PoX anchor blocks.  Regarding the second, an on-burnchain transaction
will be considered in each Stacks block to be mined in the subsequent six
burnchain blocks, instead of the next.  Regarding the third, the median Bitcoin spend of the
miner's last six block-commits will include _late_ block-commits.  Regarding the
fourth, the SIP proposes changing a few rules for transaction-processing that
would permit the miner to include transactions in blocks that it cannot include today.

Finally, a year of active development of smart contracts on the Stacks
blockchain highlighted areas for improved support in the Clarity smart
contracting language. These areas include increasing the visibility of
burnchain operations (on the Stacks chain, these are *Bitcoin
operations*) and general improvements to the Clarity programming
language.

## How to Read this SIP

This SIP specification is structured like a changelog.  Proposed changes to each of these
areas of the system are described in order of new features, changed features,
and fixed features.  A rationale is provided for each proposed modification to
Stacks.

# Specification

Applying these upgrades to the Stacks blockchain requires a
consensus-breaking network upgrade, in this case, a hard fork. Like
other such changes, this will require a new Stacks epoch. In this SIP,
we will refer to this new epoch as Stacks 2.1.

At the onset of Stacks 2.1, the Clarity VM will begin to support
"Clarity Version 2". This version will include support for the new
native methods proposed in this SIP (and therefore include new
*keywords* which cannot be used for method names or variable names).
New contracts launched in Stacks 2.1 will _default_ to Clarity 2, but
contract authors will be able to use a new contract-publish transaction type to
indicate if a contract should specifically launch using
Clarity 1 or Clarity 2.

Additionally, a new `pox-2` contract will be
published by the boot address. The `stacks-node` will use the new
`pox-2` contract for determining PoX reward sets and governing PoX
locks.  Similarly, new rules for block validation take effect at the onset of Stacks
2.1.

## Stacking

This section of the SIP specifies the new and changed behaviors of the
PoX-2 contract and also provides new use cases that demonstrate how
the new PoX-2 contract could be used.

### Overview

The new PoX contract operates exclusively with PoX state that was
created in the new contract. It will not "import" state from the
original PoX contract. In order to allow for this, a particular reward
cycle `N` is chosen as the "Last PoX-1" reward cycle, and then the
"First PoX-2" reward cycle is the subsequent cycle (`N+1`).  In-between these
two cycles, there exists a burnchain block height `v1_unlock_height` at which
all of the STX tokens in PoX-1 will unlock, so that they can be locked up again
in PoX-2 before cycle `N+1` starts.

This defines three periods of PoX operation:

Period 1 | Period 2a | Period 2b | Period 3
-- | -- | -- | --
2.0 Consensus Rules in Effect | 2.1 Consensus Rules enacted, PoX-2 exists, but PoX-1 is still active | PoX-1 deactivates, all tokens locked in the original PoX contract unlock, but first PoX-2 reward cycle has not begun | First PoX-2 reward cycle has begun
_This is the period of time before the 2.1 fork._  |  _This is the period of time after the 2.1 fork, and before `v1_unlock_height` (inclusive)_ | _This is the period of time between `v1_unlock_height + 1` and the start of cycle (N+1)_ |  _This is the start of cycle (N+1), and all cycles afterward.  The original PoX contract state will no longer have any impact on reward sets, account lock status, etc._

- Every account that is locked by PoX-1 for cycle `N`
  and beyond is unlocked at the end of period 2a, once the burnchain block
height `v1_unlock_height` passes.
- Accounts locked in PoX-2 will remain locked.  It will be possible to do this
  with unlocked STX at the start of period 2a.
- Every account that is locked in PoX-2 is eligible
  for the new calls to PoX-2 described in this document, including those that
extend the lock period and those that increase the amount locked.
- Calls to PoX-2 which would attempt to create state for a cycle
  _before_ `(N+1)` will fail.  Accounts cannot "back-date" their PoX-2 lockups.
- Calls to the PoX-1 contract which would attempt to create
  state for a cycle `>= N+1` will be made to fail, and any state after
  `N+1` is ignored.  This requires interposing on contract-calls
  during period 2 and checking the reward cycles arguments: the
  relevant functions are `stack-stx`, `delegate-stack-stx`,
  `stack-aggregation-commit`, and `reject-pox`.

PoX-2 is available at the start of period 2a.  Accounts can begin to lock up
their liquid STX into PoX-2 at the start of this period, even though PoX-1 is
still active.

Once period 2a finishes, PoX-1 is defunct.  It will only be possible to call its
read-only functions.  Calling any functions that create or modify state will
fail.  All tokens locked in PoX-1 (but not PoX-2) will unlock at the end of
period 2a.

PoX reward payouts continue to happen for cycle `N`, even though PoX-1 will be
disabled between cycle `N` and `N+1`.  This is because the reward set is chosen
at the _start_ of the reward cycle, and used to schedule miner payouts over the
course of the cycle.  Cycle `N` is the last reward cycle in which PoX-1 will be
queried to calculate the reward set.  Starting in cycle `N+1`, PoX-2 will be
queried for the reward set.

The value for `v1_unlock_height` will be determined before this SIP activates.
It will fall as close to the start of cycle `N` as possible in order to give
users the maximum amount of time to re-lock their now-unlocked STX into PoX-2,
if they so desire.

### New method: `stack-extend`

This method allows direct stackers to re-lock their funds for up to an additional
12 cycles *before* their current lock has expired. The target use case for
this is to allow users to repeatedly stack without a cooldown phase.

This method checks that the stacker is still allowed to stack, that
the stacker has not delegated to an operator, and that their funds are
currently stacked. The caller may supply a new reward address for the
extension.

The special case handler for the PoX contract in the Clarity VM will
check this method's return value and set the stacker's STX account to
"auto-unlock" at the end of the last extended-to reward cycle.

### New method: `delegate-stack-extend`

This method allows operators to re-lock the funds of one of their
delegation participants for additional cycles *before* that participants
lock has expired. The target use case for this is to allow delegation
operators to repeatedly stack on behalf of their users without cooldowns.

This method checks that the delegator is still authorized on behalf of
the given stacker (and remains authorized until the end of the
locked-for period) and that the funds are currently stacked.

Note that, just as with the existing `delegate-stack-stx` function,
this method locks *but does not commit* the user's STX. The delegation
operator must invoke `stack-aggregation-commit` to set a reward address
for the locked funds.

The special case handler for the PoX contract in the Clarity VM will
check this method's return value and set the stacker's STX account to
"auto-unlock" at the end of the last extended-to reward cycle.

### New method: `stack-increase`

This method allows direct stackers to lock additional funds for
the remaining cycles on their current lock.

This method checks that the stacker is still allowed to stack, that
the stacker has not delegated to an operator, that their funds are
currently stacked, and that they have enough unlocked funds to cover
the increase. The caller may *not* supply a new reward address for the
increase.

The special case handler for the PoX contract in the Clarity VM will
check this method's return value and set the locked amount in the
stacker's STX account to correspond to the increased amount.

### New method: `delegate-stack-increase`

This method allows operators to lock additional funds for one of their
delegation participants for the remaining cycles on that user's
current lock.

This method checks that the delegator is still authorized on behalf of
the given stacker (and remains authorized until the end of the
locked-for period), that the increased amount remains less than the
delegation's `amount-ustx` field, that the user is currently locked,
and that the user has enough unlocked funds to cover the increase.

Note that, just as with the existing `delegate-stack-stx` function,
this method locks *but does not commit* the user's STX. The delegation
operator must invoke `stack-aggregation-commit` to set a reward address
for the newly locked funds.

The special case handler for the PoX contract in the Clarity VM will
check this method's return value and set the locked amount in the
stacker's STX account to correspond to the increased amount.

### New method: `stack-aggregation-commit-indexed`

This method behaves identically to `stack-aggregation-commit`, except that it
returns the _index_ of the PoX address in the reward set calculated by the
`pox-2` contract.  This index can be used as an argument to the
`stack-aggregation-increase` function (see below), which is used to increase the
amount of STX locked for a PoX address by amounts that may be smaller than the
current Stacking minimum.

### New method: `stack-aggregation-increase`

This method behaves identically to `stack-aggregation-commit`, but will permit
the caller to increase the amount of STX locked up for an _existing_ PoX
address.  The PoX address is identified by an index returned by the new
`stack-aggregation-commit-indexed` method.

Unlike `stack-aggregation-commit`, there is no minimum number of STX that
must have been locked to be added to the
existing PoX address.  The only requirement is that the _total_ number of STX
exceeds the _current_ Stacking minimum at the time of the call.

### Changed: `delegate-stx`

This method has been changed so that the user can call it even while their STX
are locked.  This is meant to enable the user to increase their STX allowance to
their delegator can lock up for them while their STX are locked.  In such cases,
the user would call `revoke-delegate-stx` and then `delegate-stx` with their
higher STX allowance, and the delegator would subsequently call
`delegate-stack-increase` and `stack-aggregation-commit` to increase the user's 
locked-and-committed STX.

### Changed: Auto-Unlock

This SIP proposes that if the user's STX do not earn a single
reward slot in a reward cycle, then the user's STX are _automatically unlocked_
at the start of the reward cycle.  The user's STX would remain unlocked even if
they had locked them for subsequent reward cycles.

If a user has amassed more STX, or believes that they can acquire a reward slot
in a subsequent cycle, then they can re-stack their STX.

The current system behavior is to keep the user's STX locked for the duration of
their specified lock-up period, even if they are not earning them any reward
slots.

### Changed: Automatic PoX Sunset Removed

This SIP proposes removing the automatic PoX sunset that is currently slated to
activate in the current system.  The rationale for the PoX sunset was to address
an incentive problem with PoX: if miners acquire enough STX, they can mine at a
discount because the burnchain tokens they pay will be paid back to themselves.
While the existence of Stacking pools means this can't be avoided at any scale,
it is particularly bad for the chain if miners have so many STX Stacked that
they can control the _median_ mining commitment value in the sortition weight
calculation.  If they can do this, then they can spend as many burnchain tokens
as they want since they will get them back right away.  The effect of this
behavior is that miners who can discount-mine will eventually price out all
other miners, leading to a chain where only large STX-holders can effectively mine.

The PoX sunset fixes this incentive problem by capping any gains such a miner
could ever make over the system's lifetime.  Even if a miner did this, it would
lead to a temporary gain in their mining power.  However, Stacking has proven
successful and popular, and we believe that stopping its operation on an
automated schedule has the potential to harm the Stacks blockchain ecosystem.

As an alternative, we recommend that discount-mining behavior be policed by
vigilent users.  In the system today, 25% of the liquid STX can vote to stop PoX
payouts for the next reward cycle.  If discount-mining behavior becomes the
dominant strategy in Stacks, then users already have the power to fix the
miner incentives _if it becomes a problem_.  We do not know when, or if, it ever
will, but we believe that the mechanism(s) for determining when or if PoX
deactivates to counter discount-mining must be (1) adaptive in the face of an
ever-shifting set of discount-mining strategies and the strategies for
countering them, and (2) under the control of users, not Stacks blockchain developers.
The PoX sunset has neither of these properties.

A future SIP may propose an alternative mechanism for empowering users to
collectively address discount-mining in the event that either
a better strategy be discovered, the Stacks blockchain incentives
get altered to render this impractical.

### Changed: Support Segwit PoX Payout Addresses

The type of a PoX address is now `(tuple (hashbytes (buff 32)) (version (buff
1)))`.  This is to accommodate pay-to-witness-script-hash (p2wsh) and taproot (p2tr) scriptPubKeys on Bitcoin.  In
addition, new values for `version` are supported to represent these encodings:

   * `0x04` means this is a pay-to-witness-public-key-hash (p2wpkh) address, and `hashbytes` is the 20-byte hash160 of the witness script
   * `0x05` means this is a pay-to-witness-script-hash (p2wsh) address, and `hashbytes` is the 32-byte sha256 of the witness script
   * `0x06` means this is a pay-to-taproot (p2tr) address, and `hashbytes` is the 32-byte sha256 of the witness script

### Fixed: Expiration of `contract-caller` Allowances

The `pox` contract's implementation of contract-caller allowance
expirations is broken. `pox-2` should fix this behavior.

This behavior is not explicitly specified in SIP-007, but is a behavior present
in the reference implementation which enables users to allow other principals
(e.g. smart contracts) to Stack their STX on their behalf.  In the `pox`
contracts, users can set the principal allowed to do this at any time, and
specify the maximum burnchain height for which the authorization will be valid
(once this burnchain height passes, the allowance is automatically revoked).

### Usecase: Stacking without a Cooldown Cycle

#### Direct stackers

Direct stackers wishing to repeatedly stack without cooldown may
issue a simple contract-call transaction, invoking:

     `SP000000000000000000002Q6VF78.pox-2 stack-extend`

If the user wishes to *increase* the amount that they have stacked
in addition to repeating the stacking lockup, they can separately issue
a contract-call transaction to:

     `SP000000000000000000002Q6VF78.pox-2 stack-increase`

Users can launch a utility smart-contract to execute both of these
contract-calls atomically, but they must remember to invoke the
contract caller allowance for that utility contract.

#### Delegated stackers

Delegation operators wishing to repeatedly stack with a given user
without cooldown may do the following:

1. Issue a contract-call on behalf of the delegated stacker they wish
   to re-stack:
   
```clarity
   (contract-call? SP000000000000000000002Q6VF78.pox-2 delegate-stack-extend
     ...)
```

2. If the user wishes to *increase* the amount of STX they have stacked, then
   the user must first reset their delegated STX allowance:

```clarity
   (contract-call? SP000000000000000000002Q6VF78.pox-2 revoke-delegate-stx
      ...)
   (contract-call? SP000000000000000000002Q6VF78.pox-2 delegate-stx
      ...)
```

   Then, the delegator must separately issue a `delegate-stack-increase` call
   which locks but does not commit the increased funds.


3. The aforementioned contract-calls *lock but do not commit* the user's funds. In
   order for the delegation operator to register the reward address for
   those funds, they must invoke the aggregation commit function (just as
   when they invoke `delegate-stack-stx`).

   If this is the _first time_ the delegation operator is Stacking STX for a
    particular PoX reward address, they would call the new
   `stack-aggregation-commit-indexed` function, and remember its return value:

```clarity
   (contract-call? SP000000000000000000002Q6VF78.pox-2 stack-aggregation-commit-indexed
     ...)
```

   If this is a _subsequent time_ locking up user STX -- that is, if the
   delegation operator has already earned a PoX reward address, or did so during
   this reward cycle, then they can _increase_ the amount of STX locked to it as
   follows:

```clarity
   (contract-call? SP000000000000000000002Q6VF78.pox-2 stack-aggregation-increase
     ...)
```

   This permits the delegation operator to lock additional user STX, even if
   they do not by themselves exceed the Stacking minimum (a limitation imposed
   by the older `stack-aggregation-commit` function).

### Usecase: Stacking with Multiple PoX Addresses

#### Direct stackers

Direct stackers cannot use multiple reward address for the same Stacking
address. This is because the direct stacking interface does not support
partial stacking operations (i.e., separating the locking operation from
the stacking operation), which are necessary for supporting this use case.
Users who wish to do this should use the delegation interfaces.

#### Delegated stackers

Even in PoX-1, delegation operators are capable of assigning multiple
reward addresses for funds that they manage: each invocation of
`delegate-stack-stx` can use a different PoX reward address. However,
if operators wished to split the same _account's_ lockups across
multiple addresses, this would not be possible.

In PoX-2, however, delegation operators can use
`delegate-stack-increase` to achieve this. This method allows the
delegation operator to set a new PoX address to receive the
"partially stacked" funds from the increase. Once called, the operator
can use `stack-aggregation-commit` to commit to each PoX reward
address separately.

## Clarity

If this SIP is ratified, then at the time of the
Stacks 2.1 network upgrade, the Clarity smart
contract language will be expanded with new features and some new
behaviors for existing native methods. In order to support these
changes, this SIP proposes to introduce *Clarity versioning for smart
contracts*. In this scheme, each smart contract will be associated
with a particular Clarity version. The execution environment will
track the current version for a given execution (in the
implementation, this will be via the `ContractContext`), and use that
to select which features are available, and which native method
implementations will be used. Clarity 2 contracts can invoke Clarity 1
contracts (and vice-versa), but particular care will need to be taken
if a new native keyword is used in the Clarity 1 contract's API.

For example:

```
Contract A (Clarity 1 Contract):
  (define-public (stx-account) ...)
  
Contract B (Clarity 2 Contract):
  (contract-call contract-A stx-account)
```

In such cases, the new keyword (in the above example, the keyword is
`stx-account`) *cannot* be used by the Clarity 2 contract, and
therefore invoking the public method of Contract A from Contract B is
not allowed. To address these cases, contract authors must launch a
Clarity 1 contract that interposes on Contract A, providing a Clarity
2 compatible interface.

Existing, pre-2.1 contracts will all be Clarity 1. New contracts will
default to Clarity 2, and a new Stacks transaction wire format for publishing
smart contracts will allow contract publishers to choose the version that
their contract should use.

Note that the act of adding, changing, or removing a native Clarity function or native
Clarity global variable (including comparators and operators, which are
themselves native functions) necessitates the creation of a new version of the
Clarity language, and must be treated as a breaking change.
This is because adding, changing, or removing either of these
things alters the rules for block validation, which makes these 
consensus-level changes.  This SIP proposes introducing a new version of Clarity
(Clarity 2) _while also_ preserving the current version of Clarity (Clarity 1).  This
shall not be construed as setting a precedent -- a future SIP may remove the
ability to publish new smart contracts with older versions of Clarity.

### New method: `stx-account`

* **Input Signature:** `(stx-account (principal-address principal))`
* **Output Signature:** `{ locked: uint, unlock-height: uint, unlocked: uint }`

This method returns the status of STX account: the account's
current STX balance, the amount of STX that is currently locked,
and the unlock height for the account.

**Rationale:**  This method will be used by the `pox-2` contract to validate
various contract-calls: implementing early unlocks, lock extensions,
lock increases, etc.  It exposes PoX lock state to smart contracts, which is not
entirely visible even in `pox` or `pox-2`.

**Examples:**

```clarity
(stx-account 'SZ2J6ZY48GV1EZ5V2V5RB9MP66SW86PYKKQ9H6DPR) ;; Returns (tuple (locked u0) (unlock-height u0) (unlocked u0))
(stx-account (as-contract tx-sender)) ;; Returns (tuple (locked u0) (unlock-height u0) (unlocked u1000))
```

### New method: `principal-destruct?`

* **Input Signature:** `(principal-destruct? (principal-address principal))`
* **Output Signature:** `(response { hash-bytes: (buff 20), name: (optional (string-ascii 40)), version: (buff 1) } { hash-bytes: (buff 20), name: (optional (string-ascii 40)), version: (buff 1) })`

A principal value represents either a set of keys, or a smart contract.
The former, called a _standard principal_,
is encoded as a `(buff 1)` *version byte*, indicating the type of account
and the type of network that this principal can spend tokens on,
and a `(buff 20)` *public key hash*, characterizing the principal's unique identity.
The latter, a _contract principal_, is encoded as a standard principal concatenated with
a `(string-ascii 40)` *contract name* that identifies the code body.

`principal-destruct?` will decompose a principal into its component parts.  Standard principals will
be decomposed into a tuple containing the **version byte** and **public key
hash**.  Decomposed contract principals contain the same data as standard
principals, as well as a **name** element that contains the human-readable
contract name part of the contract address.

This method returns a `Response` that wraps this data as a tuple.

If the version byte of `principal-address` matches the network (see `is-standard`), then this method
returns the pair as its `ok` value.

If the version byte of `principal-address` does not match the network, then this method
returns the pair as its `err` value.

In both cases, the value itself is a tuple containing three fields: a `version` value as a `(buff 1)`,
a `hash-bytes` value as a `(buff 20)`, and a `name` value as an `(optional (string-ascii 40))`.  The `name`
field will only be `(some ..)` if the principal is a contract principal.

**Rationale:** This method is meant to help smart contracts integrate with
Bitcoin addresses and with other Stacks network instances' addresses.  Legacy
Bitcoin addresses can already be converted to Stacks addresses; the ability to
decode them in Clarity empowers developers to do things like verify that a
Bitcoin owner signed some data, or also controlled some Stacks data.

**Examples:**

```clarity
(principal-destruct? 'STB44HYPYAT2BB2QE513NSP81HTMYWBJP02HPGK6) ;; Returns (ok (tuple (hash-bytes 0x164247d6f2b425ac5771423ae6c80c754f7172b0) (name none) (version 0x1a)))
(principal-destruct? 'STB44HYPYAT2BB2QE513NSP81HTMYWBJP02HPGK6.foo) ;; Returns (ok (tuple (hash-bytes 0x164247d6f2b425ac5771423ae6c80c754f7172b0) (name (some "foo")) (version 0x1a)))
(principal-destruct? 'SP3X6QWWETNBZWGBK6DRGTR1KX50S74D3433WDGJY) ;; Returns (err (tuple (hash-bytes 0xfa6bf38ed557fe417333710d6033e9419391a320) (name none) (version 0x16)))
(principal-destruct? 'SP3X6QWWETNBZWGBK6DRGTR1KX50S74D3433WDGJY.foo) ;; Returns (err (tuple (hash-bytes 0xfa6bf38ed557fe417333710d6033e9419391a320) (name (some "foo")) (version 0x16)))
```

### New method: `principal-construct?`

* **Input Signatures:**
   * `(principal-construct? (buff 1) (buff 20))`
   * `(principal-construct? (buff 1) (buff 20) (string-ascii 40))`
* **Output Signature:** `(response principal { error_code: uint, value: (optional principal) })`

A principal value represents either a set of keys, or a smart contract.
The former, called a _standard principal_,
is encoded as a `(buff 1)` *version byte*, indicating the type of account
and the type of network that this principal can spend tokens on,
and a `(buff 20)` *public key hash*, characterizing the principal's unique identity.
The latter, a _contract principal_, is encoded as a standard principal concatenated with
a `(string-ascii 40)` *contract name* that identifies the code body.

The `principal-construct?` function allows users to create either standard or contract principals,
depending on which form is used.  To create a standard principal, 
`principal-construct?` would be called with two arguments: it
takes as input a `(buff 1)` which encodes the principal address's
`version-byte`, a `(buff 20)` which encodes the principal address's `hash-bytes`.

To create a contract principal, `principal-construct?` would be called with
three arguments: the `(buff 1)` and `(buff 20)` to represent the standard principal
that created the contract, and a `(string-ascii 40)` which encodes the contract's name.
On success, this function returns either a standard principal or contract principal, 
depending on whether or not the third `(string-ascii 40)` argument is given.

This function returns a `Response`. On success, the `ok` value is a `Principal`.
The `err` value is a value tuple with the form `{ error_code: uint, value: (optional principal) }`.

If the single-byte `version-byte` is in the valid range `0x00` to `0x1f`, but is not an appropriate
version byte for the current network, then the error will be `u0`, and `value` will contain
`(some principal)`, where the wrapped value is the principal.  If the `version-byte` is not in this range, 
however, then the `value` will be `none`.

If the `version-byte` is a `buff` of length 0, if the single-byte `version-byte` is a
value greater than `0x1f`, or the `hash-bytes` is a `buff` of length not equal to 20, then `error_code`
will be `u1` and `value` will be `None`.

If a name is given, and the name is either an empty string or contains ASCII characters
that are not allowed in contract names, then `error_code` will be `u2`.

**Rationale:** This method empowers developers to convert between Bitcoin and
Stacks addresses, and between Stacks addresses and subnet addresses (with
applications to enabling users to prove control of assets on different chains
within a smart contract).  In addition, it provides introspection for smart
contract addresses.

**Examples:**

```clarity
(principal-construct? 0x1a 0xfa6bf38ed557fe417333710d6033e9419391a320) ;; Returns (ok ST3X6QWWETNBZWGBK6DRGTR1KX50S74D3425Q1TPK)
(principal-construct? 0x1a 0xfa6bf38ed557fe417333710d6033e9419391a320 "foo") ;; Returns (ok ST3X6QWWETNBZWGBK6DRGTR1KX50S74D3425Q1TPK.foo)
(principal-construct? 0x16 0xfa6bf38ed557fe417333710d6033e9419391a320) ;; Returns (err (tuple (error_code u0) (value (some SP3X6QWWETNBZWGBK6DRGTR1KX50S74D3433WDGJY))))
(principal-construct? 0x16 0xfa6bf38ed557fe417333710d6033e9419391a320 "foo") ;; Returns (err (tuple (error_code u0) (value (some SP3X6QWWETNBZWGBK6DRGTR1KX50S74D3433WDGJY.foo))))
(principal-construct? 0x   0xfa6bf38ed557fe417333710d6033e9419391a320) ;; Returns (err (tuple (error_code u1) (value none)))
(principal-construct? 0x16 0xfa6bf38ed557fe417333710d6033e9419391a3)   ;; Returns (err (tuple (error_code u1) (value none)))
(principal-construct? 0x20 0xfa6bf38ed557fe417333710d6033e9419391a320) ;; Returns (err (tuple (error_code u1) (value none)))
(principal-construct? 0x1a 0xfa6bf38ed557fe417333710d6033e9419391a320 "") ;; Returns (err (tuple (error_code u2) (value none)))
(principal-construct? 0x1a 0xfa6bf38ed557fe417333710d6033e9419391a320 "foo[") ;; Returns (err (tuple (error_code u2) (value none)))
```

### New method: `get-burn-block-info?`

* **Input Signature:** `(get-burn-block-info? (prop-name BurnBlockPropertyName) (block-height uint))`
* **Output Signature:** `(optional buff) | (optional (tuple (addrs (list 2 (tuple (hashbytes (buff 32)) (version (buff 1))))) (payout uint)))`

The `get-burn-block-info?` function fetches data for a block of the given *burnchain* block height. The
value and type returned are determined by the specified `BlockInfoPropertyName`.  Valid values for `block-height` only
include heights between the burnchain height at the time the Stacks chain was launched, and the last-processed burnchain
block.  If the `block-height` argument falls outside of this range, then `none` shall be returned.

The following `BurnBlockInfoPropertyName` values are defined:

* The `header-hash` property returns a 32-byte buffer representing the header hash of the burnchain block at
burnchain height `block-height`.

* The `pox-addrs` property returns a tuple with two items: a list of up to two PoX addresses that received a PoX payout at that block height, and the amount of burnchain
tokens paid to each address (note that per the blockchain consensus rules, each PoX payout will be the same for each address in the block-commit transaction).
The list will include burn addresses -- that is, the unspendable addresses that miners pay to when there are no PoX addresses left to be paid.  During the prepare phase,
there will be exactly one burn address reported. During the reward phase, up to two burn addresses may be reported in the event that some PoX reward slots are not claimed.

The `addrs` list contains the same PoX address values passed into the PoX smart contract:
   * They each have type signature `(tuple (hashbytes (buff 32)) (version (buff 1)))`
   * The `version` field can be any of the following:
      * `0x00` means this is a pay-to-public-key-hash (p2pkh) address, and `hashbytes` is the 20-byte hash160 of a single public key
      * `0x01` means this is a pay-to-script-hash (p2sh) address, and `hashbytes` is the 20-byte hash160 of a redeemScript script
      * `0x02` means this is a pay-to-witness-public-key-hash-over-pay-to-script-hash (p2wpkh-p2sh) address, and `hashbytes` is the 20-byte hash160 of a p2wpkh witness script
      * `0x03` means this is a pay-to-witness-script-hash-over-pay-to-script-hash (p2wsh-p2sh) address, and `hashbytes` is the 20-byte hash160 of a p2wsh witness script
      * `0x04` means this is a pay-to-witness-public-key-hash (p2wpkh) address, and `hashbytes` is the 20-byte hash160 of the witness script
      * `0x05` means this is a pay-to-witness-script-hash (p2wsh) address, and `hashbytes` is the 32-byte sha256 of the witness script
      * `0x06` means this is a pay-to-taproot (p2tr) address, and `hashbytes` is the 32-byte sha256 of the witness script

**Rationale:**  This method empowers developers to query Bitcoin state.  Stacks
2.1 adds the ability for smart contracts to query PoX payouts directly, with applications
for Stacking pools and other Stacking-centric programs.

**Examples:**

```clarity
(get-burn-block-info? header-hash u677050) ;; Returns (some 0xe67141016c88a7f1203eca0b4312f2ed141531f59303a1c267d7d83ab6b977d8)
(get-burn-block-info? pox-addrs u677050) ;; Returns (some (tuple (addrs ((tuple (hashbytes 0x395f3643cea07ec4eec73b4d9a973dcce56b9bf1) (version 0x00)) (tuple (hashbytes 0x7c6775e20e3e938d2d7e9d79ac310108ba501ddb) (version 0x01)))) (payout u123)))
```

### New method: `slice?`

* **Input Signature:** `(slice? (sequence sequence_A) (left-position uint) (right-position uint))`
* **Output Signature:** `(optional sequence_A)`

The `slice?` function attempts to return a sub-sequence of `sequence_A` that starts
at `left-position` (inclusive), and ends at `right-position`
(non-inclusive).

If `left-position`==`right-position`, the function returns an empty
sequence.

If either `left-position` or `right-position` are out of bounds OR if
`right-position` is less than `left-position`, the function returns
`none`.

Values in `sequence_A` are zero-based.  That is, the first item in `sequence_A`
is at index 0. 

**Rationale:** This method facilitates parsing user-supplied encoded data, such
as data from oracles and other off-chain services.  While a variant of `slice?`
could be implemented entirely in Clarity, it would be much slower and much more
expensive than supplying a native method.

**Examples:**

```clarity
(slice? "blockstack" u5 u10) ;; Returns (some "stack")
(slice? (list 1 2 3 4 5) u5 u9) ;; Returns none
(slice? (list 1 2 3 4 5) u3 u4) ;; Returns (some (4))
(slice? "abcd" u0 u2) ;; Returns (some "ab")
(slice? "abcd" u1 u3) ;; Returns (some "bc")
(slice? "abcd" u2 u2) ;; Returns (some "")
(slice? "abcd" u3 u1) ;; Returns none
```

### New method: `string-to-int?`

* **Input Signature:** `(string-to-int? (input (string-ascii|string-utf8)))`
* **Output Signature:** `(optional int)`

Converts a string, either `string-ascii` or `string-utf8`, to an
optional-wrapped signed integer.  If the input string does not
represent a valid integer in decimal format, then the function returns `none`. Otherwise
it returns an `int` wrapped in `some`.

**Rationale:** This method facilitates parsing user-supplied encoded data from
text.  While this functionality could have been implemented in Clarity, it would
have been much more expensive to use.

**Examples:**

```clarity
(string-to-int? "1") ;; Returns (some 1)
(string-to-int? u"-1") ;; Returns (some -1)
(string-to-int? "a") ;; Returns none
```

### New method: `string-to-uint?`

* **Input Signature:** `(string-to-uint? (input (string-ascii|string-utf8)))`
* **Output Signature:** `(optional uint)`

Converts a string, either `string-ascii` or `string-utf8`, to an
optional-wrapped `uint`.  If the input string does not represent a
valid non-negative integer in decimal format, then the function returns
`none`. Otherwise it returns an `uint` wrapped in `some`.

**Rationale:** This method facilitates parsing user-supplied encoded data from
text.  While this functionality could have been implemented in Clarity, it would
have been much more expensive to use.

**Examples:**

```clarity
(string-to-uint? "1") ;; Returns (some u1)
(string-to-uint? u"1") ;; Returns (some u1)
(string-to-uint? "a") ;; Returns none
```

### New method: `int-to-ascii`

* **Input Signature:** `(int-to-ascii (input (int|uint)))`
* **Output Signature:** `string-ascii`

Converts  an integer,  either  `int` or  `uint`,  to a  `string-ascii`
string-value representation in decimal format.

**Rationale:** This method facilitates parsing user-supplied encoded data from
text.  While this functionality could have been implemented in Clarity, it would
have been much more expensive to use.

**Examples:**

```clarity
(int-to-ascii 1) ;; Returns "1"
(int-to-ascii u1) ;; Returns "1"
(int-to-ascii -1) ;; Returns "-1"
```

### New method: `int-to-utf8`

* **Input Signature:** `(int-to-utf8 (input (int|uint)))`
* **Output Signature:** `string-utf8`

Converts an integer, either `int` or `uint`, to a `string-utf8`
string-value representation in decimal format.

**Rationale:** This method facilitates parsing user-supplied encoded data from
text.  While this functionality could have been implemented in Clarity, it would
have been much more expensive to use.

**Examples:**

```clarity
(int-to-utf8 1) ;; Returns u"1"
(int-to-utf8 u1) ;; Returns u"1"
(int-to-utf8 -1) ;; Returns u"-1"
```

### New method: `buff-to-int-le`

* **Input Signature:** `(buff-to-int-le (input (buff 16)))`
* **Output Signature:** `int`

Converts a byte buffer to a signed integer use a little-endian
encoding.  The byte buffer can be up to 16 bytes in length. If there
are fewer than 16 bytes, as this function uses a little-endian
encoding, the input behaves as if it is zero-padded on the _right_.

**Rationale:** This method facilitates parsing user-supplied encoded data, or
data available on-chain that is currently represented as a `buff`.
While this functionality could have been implemented in Clarity, it would
have been much more expensive to use.

**Examples:**

```clarity
(buff-to-int-le 0x01) ;; Returns 1
(buff-to-int-le 0x01000000000000000000000000000000) ;; Returns 1
(buff-to-int-le 0xffffffffffffffffffffffffffffffff) ;; Returns -1
(buff-to-int-le 0x) ;; Returns 0
```

### New method: `buff-to-uint-le`

* **Input Signature:** `(buff-to-uint-le (input (buff 16)))`
* **Output Signature:** `uint`

Converts a byte buffer to an unsigned integer use a little-endian
encoding..  The byte buffer can be up to 16 bytes in length. If there
are fewer than 16 bytes, as this function uses a little-endian
encoding, the input behaves as if it is zero-padded on the _right_.

**Rationale:** This method facilitates parsing user-supplied encoded data, or
data available on-chain that is currently represented as a `buff`.
While this functionality could have been implemented in Clarity, it would
have been much more expensive to use.

**Examples:**

```clarity
(buff-to-uint-le 0x01) ;; Returns u1
(buff-to-uint-le 0x01000000000000000000000000000000) ;; Returns u1
(buff-to-uint-le 0xffffffffffffffffffffffffffffffff) ;; Returns u340282366920938463463374607431768211455
(buff-to-uint-le 0x) ;; Returns u0
```

### New method: `buff-to-int-be`

* **Input Signature:** `(buff-to-int-be (input (buff 16)))`
* **Output Signature:** `int`

Converts a byte buffer to a signed integer use a big-endian encoding.
The byte buffer can be up to 16 bytes in length. If there are fewer
than 16 bytes, as this function uses a big-endian encoding, the input
behaves as if it is zero-padded on the _left_.

**Rationale:** This method facilitates parsing user-supplied encoded data, or
data available on-chain that is currently represented as a `buff`.
While this functionality could have been implemented in Clarity, it would
have been much more expensive to use.

**Examples:**

```clarity
(buff-to-int-be 0x01) ;; Returns 1
(buff-to-int-be 0x00000000000000000000000000000001) ;; Returns 1
(buff-to-int-be 0xffffffffffffffffffffffffffffffff) ;; Returns -1
(buff-to-int-be 0x) ;; Returns 0
```

### New method: `buff-to-uint-be`

* **Input Signature:** `(buff-to-uint-be (input (buff 16)))`
* **Output Signature:** `uint`

Converts a byte buffer to an unsigned integer use a big-endian
encoding.  The byte buffer can be up to 16 bytes in length. If there
are fewer than 16 bytes, as this function uses a big-endian encoding,
the input behaves as if it is zero-padded on the _left_.

**Rationale:** This method facilitates parsing user-supplied encoded data, or
data available on-chain that is currently represented as a `buff`.
While this functionality could have been implemented in Clarity, it would
have been much more expensive to use.

**Examples:**

```clarity
(buff-to-uint-be 0x01) ;; Returns u1
(buff-to-uint-be 0x00000000000000000000000000000001) ;; Returns u1
(buff-to-uint-be 0xffffffffffffffffffffffffffffffff) ;; Returns u340282366920938463463374607431768211455
(buff-to-uint-be 0x) ;; Returns u0
```

### New method: `stx-transfer-memo?`

* **Input Signature:** `(stx-transfer? (amount uint) (sender principal) (recipient principal) (memo (buff 34)))`
* **Output Signature:** `(response bool uint)`

`stx-transfer-memo?` is similar to `stx-transfer?`, except that it
adds a `memo` field.

This function returns `(ok true)` if the transfer
is successful, or, on an error, returns the same codes as
`stx-transfer?`.

**Rationale:** This method facilitates integration with exchanges.  Exchanges
often require a memo field for transferring funds into and out of their wallets,
often for the purposes of multiplexing one address into many user accounts.
Adding a native `stx-transfer-memo?` function makes it easy for smart contracts
to interact with exchanges that follow this convention.

**Examples:**

```clarity
(as-contract
  (stx-transfer-memo? u50 'SP3X6QWWETNBZWGBK6DRGTR1KX50S74D3433WDGJY tx-sender 0x00)) ;; Returns (err u4)
(stx-transfer-memo? u60 tx-sender 'SP3X6QWWETNBZWGBK6DRGTR1KX50S74D3433WDGJY 0x010203) ;; Returns (ok true)
```

### New method: `is-standard`

* **Input Signature:** `(is-standard (standard-or-contract principal))`
* **Output Signature:** `bool`

Tests whether `standard-or-contract` _matches_ the current network
type, and therefore represents a principal that can spend tokens on the current
network type. That is, the network is either of type `mainnet`, or `testnet`.
Only `SPxxxx` and `SMxxxx` _c32check form_ addresses can spend tokens on
a mainnet, whereas only `STxxxx` and `SNxxxx` _c32check forms_ addresses can spend
tokens on a testnet. All addresses can _receive_ tokens, but only principal
_c32check form_ addresses that match the network type can _spend_ tokens on the
network.  This method will return `true` if and only if the principal matches
the network type, and false otherwise.

**Rationale:** This method facilitates authenticating principal types supplied
by contract-calls or from decoded data.  Because principals must have the
correct version byte in order to spend tokens, ensuring contracts can perform
this check is of paramount importance to avoid loss-of-funds.

**Examples:**

```clarity
(is-standard 'STB44HYPYAT2BB2QE513NSP81HTMYWBJP02HPGK6) ;; returns true on testnet and false on mainnet
(is-standard 'STB44HYPYAT2BB2QE513NSP81HTMYWBJP02HPGK6.foo) ;; returns true on testnet and false on mainnet
(is-standard 'SP3X6QWWETNBZWGBK6DRGTR1KX50S74D3433WDGJY) ;; returns true on mainnet and false on testnet
(is-standard 'SP3X6QWWETNBZWGBK6DRGTR1KX50S74D3433WDGJY.foo) ;; returns true on mainnet and false on testnet
(is-standard 'SZ2J6ZY48GV1EZ5V2V5RB9MP66SW86PYKKQ9H6DPR) ;; returns false on both mainnet and testnet
```

### New method: `to-consensus-buff?`

* **Input Signature:** `(to-consensus-buff? x)`
* **Output Signature:** `(optional buff))`

The `to-consensus-buff?` function is a special function that will serialize any
Clarity value into a buffer, using the SIP-005 serialization of the
Clarity value. Not all values can be serialized: some value's
consensus serialization is too large to fit in a Clarity buffer (this
is because of the type prefix in the consensus serialization).

If the value cannot fit as serialized into the maximum buffer size (1 MB),
this returns `none`, otherwise, it will be
`(some consensus-serialized-buffer)`. During type checking, the
analyzed type of the result of this method will be the maximum possible
consensus buffer length based on the inferred type of the supplied value.
Note that it is possible to construct a valid Clarity value whose buffer
serialization exceeds 1 MB; however, this is highly unlikely to happen by
accident.

**Rationale:** This method is used to facilitate interactions with off-chain
services, and to export data from a smart contract that can be consumed
off-chain.

**Examples:**

```clarity
(to-consensus-buff? 1) ;; Returns (some 0x0000000000000000000000000000000001)
(to-consensus-buff? u1) ;; Returns (some 0x0100000000000000000000000000000001)
(to-consensus-buff? true) ;; Returns (some 0x03)
(to-consensus-buff? false) ;; Returns (some 0x04)
(to-consensus-buff? none) ;; Returns (some 0x09)
(to-consensus-buff? 'SZ2J6ZY48GV1EZ5V2V5RB9MP66SW86PYKKQ9H6DPR) ;; Returns (some 0x051fa46ff88886c2ef9762d970b4d2c63678835bd39d)
(to-consensus-buff? { abc: 3, def: 4 }) ;; Returns (some 0x0c00000002036162630000000000000000000000000000000003036465660000000000000000000000000000000004)
```

### New method: `from-consensus-buff?`

* **Input Signature:** `(from-consensus-buff? type-signature buff)`
* **Output Signature:** `(optional t)`

The `from-consensus-buff?` function is a special function that will deserialize a
buffer into a Clarity value, using the SIP-005 serialization of the
Clarity value. The type that `from-consensus-buff?` tries to deserialize
into is provided by `type-signature`. If it fails
to deserialize the `buff` argument to this type, the method returns `none`.
Note that the given `buff` argument cannot represent more than 1 MB of data.

**Rationale:** This method is used to facilitate interactions with off-chain
services, and to implement "transaction-less transactions.""  The ability to
both verify the signature on a buffer and decode the buffer into a Clarity
value is a basic building block for bridges and oracles.  In addition, it is a
basic building block for smart contracts where users are not expected to send
transactions (e.g. because they lack a wallet, or because the transaction volume
would be too high).  Instead, they can send signed Clarity values to someone willing to
package them into transactions at a later time (possibly as a batch), and
send them to the smart contract on their behalf.

**Examples:**

```clarity
(from-consensus-buff? int 0x0000000000000000000000000000000001) ;; Returns (some 1)
(from-consensus-buff? uint 0x0000000000000000000000000000000001) ;; Returns none
(from-consensus-buff? uint 0x0100000000000000000000000000000001) ;; Returns (some u1)
(from-consensus-buff? bool 0x0000000000000000000000000000000001) ;; Returns none
(from-consensus-buff? bool 0x03) ;; Returns (some true)
(from-consensus-buff? bool 0x04) ;; Returns (some false)
(from-consensus-buff? principal 0x051fa46ff88886c2ef9762d970b4d2c63678835bd39d) ;; Returns (some SZ2J6ZY48GV1EZ5V2V5RB9MP66SW86PYKKQ9H6DPR)
(from-consensus-buff? { abc: int, def: int } 0x0c00000002036162630000000000000000000000000000000003036465660000000000000000000000000000000004) ;; Returns (some (tuple (abc 3) (def 4)))
```

### New method: `replace-at?`

* **Input Signature:** `(replace-at? (x (sequence Y)) (i uint) (value Y))`
* **Output Signature:** `(optional (sequence Y))`

The `replace-at?` function takes in a sequence, an index, and an element,
and returns a new sequence with the data at the index position replaced with the given element. 

The supported sequence types are `string-ascii`, `string-utf8`, `buff`, and
`list`.

* If `(sequence Y)` is either `(string-ascii ...)` or `(string-utf8...)`, then `(value Y)`
must be a `(string-ascii 1)` or a `(string-utf8 1)`, respectively.

* If `(sequence Y)` is `(buff ...)`, then `(value Y)` must be a `(buff 1)`.

* If `(sequence Y)` is a `(list Y ...)` (i.e. a list of values of type `Y`), then `(value Y)` 
must be a value of type `Y`.

If the provided index is out of bounds, this functions returns `none`.

Note that sequences are zero-indexed.  The first item in `x` is at index 0.

**Rationale:** This method makes it possible to set values of a sequence type
without incurring the costs of splitting the sequence into subsequences,
and concatenating them back together with the new value.  This operation is also
a common facility in many other languages, so supporting it in Clarity would
make Clarity more approachable to new developers.

**Examples:**

```clarity
(replace-at? u"ab" u1 u"c") ;; Returns (some u"ac")
(replace-at? 0x00112233 u2 0x44) ;; Returns (some 0x00114433)
(replace-at? "abcd" u3 "e") ;; Returns (some "abce")
(replace-at? (list 1) u0 10) ;; Returns (some (10))
(replace-at? (list (list 1) (list 2)) u0 (list 33)) ;; Returns (some ((33) (2)))
(replace-at? (list 1 2) u3 4) ;; Returns none
```

### New global: `tx-sponsor?`

* **Type:** `(optional principal)`

This global variable evaluates to the fee-sponsoring principal of the current transaction (if there is such a principal).

**Rationale:** Stacks already exposes the origin address as `tx-sender`, and
exposing this data would make it possible to build smart contracts where only
certain entities could relay transactions on behalf of users (such as entities
with which the user has an off-chain business relationship).

### New global: `chain-id`

* **Type:** `uint`

This global variable evaluates to a 32-bit chain ID that identifies this
instance of the Stacks blockchain.  The purpose of this global variable is to
give contract developers a way to determine which instance of Stacks their code
runs on, in order to execute instance-specific logic or integrate with
instance-specific off-chain services.

The following values are reported:

* `u1`:  This is the Stacks mainnet chain (this is `0x00000001`).

* `u2147483648`:  This is the Stacks testnet chain (this is `0x80000000`).

Other values may be used for other deployments of Stacks, such as (but not
limited to) subnets.

**Rationale**: Stacks already has two instances (testnet and mainnet), and
subnets are instances of Stacks.  Enabling smart contracts to differentiate
between which environments they run in would empower developers to add
(or rely on) chain-specific features.

### Added method alias: `element-at?`

This method aliases the existing method `element-at`, which returns the element at the specified index in the provided sequence.

**Rationale**: This alias is added so that there is consistent usage of the `?` suffix in builtin Clarity methods (used when the return value is a `response` or an `optional`). `element-at` from Clarity1 is left for compatibility, though tools should encourage use of the new spelling.

### Added method alias: `index-of?`

This method aliases the existing method `index-of`, which returns the first index at which the specified item can be found in the provided sequence.

**Rationale**: This alias is added so that there is consistent usage of the `?` suffix in builtin Clarity methods (used when the return value is a `response` or an `optional`). `index-of` from Clarity1 is left for compatibility, though tools should encourage use of the new spelling.

### New feature: Faster Clarity Parser

This SIP proposes a new Clarity lexer and parser implementation that will be
specific to Clarity 2.  Current benchmarks indicate that the reference
implementation of this new lexer and parser it is around 3x faster than the
reference implementation of the chain today.

The reason for proposing this as a breaking change is to de-risk the possibility
that the new lexer and parser are not "bug-for-bug" compatible with the current
parser.  Tying the release of this new lexer and parser with a breaking change,
while keeping the old parser for processing all blocks prior to the change,
ensures that any new behaviors introduced will only apply to transactions after
the change takes effect.

### New feature: New Trait Semantics

Over the lifetime of Clarity 1, it was discovered that there are a few
surprising behaviors of the current trait semantics.  Clarity 2 proposes the
following new trait semantics:

* **Traits are real types.** The Clarity 2 type system will analyze and propagate
  the type of a trait value in the same ways that it does with other types. This
means that in Clarity 2, trait values can be embedded into complex types (lists,
tuples, optionals, and responses) and maintain their ability to enable dynamic
dispatch. The only limitation on the trait type is that trait values may not be
persisted (e.g. in a data-var or map). Persisting a trait value, and maintaining
the ability to perform dynamic dispatch would inhibit static analysis of a
contract.

* **Trait values may be coerced into compatible trait types.**  When type
  checking a trait value against another trait type, the expected trait type
need not be the identical trait type of the value; instead, the trait value is
checked for compatibility with the expected type, meaning that if the expected
trait type is a subset of the trait value's type, then the coercion is legal.
See below for an example.

* **Trait values may be locally bound.** A trait value can be bound in a local
  scope and maintain its trait type and dynamic dispatch functionality. Binding
in a local scope includes `let` expressions and `match` expressions.

* **Traits may not be defined with duplicate function names.**  In Clarity 1 a
  trait could be defined with duplicated function names in its signature. This
resulted in only the last instance (in program order) being kept as part of the
trait's definition. In Clarity 2, a trait defined with a duplicate function name
will trigger an analysis error, preventing it from being mined in a block.

* **Contract principals stored in constants are callable.**  Clarity 2 allows a
  constant contract principal (defined with a `define-constant`) to be callable --
used to make static dispatch contract calls, or coerced into a trait type.

* **Imported trait name conflicts.** In Clarity 1, there is surprising behavior
  relating to the names of imported traits (from `use-trait`). The name of a local
trait will conflict with the trait name of the imported trait. For example, in
`(use-trait a-alias .a-trait.a)` if a local trait is defined with the name `a`, then
uses of `a-alias` will refer to the local trait, `a`, instead of the imported trait.
In Clarity 2, the imported trait can always be referenced by its alias and the
imported trait name will not conflict with local traits.

Clarity 1 trait semantics are preserved in Clarity 1 code.

#### Examples

```clarity
;; use trait in optional
(define-public (execute (job (optional <job-trait>))) (match job j (contract-call? j fn) (default-job)))

;; use of compatible traits, e.g. marketplace commissions
(define-trait t1 ((fn (response uint bool)))) 
(define-trait t2 ((fn (response uint bool)))) 
```

```clarity
;; --- contract 1
(impl-trait .contract.t1)
;; ...
```

```clarity
;; -- contract 2
(impl-trait .contract.t2)
;; ...
```

```clarity
;; -- contract 3
(use-trait t contract.t1)
;; can be called with contract 1 and contract 2 because of compatible traits
(define-public (meta-fn (ctr <t>)...)
```

### Changed: Comparators `>`, `>=`, `<=`, `<`

In Clarity version 2, these binary comparators will be extended to support
comparison of `string-ascii`, `string-utf8` and `buff`.

These comparisons are executed as follows:

* For `buff` and `string-ascii`, comparison is done on a byte-by-byte basis
  until a difference is found, or until the end of one sequence is reached.  In
  the first case, the differing bytes are compared with the comparator.  In the
  second case, the shorter of the two sequences is considered to be "less than"
  the other.

* For `string-utf8`, comparison is done on a codepoint-by-codepoint basis.
  Each codepoint occupies between 1 and 4 bytes.  Comparing two codepoints is
  the act of comparing them on a byte-by-byte basis, just as if they were `(buff 4)`s.
  Comparison proceeds until either a difference is found, or until the end of
  one sequence is reached. In the first case, the differing codepoints are
  compared with the comparator. In the second case, the shorter of the two
  sequences (as measured by number of codepoints) is considered to be "less than"
  the other.

Examples:
```
(>= "baa" "aaa") ;; Returns true
(>= "aaa" "aa") ;; Returns true
(>= 0x02 0x01) ;; Returns true
(> "baa" "aaa") ;; Returns true
(> "aaa" "aa") ;; Returns true
(> 0x02 0x01) ;; Returns true
(< "aaa" "baa") ;; Returns true
(< "aa" "aaa") ;; Returns true
(< 0x01 0x02) ;; Returns true
(< 0x01 0x0100) ;; Returns true
(< 0x01ff 0x01) ;; Returns false
(> 0x01ff 0x01) ;; Returns true
(< 0x01ff 0x02) ;; Returns true
(< u"\u{0380}" u"Z") ;; Returns false
(< u"\u{5a}" u"\u{5b}") ;; Returns true
(<= u"\u{5a}" u"Z") ;; Returns true
(>= u"\u{5a}" u"Z") ;; Returns true
(< u"stacks" u"st\u{c3a4}cks") ;; Returns true
```

### Changed: `get-block-info?`

The following new properties are proposed to be added `get-block-info?`:

* `block-reward`: This property returns a `uint` value for the total block reward of the indicated Stacks block.  This value is only available once the reward for 
the block matures.  That is, the latest `block-reward` value available is at least 101 Stacks blocks in the past (on mainnet).  The reward includes the coinbase,
the anchored block's transaction fees, and the shares of the confirmed and produced microblock transaction fees earned by this block's miner.  Note that this value may 
be smaller than the Stacks coinbase at this height, because the miner may have been punished with a valid `PoisonMicroblock` transaction in the event that the miner
published two or more microblock stream forks.

* `miner-spend-total`: This property returns a `uint` value for the total number of burnchain tokens (i.e. satoshis) spent by all miners trying to win this block.  This does _not_ 
include burnchain transaction fees.

* `miner-spend-winner`: This property returns a `uint` value for the number of burnchain tokens (i.e. satoshis) spent by the winning miner for this Stacks block.  Note that
this value is less than or equal to the value for `miner-spend-total` at the same block height.  This does _not_ include burnchain transaction fees.

**Rationale**: These changes to `get-block-info?` empower developers to
determine how many burnchain tokens (e.g. satoshis) are spent for STX on a
block-by-block basis, without having to rely on an off-chain oracle.

### Fixed: `principal-of?`

The `principal-of?` function returns the principal derived from the
provided public key.

If the `public-key` is invalid, it will return the error code `(err u1).`.

Before Stacks 2.1, this function has a bug, in that the principal
returned would always be a testnet single-signature principal, even if
the function were run on the mainnet. In Clarity version 2, this bug
is fixed, so that this function will return a principal suited to the
network it is called on. In particular, if this is called on the
mainnet, it will return a single-signature mainnet principal.

## Block Validation

### New Transaction: Alternative Coinbase Recipient

To enable miners to pay their block rewards to a cold wallet address, and to
enable mining pools to pay block rewards to a custodian smart contract address,
the Coinbase transaction payload is altered to include an alternative address to
which the block reward will materialize.  Its encoding is described as follows:

* **Payload Type ID**: 0x05
* **Payload**:
   * **Coinbase Memo**: 32 bytes
   * **Recipient Address**: variable bytes
      * This is an encoded Clarity `principal` value.  See SIP-005 for details.

### New Transaction: Versioned Smart Contract 

To support offering developers a choice to use Clarity 1 or Clarity 2, this SIP
proposes a new Stacks transaction payload variant for deploying a **versioned
smart contract**.  Its encoding is described as follows:

* **Payload Type ID**: 0x06
* **Payload**:
   * **Clarity Version**: 1 byte
      * 0x01: this is a Clarity 1 contract
      * 0x02: this is a Clarity 2 contract
   * **Smart-contract Payload**: variable bytes
      * This is identical to the smart-contract payload definition in SIP-005.
        It contains a length-prefixed name and a length-prefixed code body.  See
        SIP-005 for details.

## New Burnchain Transaction: `delegate-stx`

This SIP proposes adding support for issuing a `delegate-stx` call to PoX
from the burnchain, similar to how `stx-transfer?` and `stack-stx` are supported
as burnchain-hosted transactions in SIP-007.  The rationale for this is that
users may wish to maintain ownership of their STX tokens via burnchain
addresses, but permit Stacking them via Stacks addresses.  This way, the Stacks
blockchain will be better able to accommodate users who already have set up
and secured burnchain wallets to hold their STX.

The burnchain wire format is as follows:

* The first transaction input must consume a `PreSTX` operation's second output
  (see SIP-007).

* The first transaction output must contain an `OP_RETURN` payload:
   * **Bytes 0-2** are the magic bytes that identify this as a Stacks transaction
   * **Bytes 2-3** is the opcode, which shall be `#` (0x23).
   * **Bytes 3-19** is the number of uSTX to delegate, as a 128-bit big-endian
     integer.  This corresponds to the `amount-ustx` argument to `delegate-stx`.
   * **Bytes 19-23** is the optional index of the transaction output.
      * If **Byte 19** is set to `0x00`, then this field is ignored.  This
        corresponds to passing `none` to the `pox-addr` argument in
`delegate-stx`.
      * If **Byte 19** is set to `0x01`, then **bytes 20-23** are interpreted as
        a 32-bit big-endian integer, which shall index one of the transaction
outputs _after_ the `OP_RETURN` output.  For example, the output at index 0 is
the first output after the `OP_RETURN` output, index 1 is the second after, etc.
The output's address, if it decodes to a valid PoX address tuple, will be used
as the `pox-addr` argument to `delegate-stx`.
   * **Bytes 24-33** is the optional last burnchain block height for which these
     uSTX are delegated.
      * If **Byte 24** is set to `0x00`, then this field is ignored.  It
        corresponds to passing `none` to the `until-burn-ht` argument in
`delegate-stx`.
      * If **Byte 24** is set to `0x01`, then this field is the 128-bit
        big-endian integer that encodes the burnchain block height at which this
delegation expires.  This value corresponds to the `until-burn-ht` argument in
`delegate-stx`.

* The second transaction output encodes the Stacks address to which the STX are
  delegated.  This output must decode to a Stacks address.  This field corresponds to the
`delegate-to` argument in `delegate-stx`.

If the index that represents `pox-addr` in **bytes 19-23** does not correspond
to an existing output, or does not decode to a valid PoX address, then the
transaction is ignored.

The effect of sending a well-formed burnchain transaction with the above data is
to materialize a contract-call to `delegate-stx` in `pox-2` with the
aforementioned argument values.  Calls to `delegate-stx` will happen after the
calls to `stack-stx` and `transfer-stx` are materialized from burnchain
transactions.

### Changed: PoX with Forkable PoX Anchor Blocks

This SIP proposes making the history of PoX anchor blocks itself forkable, and by implementing
Nakamoto consensus on the PoX anchor block history forks so that there
will always be a canonical PoX anchor block history. In doing so, the
Stacks blockchain now has three levels of forks: the Bitcoin chain,
the history of PoX anchor blocks, and the history of Stacks
blocks. The canonical Stacks fork is the longest history of Stacks
blocks that passes through the canonical history of anchor blocks
which resides on the canonical Bitcoin chain.

To enable this, this SIP proposes a couple additional requirements for PoX anchor block
selection over those in SIP-007.  The rationale for these changes is to make
it possible to identify the PoX anchor block's block-commit _without having the Stacks chain state._
The reason this is important is because it allows the honest miners of the
network to identify _and ignore_ a hidden PoX anchor block that is no longer on the
canonical Stacks fork (should it be found later), which in turn prevents a hidden PoX anchor block's
arrival from triggering a deep chain reorganization.  The two additional
requirements for PoX anchor blocks are:

* **Heaviest block-commit**.  The PoX anchor block _must_ be mined in a
block-commit whose descendant block-commits in the prepare phase collectively represent
the most burnchain tokens (i.e. satoshis) burnt, out of all candidates.
This will statistically almost always be the case if there
is no or little contention between miners, but in the event that two or more
histories of block-commits arise which both (a) span at least 80 Bitcoin blocks
and (b) descend from conflicting PoX anchor blocks, then this rule is used to
break ties.  In the event that two such histories arise that burn an equal
amount of BTC, the PoX anchor block-commit which occurs _higher_ in the
burnchain is selected.  Note that at most one of these block-commit histories
will correspond to valid Stacks blocks, since each Bitcoin block in a prepare
phase chooses at most one block-commit as corresponding to a Stacks block in _any_
Stacks fork.

* **A block can be a PoX anchor block at most once.**  Today, miners can select
  the same Stacks block as the PoX anchor block over and over.  This SIP
proposes removing this possibility.

When these two changes are applied, it becomes possible for nodes to inspect
the burnchain transactions and determine the degree of _affirmation_ each PoX
anchor block selection has received by the network in subsequent prepare phases.  
The miners implicitly affirm the block's presence or absence when they make the
choice as to whether or not to mine off of a descendant.  With these changes, a
node can determine which block-commits correspond to PoX anchor blocks, and
from there, determine how many _subsequent_ PoX anchor blocks have built on a
previous PoX anchor block.

With this knowledge, the node can partition the set of Stacks forks that could
exist (based on the block-commits) into disjoint fork sets, where two forks are in
the same set if they contain the same sequence of PoX anchor blocks.  The
canonical fork _must_ contain the highest possible number of PoX anchor blocks;
all forks that contain fewer than this _must_ be non-canonical.  The node then
proceeds to ignore (i.e. store but not process) PoX anchor blocks for
forks classified as non-canonical in this manner.

The node only processes a deep reorganization -- i.e. switching its view of the
canonical Stacks fork to a Stacks fork that contains a _different_ set of PoX
anchor blocks -- once there exists a Stacks fork with _more_ PoX anchor blocks
than the one it currently treats as canonical.  This would only happen if the
miners of the network had mined sufficiently many PoX anchor blocks in
subsequent prepare phases to have made canonical a fork from a different fork set.
The barrier to entry for miners doing this is quite high: they must
_repeatedly_ select PoX anchor blocks in _multiple_ prepare phases -- more than 
those which have affirmed the current canonical fork -- and they must do so
without 20% or more opposition _each time_.  This makes executing a deep reorg
much harder to do for a malicious miner than it is today, because the malicious
miner must _repeatedly_ out-mine the defenders in each subsequent reward cycle
(and must consistently do so at a level of 80% or higher).  This is what we mean
when we say that PoX anchor blocks are now forkable, and follow Nakamoto
consensus.

In the event that two fork sets represent different PoX anchor blocks, but
represent the same _number_ of PoX anchor blocks, the canonical fork will reside
in the fork set whose PoX anchor block was mined last.

Details on how this feature works can be found in [6] and [7].

## Changed: Burnchain Transaction Grace Period

To address the usability limitations of on-burnchain Stacks operations, this SIP proposes allowing
these transactions to be processed in _all_ Stacks forks built upon within the _six_
Bitcoin blocks mined after such a transaction is mined.  Today, SIP-007
only requires that they are considered in the Stacks block mined in the
subsequent burnchain block (if one exists at all).

The reason for this change is because the existence of orphaned Stacks blocks
and burnchain blocks with no sortition (i.e. "flashblocks") are the root cause
of the poor user experience for on-burnchain transactions.
Today, if a burnchain operation is mined in
burnchain block N, and burnchain block N+1 either has no Stacks block selected
or selects an orphaned Stacks block, then the on-burnchain operation does not
take effect in the canonical Stacks fork.  The user will have wasted their time
and burnchain tokens (i.e. satoshis) attempting to use this facility, and will be forced to
retry the operation.

This proposal increases the period for which an on-burnchain operation will be
considered.  Instead of considering them for the Stacks block mined in
burnchain block N+1, they will be considered in all Stacks blocks mined in
burnchain blocks N+1 through N+6, inclusive.

When mining a block, miners track the on-burnchain transactions from the last 6
burnchain blocks and determine whether or not they have been applied to an
ancestor of the block they are mining.  If not, then they are applied to this
block.  Miners will not apply the on-burnchain operations on subsequent Stacks
blocks built off this block.

### Changed: Pay Transaction Fee before Processing

Prior to 2.1, the transaction-processing logic followed this procedure:

1. Verify that the spending account has enough balance to pay the fee
2. Run the transaction
3. Debit the fee from the spending account

The benefits of this approach are two-fold.  First, a transaction can earn the
paying account the requisite STX to pay its fee in step 3, so a too-low balance is not
necessarily a barrier-to-entry for new users.  Second, a user will not be
penalized in a contract-call or contract-publish transaction if the act of
running the transaction spends more STX than they anticipated.

The problem with this approach in practice is that it creates too much work for
miners.  If the account did not have enough STX left over at the end of step 2,
then step 3 could not complete, and the transaction could not be mined.  This
creates a problem for miners, who are obliged to process the transaction in
step 2 without knowing if an insufficient balance condition could arise in step
3.  Miners are not compensated for this work.  The reference implementation
addresses this problem by never re-considering
such transactions if they are detected, and never propagating them.

In 2.1, this problem is addressed by an alternative processing procedure:

1. Verify that the spending account has enough balance to pay the fee
2. Debit the fee from the spending account
3. Run the transaction

While this means that the benefits of the old approach are lost, it is the
authors' opinion that the upside of ensuring miners get paid for processing
transactions more than make up for this.  Specifically, this change means that
a user's pending transactions cannot be blocked by a dependent transaction in
the mempool which is unmineable simply because the balance was ultimately too
low.  Eliminating this class of errors with this alternative processing
procedure is deemed to itself be beneficial to the user experience above and
beyond what the original benefits offerred.  Furthermore, both the newly-added
`to-consensus-buff?` and `from-consensus-buff?` Clarity functions and the
existence of sposnored transactions already enable users to send signed Clarity
data to the Stacks blockchain without possessing any STX of their own.

### Changed: Analysis Errors are Runtime Errors

When processing a transaction, a transaction can fail for one of the following two
reasons:

* A **runtime error** can occur, in which the transaction attempts to do
  something the VM prohibits.  For example, divide-by-0 is a runtime error, as
is spending tokens that the caller does not possess.  Today, a transaction that
encounters a runtime error can be included in a block without invalidating the
block.

* A **check error** can occur, in which the transaction attempts to do something
  prohibited by the Clarity language itself.  For example, a transaction
can use `(at-block 0xabc)` to attempt to call a smart contract method in a trait
implementation which was instantiated _after_ the block `0xabc` was mined.
Attempting to call a function in a contract that does not exist is prohibited by
the Clarity language -- such an action cannot be expressed in Clarity.  Today,
a transaction that encounters a check error _cannot_ be included in a valid block.

Most check errors can be caught in the analysis pass of publishing a smart
contract, which would then prohibit them from being published.  However, there
are some classes of check errors (such as the one described above) where this is
infeasible.

This SIP proposes treating all check errors that arise during
transaction-processing as runtime errors.  Transactions that encounter check
errors would be mined, and their fees would be collected, but the transaction's
payload would never be applied to the chainstate.

This change benefits both miners and users.  With this change, miners will get paid for
processing a transaction regardless of how it would fail when executed.  Notably, this change
does _not_ prevent the user's wallet from executing a more-rigorous analysis
pass on any transactions the user sends.

This change also helps users because it means that transactions that encounter
check errors will get cleared from the mempool without any further action on
their parts.  Today, users must replace-by-fee (RBF) the problematic
transaction.

### Changed: Default Clarity Version

The behavior of the Smart-contract Payload transaction payload variant (see
SIP-005) is changed such that the Clarity version used to evaluate a new
contract shall be determined by the current system epoch.  Right now, the system
epoch is 2.05, in which case, a new smart contract instantiated this way shall
be processed with Clarity 1 rules.  In epoch 2.1, a new smart contract instantiated this
way shall be processed with Clarity 2 rules.  Users who wish to publish Clarity
1 contracts in epoch 2.1 can use the versioned smart contract payload described
above.

### Fixed: Miner Block-Commit Grace Period

Today, a Stacks miner's probability of winning a sortition is proportional to
the minimum of the spend from last block-commit mined, and the median spend of the last six
block-commits sent.  Because a block-commit transaction can be "late" -- i.e. it
may not be mined in the intended burnchain block, but in a subsequent one -- the
median block-commit spend is calculated as the median of the last six
transactions _sent_ by the miner, _including_ those that were late.

There was a bug in the implementation of this calculation whereby a late
block-commit transaction was treated as absent [8].  This not only prevented
this block-commit's spend from being considered, but it also prevented all of
the miner's _prior_ block-commits from being considered.  As a result, the act
of missing a block-commit had unduely negative impacts on mining -- for the next
two blocks, their median block-commit spend would be calculated as zero.

Upon further review from the Economics Consideration Advisory Board, it was
determined that the original algorithm was also flawed.  If a miner's previous
six transactions would be considered regardless of how late they were, then a
miner could send all six block-commits in the same burnchain block
and have a non-zero chance of winning a Stacks block.  This runs counter to the
incentives this algorithm was designed to create, which were to encourage
miners to mine persistently.

Furthermore, in the act of validating this finding from the Economics CAB, it
was discovered that the reference implementation does not check if a late block-commit
paid to the intended burnchain block's PoX recipients.  This is a logic bug that
can only be fixed in a breaking change.  Failure to fix this bug would further
mean that a miner could simply pay themselves in block-commits they knew would
be treated as late.  This in turn would mean that a miner could (a) mine
sporadically at the cost of a higher burnchain transaction fee, and (b)
discount-mine profitably if the miner happend to also own the upcoming PoX
reward slots.

To address these findings, this SIP proposes two changes to how the grace period
for late block-commits is handled:

* A block-commit can be late by no more than one burnchain block.  If it is
  late by two or more burnchain blocks, then none of its prior block-commits'
spends will be considered for the median spend calculation when the miner mines
next.  This means that a miner must have mined in at least two consecutive
burnchain blocks in order to have a non-zero chance of winning (i.e. their
median spend would be half of the lower of the two spends).  This also means
that a miner's median spend calculation will not be affected by block-commits
being late by one block, which is by far the most common miss distance for late
block-commits.

* A block-commit must always pay to its intended burnchain block's PoX
  recipients, even if it is late.  If a miner does not do this for a late
block-commit, their prior block-commits are not considered for their
subsequent median spend calculations.

# Related Work

Most blockchains regularly execute coordinated breaking changes
to add new features or behaviors.  Prominent examples include Ethereum [2] and
Tezos [3].  Indeed, Stacks has already gone through one breaking change [4] via
its SIP process.

Stacks strikes a balance between developer-coordinated breaking changes (like
Ethereum) and self-amending ledgers (like Tezos).  Like Ethereum, changes in
Stacks are sufficiently disruptive that for now, upgrades must be coordinated
out-of-band and are led by developers.  But unlike Ethereum, the Stacks SIP
process [5] has well-defined constraints on developers' ability to effect
changes unilaterally -- in particular, Stacks' governance system requires
independent reviewers to authorize breaking changes, and in the case of breaking
changes, requires (through precedent) users to explicitly consent to them
in-band through coin votes.

# Backwards Compatibility

This SIP retains full backwards-compatibility with the Stacks 2.05 chainstate.
All Clarity features that are supported today will continue to be supported.
However, developers publishing new Clarity 1 contracts will need to use the new
contract-publish transaction and explicitly indicate that they want to use
Clarity 1.  All smart contracts published with the standard contract-publish
transaction will default to using Clarity 2 rules.

Due to the introduction of new reserved keywords in Clarity 2, it will not be
possible to publish an already-written contract as a Clarity 2 contract if it
uses these reserved words for any other purpose.  However, already-deployed
contracts are not affected -- they were published under Clarity 1 rules, and
will continue to be usable as-is.

As mentioned above, Stacking will no longer be possible in the `pox` contract.  Calls to
Stacking operations in `pox` will fail.  All future Stacking operations happen
through the new `pox-2` contract.  However, the read-only contract calls in `pox`
will continue to be available for posterity.

All Stacked STX in `pox` will automatically unlock when this SIP activates, even if
they were locked for subsequent reward cycles.  Users will have a chance to re-Stack
their STX into the new `pox-2` contract in order to continue participating in
PoX once the subsequent reward cycle begins.

# Activation 

Because this SIP proposes a breaking change it is of paramount importance that
the ecosystem find this proposal acceptable.  To measure this, three sets of criteria
must be met in order to determine that there is sufficient support for
this SIP: one set for Stacked STX holders, one set for un-Stacked STX
holders, and one set for miners.  These critera are meant to broaden the set of
participating users above and beyond what has been done in the past.  In
particular, users who do not Stack are invited to vote on this SIP, as are users
who Stack in pools.

In all cases, voting will take place during reward cycles 46 and 47.  This
window is estimated to begin **starting November 10, 2022** and **ending
December 8, 2022**.

## For Stackers

In order for this SIP to activate, the following criteria must be met by the set
of Stacked STX:

* At least 80 million Stacked STX must vote _at all_ to activate this SIP.  This
  number is chosen because it is more than double the amount of STX locked by
the largest Stacker at the time of this writing (reward cycle 44).

* Of the Stacked STX that vote, at least 80% of them must vote "yes."

The act of not voting is the act of siding with the outcome, whatever it may be.
We believe that these thresholds are sufficient to demonstrate interest from
Stackers -- Stacks users who have a long-term interest in the Stacks
blockchain's succesful operation -- in performing this upgrade.

### How To Vote

If a user is Stacking, then their STX can be used to vote in one of two ways,
depending on whether or not they are solo-stacking or stacking through a
delegate.

The user must be Stacking in _both_ reward cycles 46 and 47.  Their vote
contribution will be the number of STX they have locked.  If the user has not
Stacked in either cycle, then they may vote instead as a non-stacker (see next
section).

#### Solo Stacking

The user must send a minimal amount of BTC from their PoX reward address to one
of the following Bitcoin addresses:

* For **"yes"**, the address is `11111111111111X6zHB1ZC2FmtnqJ`.  This is the
  base58check encoding of the hash in the Bitcoin script `OP_DUP OP_HASH160
000000000000000000000000007965732d322e31 OP_EQUALVERIFY OP_CHECKSIG`.  The value
`000000000000000000000000007965732d322e31` encodes "yes-2.1" in ASCII, with
0-padding.

* For **"no"**, the address is `1111111111111117CrbcZgemVNFx8`.  This is the
  base58check encoding of the hash in the Bitcoin script `OP_DUP OP_HASH160
00000000000000000000000000006e6f2d322e31 OP_EQUALVERIFY OP_CHECKSIG`.  The value
`00000000000000000000000000006e6f2d322e31` encodes "no-2.1" in ASCII, with
0-padding.

From there, the vote tabulation software will track the Bitcoin transaction back
to the PoX address in the `.pox` contract that sent it, and identify the
quantity of STX it represents.  The STX will count towards a "yes" or "no" based
on the Bitcoin address the PoX address sends to.

If the PoX address holder votes for both "yes" and "no" by the end of the vote,
the vote will be discarded.

Note that this voting procedure does _not_ apply to Stacking pool operators.
Stacking pool operator votes will not be considered.

#### Pooled Stacking

If the user is stacking in a pool, then they must send a minimal amount of STX
from their Stacking address to one of the following Stacks addresses to commit
their STX to a vote:

* For **"yes"**, the address is `SP00000000000003SCNSJTCHE66N2PXHX`.  This is the
  c32check-encoded Bitcoin address for "yes" (`11111111111111X6zHB1ZC2FmtnqJ`) above.

* For **"no"**, the address is `SP00000000000000DSQJTCHE66XE1NHQ`.  This is the
  c32check-encoded Bitcoin address for "no" (`1111111111111117CrbcZgemVNFx8`)
above.

From there, the vote tabulation software will track the STX back to the sender,
and verify that the sender also has STX stacked in a pool.  The Stacked STX will
be tabulated as a "yes" or "no" depending on which of the above two addresses
receive a minimal amount of STX.

If the Stacks address holder votes for both "yes" and "no" by the end of the
vote period, the vote will be discarded.

## For Non-Stackers

If the user is _not_ Stacking, then they can still vote with their liquid STX.
To facilitate this, the user would vote "yes" or "no" for a forthcoming Stacks 2.1
proposal in EcosystemDAO [1].  The text of this proposal shall be this SIP.
The STX in the user's Hiro Web Wallet will be tabulated 
to count for a "yes" or "no".

To prevent an influx of new STX holders from throwing the vote,
a _snapshot_ of all Stacks balances and lock-up states will be used to determine
how many STX a user can commit to a "yes" or "no" vote.  The snapshot will
be back-dated to a Stacks block prior to this SIP's activation, whose hash and
height shall be provided here when voting begins.

To prevent whales who possess many liquid STX from throwing the vote,
each address will only be permitted to vote with a maximum amount of STX equal
to the reward cycle's minimum Stacking threshold in the reward cycle in which
they vote.  This, combined with the snapshot, prevents whales from having too
much influence, even if they distributed their STX across many addresses.
For example, if Alice has 1 million liquid STX, but votes in a reward cycle
in which the minimum Stacking threshold is 120,000 STX, then only 120,000 of her
STX will be counted.

For this SIP to activate, a 66% majority of liquid STX must vote "yes".  There is no
threshold for how many STX must participate.

## For Miners

There is only one criterion for miners to activate this SIP: they must mine the
Stacks blockchain up to and past the end of the voting period.  In all reward
cycles between cycle 45 and the end of the voting period, PoX must activate.

## Examples

### Voting "yes" as a solo Stacker

Suppose Alice has stacked 100,000 STX to `1LP3pniXxjSMqyLmrKHpdmoYfsDvwMMSxJ`
during at least one of the voting period's reward cycles.  To vote,
she sends 5500 satoshis for **yes** to `11111111111111X6zHB1ZC2FmtnqJ`.  Then, her 100,000
STX are tabulated as "yes".

### Voting "no" as a pool Stacker

Suppose Bob has Stacked 1,000 STX in a Stacking pool and wants to vote "no", and
suppose it remains locked in PoX during at least one reward cycle in the voting
period.  Suppose his Stacks address is `SP2REA2WBSD3XMVMYS48NJKS3WB22JTQNB101XRRZ`.  To
vote, he sends 1 uSTX from `SP2REA2WBSD3XMVMYS48NJKS3WB22JTQNB101XRRZ` for **no** to
`SP00000000000000DSQJTCHE66XE1NHQ`. Then, his 1,000 STX are tabulated as "no."

### Voting "yes" with liquid STX

Suppose Charlie owns 1,000 STX in his wallet at the time of the snapshot,
and he wants to vote "yes."  To do
this, he would sign into EcosystemDAO (https://stx.eco) with his Web wallet,
select the Stacks 2.1 proposal, and select "yes."  If he does not spend or Stack
any STX over the voting period, then his 1,000 STX are tabulated as "yes."

### Voting "no" with liquid STX that are partially spent

Suppose Danielle owns 1,000 STX in her wallet at the time of the snapshot, and
she wants to vote "no."  After voting as Charlie had done in the previous
example, she sends 200 STX to an exchange, and later receives 30 STX, and
finally unlocks 2,000 STX from a Stacking pool.  In total, only 800 of her STX
will be tabulated as "no."

### Voting with both liquid STX and Stacked STX

Suppose Erik owns 1,000 STX in his wallet at the time of the snapshot, and he
wants to vote "yes."  After voting via EcosystemDAO as Charlie and Danielle
have, he Stacks 900 of his STX in the first voting period reward cycle.  When
the second reward cycle comes, he votes with his Stacked STX address for "no".
In this scenario, 1000 of Erik's STX will have counted towards "yes" for the
non-Stacker vote criteria, and 900 of his STX will have counted towards "no" for
the Stacker vote criteria.

## Activation Status

At the end of cycle 47, the following vote was calculated.  A total of
142,239,429 STX participated.

* For solo stacking, 135,287,693 STX voted YES (from 36 accounts), and 0 voted
  NO.

* For pool stacking, 6,173,363 STX voted YES (from 176 accounts), and 5,000 voted NO (from 2 accounts).

* For non-stackers, 612,756 STX voted YES (from 262 accounts), and 159,616 voted
  NO (from 5 accounts).

All voting criteria from STX holders have been met.  A breakdown of the
transactions can be found
[here](https://stx.eco/dao/voting/results/SP3JP0N1ZXGASRJ0F7QAHWFPGTVK9T2XNXDB908Z.edp015-sip-activation).

A copy of the scripts used to tabulate the solo and pool stacking can be found
[here](./sip015-vote-main.zip).

# Reference Implementation 

The reference implementation of this SIP can be found in the `develop` branch of
the Stacks blockchain reference implementation.  It is available at
https://github.com/stacks-network/stacks-blockchain.

## Errata

The reference implementation is known to have the following inconsistencies with
SIP-015:

* `get-burn-block-info?` only returns data for Bitcoin blocks of the grandparent
  block.  It does not return data for the parent of the block in which it is
evaluated.

# References

[1] https://github.com/Clarity-Innovation-Lab/ecosystem-dao

[2] https://ethereum.org/en/history/

[3] https://opentezos.com/tezos-basics/history-of-amendments/

[4] https://github.com/stacksgov/sips/blob/main/sips/sip-012/sip-012-cost-limits-network-upgrade.md

[5] https://github.com/stacksgov/sips/blob/main/sips/sip-000/sip-000-stacks-improvement-proposal-process.md

[6] https://docs.google.com/presentation/d/1iXvQlVZJ30xEB25v3ILHlcKU8eXB9MqpcMUPtoISpM4/edit?usp=sharing

[7] https://us06web.zoom.us/rec/share/vWdVjQ9I_rHsqRiLyo_FBdZFJbsr33tvVl2BdajfwJRFcxxGWrxyyfTuIXfrd-cP.LltAXR2SgAv7H_Vf?startTime=1623866540000
   * Passcode: nHU@4ENY

[8] https://github.com/stacks-network/stacks-blockchain/issues/2358

          
# Preamble

SIP Number: 013

Title: Standard Trait Definition for Semi-Fungible Tokens

Author: Marvin Janssen <https://github.com/MarvinJanssen>

Consideration: Technical

Type: Standard

Status: Ratified

Created: 12 September 2021

License: CC0-1.0

Sign-off: Jude Nelson <jude@stacks.org>, Brice Dobry <brice@hiro.so>

Layer: Traits

Discussions-To: https://github.com/stacksgov/sips

# Abstract

Semi-Fungible Tokens, or SFTs, are digital assets that sit between fungible and
non-fungible tokens. Fungible tokens are directly interchangeable, can be
received, sent, and divided. Non-fungible tokens each have a unique identifier
that distinguishes them from each other. Semi-fungible tokens have both an
identifier and an amount. This SIP describes the SFT trait and provides a
reference implementation.

# License and Copyright

This SIP is made available under the terms of the Creative Commons CC0 1.0
Universal license, available at
https://creativecommons.org/publicdomain/zero/1.0/. This SIP's copyright is held
by the Stacks Open Internet Foundation.

# Introduction

Digital assets commonly fall in one of two categories; namely, they are either
fungible or non-fungible. Fungible tokens are assets like the native Stacks
Token (STX), stablecoins, and so on. Non-Fungible Tokens (NFTs) are tokens
expressed as digital artwork and other use-cases that demand them to be globally
unique. However, not all asset classes can be represented as either exclusively
fungible or non-fungible tokens. This is where semi-fungible tokens come in.

Semi-fungible tokens are a combination of the aforementioned digital asset types
in that they have both an identifier and an amount. A single semi-fungible token
class can therefore represent a multitude of digital assets within a single
contract. A user may own ten tokens of ID 1 and twenty tokens of ID 2, for
example. It effectively means that one contract can represent any combination of
fungible and non-fungible tokens.

Some real-world examples can highlight the value and use-cases of semi-fungible
tokens. People who collect trading cards or postage stamps will know that not
all of them are of equal value, although there may be more than one of a
specific kind. Video games can feature in-game items that have different
economic values compared to others. There are many more such parallels to be
found.

Semi-fungible tokens give operators the ability to create new token classes at
will. They no longer need to deploy a new contract every time new token type is
introduced. It greatly simplifies the flow for applications that require many
new tokens and token types to come into existence.

Benefits of using semi-fungible tokens:
- Art NFTs can have series and be grouped in collections.
- Games can have their in-game currencies and items easily represented.
- DeFi protocols can leverage SFTs to transfer many tokens and settle multiple
  orders at once.
- Easy bulk trades and transfers in a single contract call, saving on
  transaction fees.

# Specification

The Semi-Fungible Token trait, `sip013-semi-fungible-token-trait`, has 8
functions:

## Trait functions

### Balance

`(get-balance ((token-id uint) (who principal)) (response uint uint))`

Returns the token type balance `token-id` of a specific principal `who` as an
unsigned integer wrapped in an `ok` response. It has to respond with `u0` if the
principal does not have a balance of the specified token or if no token with
`token-id` exists. The function should never return an `err` response and is
recommended to be defined as read-only.

### Overall balance

`(get-overall-balance ((who principal)) (response uint uint))`

Returns the overall SFT balance of a specific principal `who`. This is the sum
of all the token type balances of that principal. The function has to respond
with a zero value of `u0` if the principal does not have any balance. It should
never return an `err` response and is recommended to be defined as read-only.

### Total supply

`(get-total-supply ((token-id uint)) (response uint uint))`

Returns the total supply of a token type. If the token type has no supply or
does not exist, the function should respond with `u0`. It should never return an
`err` response and is recommended to be defined as read-only.

### Overall supply

`(get-overall-supply () (response uint uint))`

Returns the overall supply of the SFT. This is the sum of all token type
supplies. The function should never return an `err` response and is recommended
to be defined as read-only.

### Decimals

`(get-decimals ((token-id uint)) (response uint uint))`

Returns the decimal places of a token type. This is purely for display reasons,
where external applications may read this value to provide a better user
experience. The ability to specify decimals for a token type can be useful for
applications that represent different kinds of assets using one SFT. For
example, a game may have an in-game currency with two decimals and a fuel
commodity expressed in litres with four decimals.

### Token URI

`(get-token-uri ((token-id uint)) (response (optional (string-ascii 256)) uint))`

Returns an optional ASCII string that is a valid URI which resolves to this
token type's metadata. These files can provide off-chain metadata about that
particular token type, like descriptions, imagery, or any other information. The
exact structure of the metadata is out of scope for this SIP. However, the
metadata file should be in JSON format and should include a `sip` property
containing a number:

```JSON
{
	"sip": 16
	// ... any other properties
}
```

Applications consuming these metadata files can base display capabilities on the
`sip` value. It should refer to a SIP number describing a JSON metadata
standard.

### Transfer

`(transfer ((token-id uint) (amount uint) (sender principal) (recipient principal)) (response bool uint))`

Transfer a token from the sender to the recipient. It is recommended to leverage
Clarity primitives like `ft-transfer?` to help safeguard users. The function
should return `(ok true)` on success or an `err` response containing an unsigned
integer on failure. The failure codes follow the existing conventions of
`stx-transfer?` and `ft-transfer?`.

| Error code | Description                                      |
|------------|--------------------------------------------------|
| `u1`       | The sender has insufficient balance.             |
| `u2`       | The sender and recipient are the same principal. |
| `u3`       | Amount is `u0`.                                  |
| `u4`       | The sender is not authorised to transfer tokens. |

Error code `u4` is broad and may be returned under different cirumstances. For
example, a token  contract with an allowance mechanism can return `(err u4)`
when the `sender` parameter has no allowance for the specified token amount or
if the sender is not equal to `tx-sender` or `contract-owner`. A token contract
without an allowance mechanism can return `(err u4)` simply when the `sender` is
not what is expected.

Since it is possible for smart contracts to own tokens, it is recommended to
check for both `tx-sender` and `contract-caller` as it allows smart contracts to
transfer tokens it owns without having to resort to using `as-contract`. Such a
guard can be constructed as follows:

```clarity
(asserts! (or (is-eq sender tx-sender) (is-eq sender contract-caller)) (err u4))
```

The `transfer` function should emit a special transfer event, as detailed in the
Events section of this document.

### Transfer with memo

`(transfer-memo ((token-id uint) (amount uint) (sender principal) (recipient principal) (memo (buff 34))) (response bool uint))`

Transfer a token from the sender to the recipient and emit a memo. This function
follows the exact same procedure as `transfer` but emits the provided memo via
`(print memo)`. The memo event should be the final event emitted by the
contract. (See also the events section of this document below.)

## Trait definition

A definition of the trait is provided below.

```clarity
(define-trait sip013-semi-fungible-token-trait
	(
		;; Get a token type balance of the passed principal.
		(get-balance (uint principal) (response uint uint))

		;; Get the total SFT balance of the passed principal.
		(get-overall-balance (principal) (response uint uint))

		;; Get the current total supply of a token type.
		(get-total-supply (uint) (response uint uint))

		;; Get the overall SFT supply.
		(get-overall-supply () (response uint uint))

		;; Get the number of decimal places of a token type.
		(get-decimals (uint) (response uint uint))

		;; Get an optional token URI that represents metadata for a specific token.
		(get-token-uri (uint) (response (optional (string-ascii 256)) uint))

		;; Transfer from one principal to another.
		(transfer (uint uint principal principal) (response bool uint))

		;; Transfer from one principal to another with a memo.
		(transfer-memo (uint uint principal principal (buff 34)) (response bool uint))
	)
)
```
## Events

Semi-fungible token contracts should emit custom events in certain situations
via `print`. These events should be emitted after any built-in token events
(such as those emitted by `ft-transfer?`) and before the memo in the case of
`transfer-memo` and `transfer-many-memo`.

| Event name           | Tuple structure                                                                                 | Description                          |
|----------------------|-------------------------------------------------------------------------------------------------|--------------------------------------|
| `sft_transfer`       | `{type: "sft_transfer", token-id: uint, amount: uint, sender: principal, recipient: principal}` | Emitted when tokens are transferred. |
| `sft_mint`           | `{type: "sft_mint", token-id: uint, amount: uint, recipient: principal}`                        | Emitted when new tokens are minted.  |
| `sft_burn`           | `{type: "sft_burn", token-id: uint, amount: uint, sender: principal}`                           | Emitted when tokens are burned.      |

## Use of native asset functions

Contract implementers should always use the built-in native assets that are
provided as Clarity primitives whenever possible. This allows clients to use
Post Conditions (explained below) and takes advantage of other benefits like
native events and asset balances. However, there are no language primitives
specific to semi-fungible tokens. The reference implementation included in this
SIP therefore leverages the primitives to the extent that Clarity allows for.

The recommended native asset primitives to use:

- `define-fungible-token`
- `ft-burn?`
- `ft-get-balance`
- `ft-get-supply`
- `ft-mint?`
- `ft-transfer?`
- `define-non-fungible-token`
- `nft-burn?`
- `nft-mint?`
- `nft-transfer?`

## Implementing in wallets and other applications

Applications that interact with semi-fungible token contracts should validate if
those contracts implement the SFT trait. If they do, then the application can
use the interface described in this SIP for making transfers and getting other
token information.

All of the functions in this trait return the `response` type, which is a
requirement of trait definitions in Clarity. However, some of these functions
should be "fail-proof", in the sense that they should never return an error. The
"fail-proof" functions are those that have been recommended as read-only. If a
contract that implements this trait returns an error for these functions, it may
be an indication of a faulty contract, and consumers of those contracts should
proceed with caution.

## Use of post conditions

The Stacks blockchain includes a feature known as Post Conditions. By defining
post conditions, users can create transactions that include pre-defined
guarantees about what might happen in a contract. These post conditions can also
be used to provide guarantees for custom fungible and non-fungible tokens that
were defined using built-in Clarity primitives.

There are no Clarity primitive counterparts for semi-fungible tokens, but
contract developers can leverage a combination of post conditions to achieve the
same result.

There are two factors that should be checked by post conditions:

1. The amount of semi-fungible tokens being transferred.
2. The token ID of the semi-fungible token being transferred.

To that end, it is recommended that developers use both Clarity primitives in
their design. Semi-fungible token contracts can achieve complete post condition
coverage by using both `define-fungible-token` and `define-non-fungible-token`.

A minimal and sufficient strategy that provides full post condition coverage is
to create a "burn-and-mint" mechanism for token creation and transfers. Such an
SFT contract tracks quantities using an internal fungible token and token IDs
using an internal non-fungible token. Since token identifiers for assets defined
by `define-non-fungible-token` need to be unique, an additional component is
added to ensure token IDs can be expressed per owner. (As SFTs may have a
quantity of a certain token ID that is larger than one.) The token ID type
identifier thus becomes `{token-id: uint, owner: principal}`. Wallet software
can then easily determine the post conditions for the amount as well as the
token ID.

An example of a burn-and-mint mechanism is provided below. The reference
implementation at the end of the document features a full SFT contract that
includes burn-and-mint.

```clarity
(define-fungible-token semi-fungible-token)
(define-non-fungible-token semi-fungible-token-id {token-id: uint, owner: principal})

(define-public (transfer (token-id uint) (amount uint) (sender principal) (recipient principal))
	(begin
		;; <guards>
		;; <token transfer logic>
		(try! (tag-nft-token-id {token-id: token-id, owner: sender}))
		(try! (tag-nft-token-id {token-id: token-id, owner: recipient}))
		;; <balance updates>
		(print {type: "sft_transfer", token-id: token-id, amount: amount, sender: sender, recipient: recipient})
		(ok true)
	)
)

(define-private (tag-nft-token-id (nft-token-id {token-id: uint, owner: principal}))
	(begin
		(and
			(is-some (nft-get-owner? semi-fungible-token-id nft-token-id))
			(try! (nft-burn? semi-fungible-token-id nft-token-id (get owner nft-token-id)))
		)
		(nft-mint? semi-fungible-token-id nft-token-id (get owner nft-token-id))
	)
)
```

## Post Condition strategies

For strategies on how to best guard a semi-fungible token contract with post
conditions, see the reference implementation included with SIP (contained in
[SIP-013-001.tar.gz](SIP-013-001.tar.gz)), or by following the link at the end
of this document.

# Optional send-many specification

SIP013 Semi-fungible tokens can also optionally implement the trait
`sip013-send-many-trait` to offer a built-in "send-many" features for bulk token
transfers. Adding this to the token contract itself may have runtime cost
benefits as of Stacks 2.0. The send-many trait contains 2 additional functions.

## Send-many functions

### Bulk transfers

`(transfer-many ((transfers (list 200 {token-id: uint, amount: uint, sender: principal, recipient: principal}))) (response bool uint))`

Transfer many tokens in one contract call. Each transfer should follow the exact
same procedure as if it were an individual `transfer` call. The whole function
call should fail with an `err` response if one of the transfers fails.

### Bulk transfers with memos

`(transfer-many-memo ((transfers (list 200 {token-id: uint, amount: uint, sender: principal, recipient: principal, memo: (buff 34)}))) (response bool uint))`

Transfer many tokens in one contract call and emit a memo for each. This
function follows the same procedure as `transfer-many` but will emit the memo
contained in the tuple after each transfer. The whole function call should fail
with an `err` response if one of the transfers fails.

## Send-many trait definition

A definition of the optional send-many trait is provided below.

```clarity
(define-trait sip013-transfer-many-trait
	(
		;; Transfer many tokens at once.
		(transfer-many ((list 200 {token-id: uint, amount: uint, sender: principal, recipient: principal})) (response bool uint))

		;; Transfer many tokens at once with memos.
		(transfer-many-memo ((list 200 {token-id: uint, amount: uint, sender: principal, recipient: principal, memo: (buff 34)})) (response bool uint))
	)
)
```

# Related work

## Ethereum ERC1155

This Semi-Fungible Token standard is similar to the
[EIP-1155](https://eips.ethereum.org/EIPS/eip-1155) standard found in the
Ethereum/EVM space. An ERC1155 token is a semi-fungible token that admits both a
token ID as well as a supply per token ID, just like SIP013. They differ in that
the ERC1155 standard describes an approval mechanism as well as "safe transfer"
functions that are specific to Ethereum/EVM. Although the biggest difference is
the requirement of post condition support, a mechanism that does not exist in
Ethereum.

# Backwards Compatibility

Not applicable.

# Trait deployments

## Mainnet

- Token trait: [SPDBEG5X8XD50SPM1JJH0E5CTXGDV5NJTKAKKR5V.sip013-semi-fungible-token-trait](https://explorer.stacks.co/txid/0x7e9d8bac5157ab0366089d00a40a2a83926314ab08807ab3efa87ebc96d9e20a?chain=mainnet)
- Send-many trait: [SPDBEG5X8XD50SPM1JJH0E5CTXGDV5NJTKAKKR5V.sip013-transfer-many-trait](https://explorer.stacks.co/txid/0x88457278a61b7e59c8a19704932eebb7b46817e0bbd3235436a1d72c956db19c?chain=mainnet)

## Testnet

- Token trait: [STDBEG5X8XD50SPM1JJH0E5CTXGDV5NJTJTTH7YB.sip013-semi-fungible-token-trait](https://explorer.stacks.co/txid/0x37e846cce0d31f34be06d969efbb6ff413308066eefffa0bf1a8669bd4be0a05?chain=testnet)
- Send-many trait: [STDBEG5X8XD50SPM1JJH0E5CTXGDV5NJTJTTH7YB.sip013-transfer-many-trait](https://explorer.stacks.co/txid/0x81ec048b187137ade2fb9519375d22ec96c271d114e79c2d44018434e9009911?chain=testnet)

# Activation

These traits will be considered activated when they are deployed to mainnet
and 3 different implementations of the main trait have been deployed to mainnet,
no later than Bitcoin block 769,950. Additionally, no revisions to the traits
were made after Bitcoin block 756,810.

# Reference Implementations

- https://github.com/MarvinJanssen/stx-semi-fungible-token

          
# Preamble

SIP Number: 012

Title: Burn Height Selection for a Network Upgrade to Introduce New Cost-Limits

Authors:
* Asteria <asteria@syvita.org>
* Aaron Blankstein <aaron@hiro.so>
* Diwaker Gupta <diwaker@hiro.so>
* Hank Stoever <hank@stackerlabs.co>
* Jason Lau <jason@okcoin.com>
* Jude Nelson <jude@stacks.org>
* Ludovic Galabru <ludo@hiro.so>
* Trevor Owens <trevor@stacks.ac>
* Xan Ditkoff <xan@daemontechnologies.co>
* Pavitthra Pandurangan <pavitthra@hiro.so>
* Reed Rosenbluth <reed@hiro.so>

Consideration: Governance, Technical

Type: Consensus

Status: Ratified

Created: 2021-10-08

License: BSD 2-Clause

Sign-off: Harold Davis (governance) <recursive3@gmail.com>, Juliet Oberding
(governance) <juliet.oberding@gmail.com>, Jason Schrader (governance) <jason@joinfreehold.com>,
Jude Nelson (technical) <jude@stacks.org>

Discussions-To: https://github.com/stacksgov/sips

# Abstract

The Stacks 2.0 blockchain artificially constrains block goodput in two
consensus-critical ways: it assesses storage costs for lists based on their
maximum length instead of actual length, and it constrains the number of indexed
I/O operations well below what the reference implementation is capable of
handling.  Changing either of these is breaking change, which requires a
network-wide upgrade.

This SIP proposes executing a breaking change to not only
address these two constraints, but also to update all Clarity cost functions to
more accurately reflect their true performance.  The breaking change is carried
out via a network-wide vote by Stackers on the Bitcoin blockchain, which will
both serve to activate this SIP and to effectively bypass the cost voting procedure
in SIP-006.

The upgraded blockchain will be called Stacks 2.05.

# License and Copyright

This SIP is made available under the terms of the Creative Commons CC0 1.0
Universal license, available at
https://creativecommons.org/publicdomain/zero/1.0/ This SIP's copyright is held
by the Stacks Open Internet Foundation.

# Introduction

The current block limits were set very conservatively in Stacks 2.0, but
since the mainnet launch on 2021-01-14, traffic on the network has grown
steadily.  In recent months, we have seen network congestion adversely
impacting the user experience: valid transactions are not
getting mined in a timely manner because there is far more demand for block compute
capacity than supply.  For example, in Stacks blocks from height 27,672 through 28,573, 675
blocks' highest-filled compute dimension was their `runtime` dimension, but 319
blocks' highest-filled compute dimension was their `read_count` dimension ([full
report](./SIP-012-001.ods)).  In another study of 455 Stacks blocks between height
30,904 and 33,002, just over 14% of them exceeded the `read_count` dimension
and just over 85% exceeded the `runtime` dimension ([full report](https://github.com/blockstack/stacks-blockchain/discussions/2883)).

While there will likely always be more demand than supply for block compute
capacity, the current supply is artifically limited in three principal ways:

* At the time of the launch, the MARF index (see SIP-004) was implemented in
  such a way that a block could only execute 7,500 MARF index reads and writes
while being processed by a non-mining node on consumer hardware in a reasonable
amount of time.  This number was arrived at by measuring how many MARF reads and writes
could be completed on a consumer laptop in 10 seconds in 2019.  But because the block limit is a
consensus-critical constant that all Stacks nodes must agree on, increasing the
number of MARF reads and writes per block can only be done via a breaking
change.  This means that any improvements to the MARF's performance that could
permit significant increase in the number of operations per block can only
be capitalized upon via a breaking change.

* An emerging Clarity contract design pattern is to store data maps and data
  variables comprised of lists with a large maximum length.  The reason
for this, we suspect, is because it permits storing a lot of MARF-indexed data with
few MARF reads and writes.  However, the Clarity VM assesses list storage based
on its _maximum_ length, not the length of the data stored.  Assessing storage
based on the length of the data would allow contracts to make better use of the
`read_length` and `write_length` compute resources (which have been hit in
practice), but this would require a breaking change.

* Most of the cost functions in SIP-006 have constants that are far too
  conservative in practice.  The numbers used when mainnet launched were chosen
to minimize the risk of a network-wide denial-of-service arising from producing
blocks that would take an inordinate amount of time to validate; they were not
chosen through a rigorous benchmarking process.  In the months since then,
we have developed a more rigorous 
[benchmark suite](https://github.com/blockstack/clarity-benchmarking)
for the Clarity VM implementation, and have arrived at more accurate runtime
constants for the cost functions that permit suitable block validation times on contemporary
hardware.  The new limits, listed in [Appendix A](#appendix-a), 
would vastly increase the number of Clarity functions that can be
executed per block.  But in order to capitalize on this new data, STX token holders would
need to execute the SIP-006 cost voting protocol to activate new cost functions.

This SIP proposes a **breaking change** that would address these first two
limitations.  It would increase the block runtime `read_count` and `write_count`
limits by a factor of 2, in order to allow the network to capitalize on a [recent MARF performance
improvement](https://github.com/blockstack/stacks-blockchain/issues/2869).  It
would also change storage cost assessment for list data to consider the length
of the value being stored, instead of its maximum length.  In addition, this 
would bypass the voting protocol in SIP-006 to set 
[new proposed runtime cost functions parameters](https://forum.stacks.org/t/more-accurate-cost-functions-for-clarity-native-functions/12386)
via a voting protocol described in this SIP.

## A Note on Bypassing SIP-006

This SIP explicitly bypasses the voting procedure in SIP-006 by means of a separate
voting procedure described below.  However, this SIP does not supersede SIP-006,
nor does it set a precedent for this particular voting procedure's general
applicability to making collective decisions in the SIP process.  The voting
procedure in this SIP is specific to this SIP, and is only meant to activate the
changes described in this SIP.

The reason for this accommodation is that the SIP-006 voting procedure may be
too costly to use in practice, since STX holders must forego Stacking to use it.
A future SIP may study this problem further, and propose a new voting procedure 
for runtime costs in recognition of this.  However, that is not the subject of
this SIP.

# Specification

## Activation Protocol

In the text below, "Stacks 2.05" refers to the proposed network-upgrade for
cost-limits.

Due to the far-reaching effects a breaking change will have on the Stacks
ecosystem, this SIP can only be activated through a collective decision-making
process.  There are three major steps to this activation procedure:

   1.  The SIP authors will propose a Bitcoin block number at which the new cost-limits
   take effect. The block number should be at least two calendar weeks from when
   this SIP transitions into _Recommended_ status, so as to provide sufficient time for
   node operators to upgrade. Tentatively this block number would be chosen to fall
   on November 29th or November 30th, 2021.  In this document, this is the
**activation block**.

   On November 15, 2021, the SIP authors finalized the choice of Bitcoin block
713,000 to be the activation block.  This block is expected to be mined at or
around December 6, 2021.  The reason for this extra week delay over the
tentative block number is to ensure that the network upgrade happens in the
_middle_ of a reward cycle.  November 29th/30th is expected to be the start of
reward cycle 21. Executing a network upgrade at the start of a reward cycle is
needlessly risky, because if the upgrade fails for some reason during the
prepare phase, it could cause PoX rewards to be disabled for cycle 21.

   2.  In the two whole reward cycles prior to the activation block, users who
       have Stacked STX will have the opportunity to cast a vote to activate
this SIP.  The cut-off for voting will be a _separate_ Bitcoin block whose
expected arrival time is one calendar week prior to the activation block.  This
document refers to this block as the **vote deadline block.**

   On November 15, 2021, the SIP authors finalized the decision to select the
first Bitcoin block height mined after November 23, 2021 at 12:00 EST to be the
vote deadline block.  This was Bitcoin block 710,001.

   3.  If the activation voting threshold is met as of the vote deadline block,
then the Stacks Foundation will make a release of the Stacks blockchain
reference implementation with this
SIP's changes applied and set to take effect once the activation block passes.
If on the other hand there is insufficient support for this SIP by the vote
deadline block, then no action will be taken and this SIP will not activate.
   
   On November 23, 2021, the SIP authors inspected the Bitcoin and Stacks chainstate
using [vote-tallying scripts](./scripts) and determined that the the voting threshold has been met.
At least 129,615,879 stacked STX had voted in favor of this SIP, and 0 against.

To activate this SIP, users who have Stacked STX in either of the last two whole
reward cycles prior to the vote deadline block height will have the opportunity to
vote with their STX by sending a minimial amount of BTC to one of two addresses.
There will be two Bitcoin addresses whose UTXOs will be used to tally the
vote: a "yes" address, and a "no" address.

* The "yes" address will be a p2pkh Bitcoin address whose inner Hash160 is
  `00000000000000000000000000000000000000ee`.  On mainnet, this is address
`111111111111111111112czxoHN`.

* The "no" address will be a separate p2pkh Bitcoin address whose inner Hash160
  is `00000000000000000000000000000000000000ff`.  On mainnet, this is address
`111111111111111111112kmzDG2`.

Note that these are similar addresses to the PoX burn address, but they all
differ from one another in their checksums.

Vote tallying is performed by examining how many STX the Bitcoin transaction
sender has most-recently Stacked.  By examining the UTXOs for these two Bitcoin
addresses, anyone with a full copy of the Stacks chain state as of the voting
deadline will be able to calculate how many recently-Stacked STX have signaled
"yes" or "no" support for this SIP.

To match the Bitcoin transaction to the Stacker's state in the `.pox` contract,
the `scriptSig` of the first transaction input must be signed by either the user's PoX reward
address's public key(s), or the public key(s) of the standard principal Stacks address that Stacked
the tokens (the option is provided here because not all Stackers have access to their PoX
addresses).  In either case, the vote commits the Stacker's
most-recently-locked STX to "yes" or "no" if the Stacker had some STX locked
in the past two reward cycles as of the vote deadline block.

For Stackers that vote with their Stacks address key(s), the STX that the
associated standard principal had locked up will be committed to the vote.

For Stackers that vote with their PoX address key(s), the STX for _all_
associated Stacks principals that use this PoX address will be committed to the
vote (note that multiple Stacks principals may use the same PoX address).

For Stackers that have delegated STX to a Stacking pool, the pool operator must
perform the vote on their behalf.  As with a normal Stacker, the pool operator may 
sign the Bitcoin transaction with either the PoX reward address key(s) or the
standard Stacks principal address key(s).

For Stackers that have Stacked via a smart contract, only the PoX reward
address key(s) may be used to vote.

Stackers can send as many Bitcoin transactions as they like, but their STX will
only be counted once.  Only the *first* such voting transaction will be
considered to determine how the STX voted.

If a Stacker votes using both their STX address and PoX address, then the PoX
address will be used to count their STX.  A subsequent vote with the STX address
will be ignored as a duplicate vote.  In particular, the PoX address will
count *all* STX it represents.  For example, if Alice Stacks 100,000 STX with two STX
addresses that share the same PoX address, and she votes with her PoX address,
then the vote will count for 200,000 STX, and she will be unable to vote
separately with her STX addresses.  If instead she votes with only one
of her STX addresses, then that vote counts for 100,000 STX.

If a Stacker votes for both "yes" and "no," their vote will not be counted at
all.  This provides a way for a Stacker to cancel their vote, but they will be
unable to change it.

### Activation Criteria 

The SIP will be considered _Ratified_ if the vote to activate Stacks 2.05
passes. This requires:

* 2/3 of all votes passed are "yes", weighted by the STX they represent, at a
  Bitcoin block height at or before the vote deadline block.

* At least 60 million STX are represented by the "yes" votes. This is 2x the
  largest Stacker at the time of this writing.

### Rationale 

The rationale for this voting procedure is that it simultaneously gives the
community a way to veto the SIP while also accommodating low turnout. The
problem with blockchain-based voting systems in the past is that unless
there is a financial incentive to vote (e.g. mining, staking), turnout is low.
For example, the Ethereum carbon vote [1] to disable the DAO smart contract had only
5.5% turnout [2].  As another example, BitShares' [3] highest-voted delegate in its
delegated proof-of-stake consensus algorithm only received 18% of the vote [4].

This SIP's activation procedure takes the position that non-voters are passive
system participants and do not care about the outcome of this SIP -- they will
be happy either way.  But, this SIP also acknowledges that of the voters that
_do_ care about the vote outcome, those who vote "no" are financially
disincentivized to do so, because it would render the Stacks blockchain in a
worse-off state.  Therefore, this SIP requires a supermajority of "yes" votes to
activate, since a strong minority of "no" votes would be a strong signal that
something is seriously wrong with this SIP (despite its apparent benefits).

## Changes to Mining

Nodes that run Stacks 2.05 must put `0x05` in the memo field. Block-commit
transactions that do not have a value that is _at least_ `0x05` will be considered invalid.

The purpose of this change is to ensure that in the unlikely event some miners didn't know
about this SIP, they will quickly find out because their blocks will never be
confirmed or recognized by other users and exchanges that have upgraded.
Moreover, the "at least" condition is meant for future compatibility: if there
is ever another SIP that requires miners to signal support for the SIP's
activation via the memo field, then they can do so by putting in a _higher_
value while still remaining able to mine under the current rules.

## Changes to Runtime Costs

This SIP proposes two breaking changes to runtime costs, as well as a new set of
default cost functions (bypassing SIP-006's voting procedure).

### Block Limit

This SIP proposes increasing the block limits
for MARF reads and writes.  This is a breaking change.

Based on the expected performance improvements in the
implementation of the MARF (see [issue #2869](https://github.com/blockstack/stacks-blockchain/issues/2869)) 
this SIP proposes doubling the current limits on blocks:

```rust
pub const BLOCK_LIMIT_MAINNET: ExecutionCost = ExecutionCost {
    write_length: 15_000_000, // roughly 15 mb
    write_count: 15_000,
    read_length: 100_000_000,
    read_count: 15_000,
    runtime: 5_000_000_000,
};
```

### Changes to Static vs. Dynamic Tabulation of Costs

The cost assessment in Clarity for most data-handling functions (e.g.,
`map-get?`) use the static cost of the fetch rather than the dynamic cost. 
This is a breaking change.  For more information, see [issue #2864](https://github.com/blockstack/stacks-blockchain/issues/2864) in the
`stacks-blockchain` repository.

There are two motivating reasons for doing this:

* It makes static analysis of costs easier, because the cost assessed at runtime
  would always use the declared size of the map entry.
* It allows the cost to be assessed before running the operation.

However, these reasons have not been shown to be practical in production.
For (1), static analysis is always going to overestimate anyways, so system
throughput would improve by using the actual runtime overhead instead of the
maximum runtime overhead when assessing storage costs.  For (2),
allowing a single "speculative" evaluation before aborting a block due to cost
overflow is not particularly burdensome to the network: the maximum size of an
overread is a single Clarity value, which takes only 2MB.

The benefit of using dynamic costs, however, could be significant. Many contracts use
patterns where potentially long lists are stored in data maps, but in practice
the stored lists are relatively short.

Because of this, this SIP proposes using a dynamic cost for these assessments.
Specifically, it proposes changes to the inputs of the following
functions' cost functions:

* `var-get`
* `var-set`
* `map-get?`
* `map-set`
* `map-insert`
* `map-delete`
* `concat`
* `nft-mint?`
* `nft-burn?`
* `nft-transfer?`
* `nft-get-owner?`

#### `(var-get var-name) -> value`

The `x` input to the `var-get` cost function should be the length in
bytes of the consensus serialization (see [SIP-005](https://github.com/stacksgov/sips/blob/main/sips/sip-005/sip-005-blocks-and-transactions.md#clarity-value-representation)
for details on this format) of the returned `value`.

#### `(var-set var-name value)`

The `x` input to the `var-get` cost function should be the length in
bytes of the consensus serialization (see [SIP-005](https://github.com/stacksgov/sips/blob/main/sips/sip-005/sip-005-blocks-and-transactions.md#clarity-value-representation)
for details on this format) of the newly stored `value`.

The memory usage of this function should be this same value. The
memory usage of `var-set` remains in effect until the end of the
transaction (data operations remain in memory during the whole
transaction to enable rollbacks and post-conditions).

#### `(map-get? map-name key) -> value`

The `x` input to the `map-get` cost function should be the sum of the
length in bytes of the consensus serialization of the supplied `key`
and the returned `value`.

#### `(map-set map-name key value)`

The `x` input to the `map-set` cost function should be the sum of the
length in bytes of the consensus serialization of the supplied `key` and
`value` arguments.

The memory usage of this function should be this same value. The
memory usage of `map-set` remains in effect until the end of the
transaction (data operations remain in memory during the whole
transaction to enable rollbacks and post-conditions).

#### `(map-insert map-name key value)`

If the insert is successful, the `x` input to the `map-insert` cost
function should be the sum of the length in bytes of the consensus
serialization of the supplied `key` and `value` arguments.

If the insert is unsuccessful, the `x` input to the `map-insert` cost
function should be the length in bytes of the consensus serialization
of just the supplied `key` argument.

The memory usage of this function should be this same `x` value. The
memory usage of `map-insert` remains in effect until the end of the
transaction (data operations remain in memory during the whole
transaction to enable rollbacks and post-conditions).

#### `(map-delete map-name key)`

The `x` input to the `map-delete` cost function should be the length
in bytes of the consensus serialization of the supplied `key`
argument plus the length in bytes of the consensus serialization of
a `none` Clarity value.

The memory usage of this function should be this same `x` value. The
memory usage of `map-delete` remains in effect until the end of the
transaction (data operations remain in memory during the whole
transaction to enable rollbacks and post-conditions).

#### `(concat list-a list-b)`

The `x` input to the `concat` cost function should be the length of
`list-a` plus the length of `list-b`.

#### `(nft-mint? asset-class asset-identifier recipient)`

The `x` input to the `nft-mint?` cost function should be the length in bytes of
the consensus serialization of the supplied `asset-identifier`.

The memory usage of this function should be this new `x` value, plus the size of
a `principal` type.  The memory
usage of `nft-mint?` remains in effect until the end of the transaction (asset
operations remain in memory during the whole transaction to enable rollbacks and
post-conditions).

#### `(nft-burn? asset-class asset-identifier owner)`

The `x` input to the `nft-burn?` cost function should be the length in bytes of
the consensus serialization of the specified `asset-identifier`.

The memory usage of this function should be this new `x` value, plus the size of
a `principal` type.  The memory
usage of `nft-burn?` remains in effect until the end of the transaction (asset
operations remain in memory during the whole transaction to enable rollbacks and
post-conditions).

#### `(nft-transfer? asset-class asset-identifier sender recipient)`

The `x` input to the `nft-transfer?` cost function should be the length in bytes
of the consensus serialization of the specified `asset-identifier`.

The memory usage of this function should be this new `x` value, plus the size of
a `principal` type.  The memory
usage of `nft-transfer?` remains in effect until the end of the transaction (asset
operations remain in memory during the whole transaction to enable rollbacks and
post-conditions).

#### `(nft-get-owner? asset-class asset-identifier)`

The `x` input to the `nft-get-owner?` cost function should be the length in
bytes of the consensus serialization of the specified `asset-identifier`.

### New Default Cost Functions

Based on results from the
[clarity-benchmarking](https://github.com/blockstack/clarity-benchmarking)
project, this SIP proposes new default cost functions. The new costs are supplied in
the form of a new Clarity smart contract in [Appendix A](#appendix-a).

This could have been carried out through a SIP-006 cost voting procedure, but
due to the opportunity costs incurred by STX holders foregoing PoX rewards to
carry this vote out, this SIP instead proposes bypassing the SIP-006 voting
procedure in this one instance and instead using this SIP's activation procedure
to install these new functions.

# Related Work 

On-chain voting for upgrades is not a new concept.  Bitcoin has famously
executed many soft-forks using time-based activation [5], miner voting
thresholds [6], and a combination of both [7].  The voting approach in this SIP
uses both a timeout and a voting threshold to activate, but differs from
Bitcoin's approach in that it empowers Stackers as the instigators of the
upgrade.

We are aware of one other proposal
(distinct from the procedure described in SIP-006) suggested
using a voting contract to determine the block height at which a
network-upgrade, described in detail in [this Github
discussion](https://github.com/blockstack/stacks-blockchain/discussions/2845).
This SIP differs from this proposal in that the burnchain is used to identify
Stackers.

# Backwards Compatibility

This SIP proposes a breaking change.  If this SIP activates, then old miners'
block-commits will no longer be considered valid.  In addition, old nodes will
not accept new blocks as valid if they exceed the Stacks 2.0 block cost limits.
Therefore, all node operators are encouraged to upgrade immediately once SIP 012
transitions to _Activation-in-Progress_ status.

# Activation

The SIP will be considered _Ratified_ once all of the following are true:

* The cost functions in Appendix A are finalized.  This is a precondition for
  advancing the SIP to _Activation-in-Progress_ status.

* A vote deadline block height and activation block height are chosen and added
  to this SIP's text.  This is a pre-condition for advancing this SIP to
_Activation-in-Progress_ status.

* This SIP is advanced to _Activation-in-Progress_ by the respective consideration
  advisory boards.

* The SIP has garnered sufficient support by the vote deadline block height. Voting by
  sending Bitcoin transactions can begin once the SIP text is updated with the
  "yes" / "no" addresses. Voting concludes at least one week prior to the Stacks 2.05
  activation block.

* A new release of Stacks blockchain (available at
  https://github.com/blockstack/stacks-blockchain/releases) contains the updated
  cost-limits and a mechanism to use the new cost-limits beyond the activation
block height listed in this SIP.  This release is announced by the Stacks
Foundation.

* The activation block height passes on the Bitcoin chain.

## Activation Status

* On November 15, 2021, the authors finalized the choice of the activation
  block to be Bitcoin block 713,000.  This block is expected to be mined at or
around December 6, 2021 at 23:00 UTC.

* The vote deadline block will be backdated to the first Bitcoin block mined
  after November 23, 2021 at 12:00 EST.  The exact block number will be added to
this SIP's text once it is known.

# Reference Implementation

The reference implementation of this SIP is developed in the `next-costs` branch
of the reference implementation of the Stacks blockchain, available at
https://github.com/blockstack/stacks-blockchain.

# References

[1] http://v1.carbonvote.com/

[2] https://en.wikipedia.org/wiki/Ethereum_Classic#Carbon_vote

[3] https://en.bitcoinwiki.org/wiki/BitShares

[4] https://bitcointalk.org/index.php?topic=916696.330;imode

[5] https://github.com/bitcoin/bips/blob/master/bip-0016.mediawiki

[6] https://github.com/bitcoin/bips/blob/master/bip-0034.mediawiki

[7] https://github.com/bitcoin/bips/blob/master/bip-0009.mediawiki

# Appendices

## Appendix A

The new proposed cost functions, which will be instantiated at
`SP000000000000000000002Q6VF78.costs-2.05.clar`:

```lisp
;; the .costs-2 contract

;; Helper Functions

;; Return a Cost Specification with just a runtime cost
(define-private (runtime (r uint))
    {
        runtime: r,
        write_length: u0,
        write_count: u0,
        read_count: u0,
        read_length: u0,
    })

;; Linear cost-assessment function
(define-private (linear (n uint) (a uint) (b uint))
    (+ (* a n) b))

;; LogN cost-assessment function
(define-private (logn (n uint) (a uint) (b uint))
    (+ (* a (log2 n)) b))

;; NLogN cost-assessment function
(define-private (nlogn (n uint) (a uint) (b uint))
    (+ (* a (* n (log2 n))) b))


;; Cost Functions
(define-read-only (cost_analysis_type_annotate (n uint))
    (runtime (linear n u1 u9)))

(define-read-only (cost_analysis_type_check (n uint))
    (runtime (linear n u113 u1)))

(define-read-only (cost_analysis_type_lookup (n uint))
    (runtime (linear n u1 u6)))

(define-read-only (cost_analysis_visit (n uint))
    (runtime u1))

(define-read-only (cost_analysis_iterable_func (n uint))
    (runtime (linear n u2 u14)))

(define-read-only (cost_analysis_option_cons (n uint))
    (runtime u6))

(define-read-only (cost_analysis_option_check (n uint))
    (runtime u3))

(define-read-only (cost_analysis_bind_name (n uint))
    (runtime (linear n u2 u176)))

(define-read-only (cost_analysis_list_items_check (n uint))
    (runtime (linear n u2 u4)))

(define-read-only (cost_analysis_check_tuple_get (n uint))
    (runtime (logn n u1 u2)))

(define-read-only (cost_analysis_check_tuple_merge (n uint))
    (runtime (linear n u1000 u1000)))

(define-read-only (cost_analysis_check_tuple_cons (n uint))
    (runtime (nlogn n u3 u5)))

(define-read-only (cost_analysis_tuple_items_check (n uint))
    (runtime (linear n u1 u59)))

(define-read-only (cost_analysis_check_let (n uint))
    (runtime (linear n u1 u12)))

(define-read-only (cost_analysis_lookup_function (n uint))
    (runtime u20))

(define-read-only (cost_analysis_lookup_function_types (n uint))
    (runtime (linear n u1 u28)))

(define-read-only (cost_analysis_lookup_variable_const (n uint))
    (runtime u15))

(define-read-only (cost_analysis_lookup_variable_depth (n uint))
    (runtime (nlogn n u1 u34)))

(define-read-only (cost_ast_parse (n uint))
    (runtime (linear n u172 u287441)))

(define-read-only (cost_ast_cycle_detection (n uint))
    (runtime (linear n u141 u72)))

(define-read-only (cost_analysis_storage (n uint))
    {
        runtime: (linear n u2 u100),
        write_length: (linear n u1 u1),
        write_count: u1,
        read_count: u1,
        read_length: u1
    })

(define-read-only (cost_analysis_use_trait_entry (n uint))
    {
        runtime: (linear n u9 u723),
        write_length: (linear n u1 u1),
        write_count: u0,
        read_count: u1,
        read_length: (linear n u1 u1)
    })


(define-read-only (cost_analysis_get_function_entry (n uint))
    {
        runtime: (linear n u81 u1303),
        write_length: u0,
        write_count: u0,
        read_count: u1,
        read_length: (linear n u1 u1)
    })


(define-read-only (cost_analysis_fetch_contract_entry (n uint))
    {
        runtime: (linear n u1000 u1000),
        write_length: u0,
        write_count: u0,
        read_count: u1,
        read_length: (linear n u1 u1)
    })

(define-read-only (cost_lookup_variable_depth (n uint))
    (runtime (linear n u2 u14)))

(define-read-only (cost_lookup_variable_size (n uint))
    (runtime (linear n u2 u1)))

(define-read-only (cost_lookup_function (n uint))
    (runtime u16))

(define-read-only (cost_bind_name (n uint))
    (runtime u256))

(define-read-only (cost_inner_type_check_cost (n uint))
    (runtime (linear n u2 u9)))

(define-read-only (cost_user_function_application (n uint))
    (runtime (linear n u26 u140)))

(define-read-only (cost_let (n uint))
    (runtime (linear n u146 u862)))

(define-read-only (cost_if (n uint))
    (runtime u200))

(define-read-only (cost_asserts (n uint))
    (runtime u170))

(define-read-only (cost_map (n uint))
    (runtime (linear n u1210 u3314)))

(define-read-only (cost_filter (n uint))
    (runtime u460))

(define-read-only (cost_len (n uint))
    (runtime u486))

(define-read-only (cost_element_at (n uint))
    (runtime u619))

(define-read-only (cost_index_of (n uint))
    (runtime (linear n u1 u243)))

(define-read-only (cost_fold (n uint))
    (runtime u483))

(define-read-only (cost_list_cons (n uint))
    (runtime (linear n u14 u198)))

(define-read-only (cost_type_parse_step (n uint))
    (runtime u5))

(define-read-only (cost_tuple_get (n uint))
    (runtime (nlogn n u4 u1780)))

(define-read-only (cost_tuple_merge (n uint))
    (runtime (linear n u4 u646)))

(define-read-only (cost_tuple_cons (n uint))
    (runtime (nlogn n u11 u1101)))

(define-read-only (cost_add (n uint))
    (runtime (linear n u14 u157)))

(define-read-only (cost_sub (n uint))
    (runtime (linear n u14 u157)))

(define-read-only (cost_mul (n uint))
    (runtime (linear n u14 u157)))

(define-read-only (cost_div (n uint))
    (runtime (linear n u14 u157)))

(define-read-only (cost_geq (n uint))
    (runtime u170))

(define-read-only (cost_leq (n uint))
    (runtime u170))

(define-read-only (cost_le (n uint))
    (runtime u170))

(define-read-only (cost_ge (n uint))
    (runtime u170))

(define-read-only (cost_int_cast (n uint))
    (runtime u170))

(define-read-only (cost_mod (n uint))
    (runtime u170))

(define-read-only (cost_pow (n uint))
    (runtime u170))

(define-read-only (cost_sqrti (n uint))
    (runtime u170))

(define-read-only (cost_log2 (n uint))
    (runtime u170))

(define-read-only (cost_xor (n uint))
    (runtime u170))

(define-read-only (cost_not (n uint))
    (runtime u170))

(define-read-only (cost_eq (n uint))
    (runtime (linear n u7 u172)))

(define-read-only (cost_begin (n uint))
    (runtime u202))

(define-read-only (cost_hash160 (n uint))
    (runtime (linear n u1 u201)))

(define-read-only (cost_sha256 (n uint))
    (runtime (linear n u1 u100)))

(define-read-only (cost_sha512 (n uint))
    (runtime (linear n u1 u176)))

(define-read-only (cost_sha512t256 (n uint))
    (runtime (linear n u1 u188)))

(define-read-only (cost_keccak256 (n uint))
    (runtime (linear n u1 u221)))

(define-read-only (cost_secp256k1recover (n uint))
    (runtime u14344))

(define-read-only (cost_secp256k1verify (n uint))
    (runtime u13540))

(define-read-only (cost_print (n uint))
    (runtime (linear n u3 u1413)))

(define-read-only (cost_some_cons (n uint))
    (runtime u230))

(define-read-only (cost_ok_cons (n uint))
    (runtime u230))

(define-read-only (cost_err_cons (n uint))
    (runtime u230))

(define-read-only (cost_default_to (n uint))
    (runtime u287))

(define-read-only (cost_unwrap_ret (n uint))
    (runtime u339))

(define-read-only (cost_unwrap_err_or_ret (n uint))
    (runtime u339))

(define-read-only (cost_is_okay (n uint))
    (runtime u287))

(define-read-only (cost_is_none (n uint))
    (runtime u287))

(define-read-only (cost_is_err (n uint))
    (runtime u287))

(define-read-only (cost_is_some (n uint))
    (runtime u287))

(define-read-only (cost_unwrap (n uint))
    (runtime u287))

(define-read-only (cost_unwrap_err (n uint))
    (runtime u287))

(define-read-only (cost_try_ret (n uint))
    (runtime u287))

(define-read-only (cost_match (n uint))
    (runtime u287))

(define-read-only (cost_or (n uint))
    (runtime (linear n u3 u149)))

(define-read-only (cost_and (n uint))
    (runtime (linear n u3 u149)))

(define-read-only (cost_append (n uint))
    (runtime (linear n u71 u176)))

(define-read-only (cost_concat (n uint))
    (runtime (linear n u75 u244)))

(define-read-only (cost_as_max_len (n uint))
    (runtime u475))

(define-read-only (cost_contract_call (n uint))
    (runtime u153))

(define-read-only (cost_contract_of (n uint))
    (runtime u13400))

(define-read-only (cost_principal_of (n uint))
    (runtime u999))


(define-read-only (cost_at_block (n uint))
    {
        runtime: u210,
        write_length: u0,
        write_count: u0,
        read_count: u1,
        read_length: u1
    })


(define-read-only (cost_load_contract (n uint))
    {
        runtime: (linear n u1 u157),
        write_length: u0,
        write_count: u0,
        ;; set to 3 because of the associated metadata loads
        read_count: u3,
        read_length: (linear n u1 u1)
    })


(define-read-only (cost_create_map (n uint))
    {
        runtime: (linear n u1 u1631),
        write_length: (linear n u1 u1),
        write_count: u1,
        read_count: u0,
        read_length: u0
    })


(define-read-only (cost_create_var (n uint))
    {
        runtime: (linear n u7 u2152),
        write_length: (linear n u1 u1),
        write_count: u2,
        read_count: u0,
        read_length: u0
    })


(define-read-only (cost_create_nft (n uint))
    {
        runtime: (linear n u1 u1610),
        write_length: (linear n u1 u1),
        write_count: u1,
        read_count: u0,
        read_length: u0
    })


(define-read-only (cost_create_ft (n uint))
    {
        runtime: u1972,
        write_length: u1,
        write_count: u2,
        read_count: u0,
        read_length: u0
    })


(define-read-only (cost_fetch_entry (n uint))
    {
        runtime: (linear n u1 u1539),
        write_length: u0,
        write_count: u0,
        read_count: u1,
        read_length: (linear n u1 u1)
    })


(define-read-only (cost_set_entry (n uint))
    {
        runtime: (linear n u4 u2204),
        write_length: (linear n u1 u1),
        write_count: u1,
        read_count: u1,
        read_length: u0
    })


(define-read-only (cost_fetch_var (n uint))
    {
        runtime: (linear n u1 u543),
        write_length: u0,
        write_count: u0,
        read_count: u1,
        read_length: (linear n u1 u1)
    })


(define-read-only (cost_set_var (n uint))
    {
        runtime: (linear n u5 u691),
        write_length: (linear n u1 u1),
        write_count: u1,
        read_count: u1,
        read_length: u0
    })


(define-read-only (cost_contract_storage (n uint))
    {
        runtime: (linear n u13 u7982),
        write_length: (linear n u1 u1),
        write_count: u1,
        read_count: u0,
        read_length: u0
    })


(define-read-only (cost_block_info (n uint))
    {
        runtime: u6321,
        write_length: u0,
        write_count: u0,
        read_count: u1,
        read_length: u1
    })


(define-read-only (cost_stx_balance (n uint))
    {
        runtime: u1385,
        write_length: u0,
        write_count: u0,
        read_count: u1,
        read_length: u1
    })


(define-read-only (cost_stx_transfer (n uint))
    {
        runtime: u1430,
        write_length: u1,
        write_count: u1,
        read_count: u1,
        read_length: u1
    })


(define-read-only (cost_ft_mint (n uint))
    {
        runtime: u1645,
        write_length: u1,
        write_count: u2,
        read_count: u2,
        read_length: u1
    })


(define-read-only (cost_ft_transfer (n uint))
    {
        runtime: u612,
        write_length: u1,
        write_count: u2,
        read_count: u2,
        read_length: u1
    })


(define-read-only (cost_ft_balance (n uint))
    {
        runtime: u547,
        write_length: u0,
        write_count: u0,
        read_count: u1,
        read_length: u1
    })


(define-read-only (cost_nft_mint (n uint))
    {
        runtime: (linear n u9 u795),
        write_length: u1,
        write_count: u1,
        read_count: u1,
        read_length: u1
    })


(define-read-only (cost_nft_transfer (n uint))
    {
        runtime: (linear n u9 u795),
        write_length: u1,
        write_count: u1,
        read_count: u1,
        read_length: u1
    })


(define-read-only (cost_nft_owner (n uint))
    {
        runtime: (linear n u9 u795),
        write_length: u0,
        write_count: u0,
        read_count: u1,
        read_length: u1
    })


(define-read-only (cost_ft_get_supply (n uint))
    {
        runtime: u483,
        write_length: u0,
        write_count: u0,
        read_count: u1,
        read_length: u1
    })


(define-read-only (cost_ft_burn (n uint))
    {
        runtime: u612,
        write_length: u1,
        write_count: u2,
        read_count: u2,
        read_length: u1
    })


(define-read-only (cost_nft_burn (n uint))
    {
        runtime: (linear n u9 u795),
        write_length: u1,
        write_count: u1,
        read_count: u1,
        read_length: u1
    })


(define-read-only (poison_microblock (n uint))
    {
        runtime: u29568,
        write_length: u1,
        write_count: u1,
        read_count: u1,
        read_length: u1
    })
```

### Determining runtime cost values

The goal of this proposal is to make the total real runtime of a full
block less than 30 seconds. 30 seconds is a short enough period of
time that prospective miners should be able to process a new block
before the next Bitcoin block 95% of the time (`exp( -1/20 ) ~= 95%`).

To determine a new proposed cost for a Clarity function, we executed a
set of benchmarks for each Clarity cost function in the
[clarity-benchmarking](https://github.com/blockstack/clarity-benchmarking)
Github repository. After running these benchmarks, constant factors in
the runtime functions were fitted using linear regression (given a
transform). These benchmarks produced regression fitted functions for
each Clarity cost function, for example:

```
runtime_ns(cost_secp256k1verify) = 8126809.571429
runtime_ns(cost_or) = 2064.4713444648587 * input_len + 91676.397154
```

The runtime block limit in the Stacks network is `5e9` (unitless), and
the goal of this proposal is that this should correspond to 30 seconds
or `3e10` nanoseconds. So, to convert the `runtime_ns` functions into
runtimes for the Stacks network, we have the simple conversion:

```
runtime_stacks = runtime_ns * 5e9 / 3e10ns
```

For running the benchmarks and analysis in the `clarity-benchmarking`
repository, see the [`README.md`](https://github.com/blockstack/clarity-benchmarking/blob/master/README.md)
file in that repository.

          
# Preamble

SIP Number: 002

Title: The Clarity Smart Contract Language

Author: Aaron Blankstein <aaron@blockstack.com>, Ludo Galabru
<ludo@blockstack.com>

Consideration: Technical

Type: Consensus

Status: Ratified

Created: 29 November 2018

License: BSD 2-Clause

Sign-off: Jude Nelson <jude@stacks.org>, Technical Steering Committee Chair

Discussions-To: https://github.com/stacksgov/sips

# Abstract

In order to support applications which require validation of some
pieces of their logic, we present a smart contracting language for use
with the Stacks blockchain. This smart contracting language can be
used on the Stacks blockchain to support programatic control over
digital assets within the Stacks blockchain (e.g., BNS names, Stacks
tokens, etc.)

While application-chains may use any smart-contract language that they
like, this smart contracting language's VM will be a part of
blockstack-core, and, as such, any blockstack-core node will be able to
validate application chains using this smart contracting language with
a simple configuration change.

This smart contracting language permits static analysis of any legal
smart contract to determine runtime costs. This smart contracting
language is not only Turing-incomplete (a requirement for such static
analysis to be guaranteed successful), but readily permits other kinds
of proofs to be made about the code as well.

# License and Copyright

This SIP is made available under the terms of the BSD-2-Clause license,
available at https://opensource.org/licenses/BSD-2-Clause.  This SIP's copyright
is held by the Stacks Open Internet Foundation.

# Introduction

A smart contract is composed of two parts:

1. A data-space, which is a set of tables of data which only the
   smart contract may modify
2. A set of functions which operate within the data-space of the
   smart contract, though they may call public functions from other smart
   contracts.

Users call smart contracts' public functions by broadcasting a
transaction on the blockchain which invokes the public function.

This smart contracting language differs from most other smart
contracting languages in two important ways:

1. The language _is not_ intended to be compiled. The LISP language
   described in this document is the specification for correctness.
2. The language _is not_ Turing complete. This allows us to guarantee
   that static analysis of programs to determine properties like
   runtime cost and data usage can complete successfully.

# Specification

## Specifying Contracts

A smart contract definition is specified in a LISP language with the
following limitations:

1. Recursion is illegal and there is no `lambda` function.
2. Looping may only be performed via `map`, `filter`, or `fold`
3. The only atomic types are booleans, integers, fixed length
   buffers, and principals
4. There is additional support for lists of the atomic types, however
   the only variable length lists in the language appear as function
   inputs (i.e., there is no support for list operations like append
   or join).
5. Variables may only be created via `let` binding and there
   is no support for mutating functions like `set`.
6. Defining of constants and functions are allowed for simplifying
   code using `define-private` statement. However, these are purely
   syntactic. If a definition cannot be inlined, the contract will be
   rejected as illegal. These definitions are also _private_, in that
   functions defined this way may only be called by other functions
   defined in the given smart contract.
7. Functions specified via `define-public` statements are _public_
   functions.
8. Functions specified via `define-read-only` statements are _public_
   functions and perform _no_ state mutations. Any attempts to 
   modify contract state by these functions or functions called by
   these functions will result in an error.

Public functions return a Response type result. If the function returns
an `ok` type, then the function call is considered valid, and any changes
made to the blockchain state will be materialized. If the function
returns an `err` type, it will be considered invalid, and will have _no
effect_ on the smart contract's state. So if function `foo.A` calls
`bar.B`, and `bar.B` returns an `ok`, but `foo.A` returns an `err`, no
effects from calling `foo.A` materialize--- including effects from
`bar.B`. If, however, `bar.B` returns an `err` and `foo.A` returns an `ok`,
there may be some database effects which are materialized from
`foo.A`, but _no_ effects from calling `bar.B` will materialize.

Unlike functions created by `define-public`, which may only return
Response types, functions created with `define-read-only` may return
any type.

## List Operations

* Lists may be multi-dimensional (i.e., lists may contain other lists), however each
  entry of this list must be of the same type.
* `filter` `map` and `fold` functions may only be called with user-defined functions
  (i.e., functions defined with `(define-private ...)`, `(define-read-only ...)`, or
  `(define-public ...)`) or simple native functions (e.g., `+`, `-`, `not`).
* Functions that return lists of a different size than the input size
  (e.g., `(append-item ...)`) take a required _constant_ parameter that indicates
  the maximum output size of the function. This is enforced with a runtime check.

## Inter-Contract Calls

A smart contract may call functions from other smart contracts using a
`(contract-call?)` function.

This function returns a Response type result-- the return value of the called smart
contract function. Note that if a called smart contract returns an
`err` type, it is guaranteed to not alter any smart contract state
whatsoever. Of course, any transaction fees paid for the execution
of that function will not be returned.

We distinguish 2 different types of `contract-call?`:

* Static dispatch: the callee is a known, invariant contract available
on-chain when the caller contract is being deployed. In this case, the
callee's principal is provided as first argument, followed by the name
of the method and its arguments:

```scheme
(contract-call?
    'SC3H92H297DX3YDPFHZGH90G8Z4NPH4VE8E83YWAQ.registrar
    register-name
    name-to-register)
```

This approach must always be preferred, when adequate.
It makes static analysis easier, and eliminates the
potential for reentrancy bugs when the contracts are
being published (versus when being used).

* Dynamic dispatch: the callee is passed as an argument, and typed
as a trait reference (<A>).

```scheme
(define-public (swap (token-a <can-transfer-tokens>)
                     (amount-a uint)
                     (owner-a principal)
                     (token-b <can-transfer-tokens>)
                     (amount-b uint)
                     (owner-b principal)))
     (begin
         (unwrap! (contract-call? token-a transfer-from? owner-a owner-b amount-a))
         (unwrap! (contract-call? token-b transfer-from? owner-b owner-a amount-b))))
```

Traits can either be locally defined:

```scheme
(define-trait can-transfer-tokens (
    (transfer-from? (principal principal uint) (response uint)))
```

Or imported from an existing contract:

```scheme
(use-trait can-transfer-tokens
    'SC3H92H297DX3YDPFHZGH90G8Z4NPH4VE8E83YWAQ.contract-defining-trait.can-transfer-tokens)
```

Looking at trait conformance, callee contracts have two different paths.
They can either be "compatible" with a trait by defining methods
matching some of the methods defined in a trait, or explicitely declare
conformance using the `impl-trait` statement:

```scheme
(impl-trait 'SC3H92H297DX3YDPFHZGH90G8Z4NPH4VE8E83YWAQ.contract-defining-trait.can-transfer-tokens)
```

Explicit conformance should be prefered when adequate.
It acts as a safeguard by helping the static analysis system to detect
deviations in method signatures before contract deployment.

The following limitations are imposed on contract calls:

1. On static dispatches, callee smart contracts _must_ exist at the time of creation.
2. No cycles may exist in the call graph of a smart contract. This
   prevents recursion (and re-entrancy bugs). Such structures can
   be detected with static analysis of the call graph, and will be
   rejected by the network.
3. `contract-call?` are for inter-contract calls only. Situations
   where the caller is also the callee will result in abortion of
   the ongoing transaction.

## Principals and Owner Verification

The language provides a primitive for checking whether or not the
smart contract transaction was signed by a particular
_principal_. Principals are a specific type in the smart contracting
language which represent a spending entity (roughly equivalent to a
Stacks address). The signature itself is not checked by the smart
contract, but by the VM. A smart contract function can use a globally
defined variable to obtain the current principal:

```scheme
tx-sender
```

The `tx-sender` variable does not change during inter-contract
calls. This means that if a transaction invokes a function in a given
smart contract, that function is able to make calls into other smart
contracts without that variable changing. This enables a wide variety
of applications, but it comes with some dangers for users of smart
contracts. However, as mentioned before, the static analysis
guarantees of our smart contracting language allow clients to know a
priori which functions a given smart contract will ever call.

Another global variable, `contract-caller`, _does_ change during
inter-contract calls. In particular, `contract-caller` is the contract
principal corresponding to the most recent invocation of `contract-call?`.
In the case of a "top-level" invocation, this variable is equal to `tx-sender`.

Assets in the smart contracting language and blockchain are
"owned" by objects of the principal type, meaning that any object of
the principal type may own an asset. For the case of public-key hash
and multi-signature Stacks addresses, a given principal can operate on
their assets by issuing a signed transaction on the blockchain. _Smart
contracts_ may also be principals (reprepresented by the smart
contract's identifier), however, there is no private key associated
with the smart contract, and it cannot broadcast a signed transaction
on the blockchain.

In order to allow smart contracts to operate on assets it owns, smart
contracts may use the special function:

```scheme
(as-contract (...))
```

This function will execute the closure (passed as an argument) with
the `tx-sender` and `contract-caller` set to the _contract's_
principal, rather than the current sender. It returns the return value
of the provided closure. A smart contract may use the special variable
`contract-principal` to refer to its own principal.

For example, a smart contract that implements something like a "token
faucet" could be implemented as so:

```scheme
(define-public (claim-from-faucet)
  (if (is-none? (map-get claimed-before (tuple (sender tx-sender))))
      (let ((requester tx-sender)) ;; set a local variable requester = tx-sender
        (map-insert! claimed-before (tuple (sender requester)) (tuple (claimed true)))
        (as-contract (stacks-transfer! requester 1))))
      (err 1))
```

Here, the public function `claim-from-faucet`:

1. Checks if the sender has claimed from the faucet before
2. Assigns the tx sender to a requester variable
3. Adds an entry to the tracking map
4. Uses `as-contract` to send 1 microstack

The primitive function `is-contract?` can be used to determine
whether a given principal corresponds to a smart contract.

## Stacks Transfer Primitives

To interact with Stacks balances, smart contracts may call the
`(stacks-transfer!)` function. This function will attempt to transfer
from the current principal to another principal:


```scheme
(stacks-transfer!
  to-send-amount
  recipient-principal)
```

This function itself _requires_ that the operation have been signed by
the transferring principal. The `integer` type in our smart contracting
language is an 16-byte signed integer, which allows it to specify the
maximum amount of microstacks spendable in a single Stacks transfer.

Like any other public smart contract function, this function call
returns an `ok` if the transfer was successful, and `err` otherwise.

## Data-Space Primitives

Data within a smart contract's data-space is stored within
`maps`. These stores relate a typed-tuple to another typed-tuple
(almost like a typed key-value store). As opposed to a table data
structure, a map will only associate a given key with exactly one
value. Values in a given mapping are set or fetched using:

1. `(map-get map-name key-tuple)` - This fetches the value
  associated with a given key in the map, or returns `none` if there
  is no such value.
2. `(map-set! map-name key-tuple value-tuple)` - This will set the
  value of `key-tuple` in the data map
3. `(map-insert! map-name key-tuple value-tuple)` - This will set
  the value of `key-tuple` in the data map if and only if an entry
  does not already exist.
4. `(map-delete! map-name key-tuple)` - This will delete `key-tuple`
   from the data map

We chose to use data maps as opposed to other data structures for two
reasons:

1. The simplicity of data maps allows for both a simple implementation
within the VM, and easier reasoning about functions. By inspecting a
given function definition, it is clear which maps will be modified and
even within those maps, which keys are affected by a given invocation.
2. The interface of data maps ensures that the return types of map
operations are _fixed length_, which is a requirement for static
analysis of smart contracts' runtime, costs, and other properties.

A smart contract defines the data schema of a data map with the
`define-map` call. The `define-map` function may only be called in the
top-level of the smart-contract (similar to `define-private`). This
function accepts a name for the map, and a definition of the structure
of the key and value types. Each of these is a list of `(name, type)`
pairs, and they specify the input and output type of `map-get`.
Types are either the values `'principal`, `'integer`, `'bool` or
the output of a call to `(buffer n)`, which defines an n-byte
fixed-length buffer. 

This interface, as described, disallows range-queries and
queries-by-prefix on data maps. Within a smart contract function,
you cannot iterate over an entire map.

### Record Type Syntax

To support the use of _named_ fields in keys and values, our language
allows the construction of named tuples using a function `(tuple ...)`,
e.g.,

```
(define-constant imaginary-number-a (tuple (real 1) (i 2)))
(define-constant imaginary-number-b (tuple (real 2) (i 3)))

```

This allows for creating named tuples on the fly, which is useful for
data maps where the keys and values are themselves named tuples. To
access a named value of a given tuple, the function `(get #name
tuple)` will return that item from the tuple.

### Time-shifted Evaluations

The Stacks language supports _historical_ data queries using the
`(at-block)` function:

```
(at-block 0x0101010101010101010101010101010101010101010101010101010101010101
  ; returns owner principal of name represented by integer 12013
  ;  at the time of block 0x010101...
  (map-get name-map 12013))
```

This function evaluates the supplied closure as if evaluated at the end of
the supplied block, returning the resulting value. The supplied
closure _must_ be read-only (is checked by the analysis).

The supplied block hash must correspond to a known block in the same
fork as the current block, otherwise a runtime error will occur and the
containing transaction will _fail_. Note that if the supplied block
pre-dates any of the data structures being read within the closure (i.e.,
the block is before the block that constructed a data map), a runtime
error will occur and the transaction will _fail_.

## Library Support and Syntactic Sugar

There are a number of ways that the developer experience can be
improved through the careful addition of improved syntax. For example,
while the only atomic types supported by the smart contract language
are integers, buffers, booleans, and principals, so if a developer
wishes to use a buffer to represent a fixed length string, we should
support syntax for representing a buffer literal using something like
an ASCII string. Such support should also be provided by transaction
generation libraries, where buffer arguments may be supplied strings
which are then automatically converted to buffers. There are many
possible syntactic improvements and we expect that over the course
of developing the prototype, we will have a better sense for which
of those improvements we should support. Any such synactic changes
will appear in an eventual language specification, but we believe
them to be out of scope for this proposal.

## Static Analysis

One of the design goals of our smart contracting language was the
ability to statically analyze smart contracts to obtain accurate
upper-bound estimates of transaction costs (i.e., runtime and storage
requirements) as a function of input lengths. By limiting the types
supported, the ability to recurse, and the ability to iterate, we
believe that the language as presented is amenable to such static
analysis based on initial investigations.

The essential step in demonstrating the possibility of accurate and
useful analysis of our smart contract definitions is demonstrating
that any function within the language specification has an output
length bounded by a constant factor of the input length. If we can
demonstrate this, then statically computing runtime or space
requirements involves merely associating each function in the language
specification with a way to statically determine cost as a function of
input length.

Notably, the fact that the cost functions produced by static analysis
are functions of _input length_ means the following things:

1. The cost of a cross-contract call can be "memoized", such
   that a static analyzer _does not_ need to recompute any
   static analysis on the callee when analyzing a caller.
2. The cost of a given public function on a given input size
   _is always the same_, meaning that smart contract developers
   do not need to reason about different cases in which a given
   function may cost more or less to execute.

### Bounding Function Output Length

Importantly, our smart contracting language does not allow the
creation of variable length lists: there are no `list` or
`cons` constructors, and buffer lengths must be statically
defined. Under such requirements (and given that recursion is
illegal), determining the output lengths of functions is rather
directly achievable. To see this, we'll examine trying to compute the
output lengths for the only functions allowed to iterate in the
language:

```
outputLen(map f list<t>)     := Len(list<t>) * outputLen(f t)
outputLen(filter f list<t>)  := Len(list<t>)
outputLen(fold f list<t> s)  := Len(s)
```

Many functions within the language will output values larger than the
function's input, _however_, these outputs will be bound by
statically inferable constants. For example, the data function
_map-get_ will always return an object whose size is equal
to the specified value type of the map.

A complete proof for the static runtime analysis of smart contracts
will be included with the implementation of the language.

## Deploying the Smart Contract

Smart contracts on the Stacks blockchain will be deployed directly as
source code. The goal of the smart contracting language is that the
code of the contract defines the _ground truth_ about the intended
functionality of the contract. While seemingly banal, many systems
chose instead to use a compiler to translate from a friendly
high-level language to a lower-level language deployed on the
blockchain. Such an architecture is needlessly dangerous. A bug in
such a compiler could lead to a bug in a deployed smart contract when
no such bug exists in the original source. This is problematic for
recovery --- a hard fork to "undo" any should-have-been invalid
transactions would be contentious and potentially create a rift in the
community, especially as it will not be easy to deduce which contracts
exactly were affected and for how long. In contrast, bugs in the VM
itself present a more clear case for a hard fork: the smart contract
was defined correctly, as everyone can see directly on the chain, but
illegal transactions were incorrectly marked as valid.

## Virtual Machine API

From the perspective of other components of `blockstack-core`, the
smart contracting VM will provide the following interface:

```
connect-to-database(db)

publish-contract(
  contract-source-code)

  returns: contract-identifier

execute-contract(
  contract-identifier,
  transaction-name,
  sender-principal,
  transaction-arguments)

  returns: true or false if the transaction executed successfully
```

## Invocation and Static Analysis

When processing a client transaction, a `blockstack-core` node will do
one of two things, depending on whether that transaction is a contract
function invocation, or is attempting to publish a new smart contract.

### Contract function invocation

Any transaction which invokes a smart contract will be included in the
blockchain. This is true even for transactions which are
_invalid_. This is because _validating_ an invalid transaction is not
a free operation. The only exceptions to this are transactions which
do not pay more than either a minimum fee or a storage fee
corresponding to the length of the transaction. Transactions which do
not pay a storage fee and clear the minimum transaction fee are
dropped from the mempool.

To process a function invocation, `blockstack-core` does the following:

1. Get the balance of the sender's account. If it's less than the tx fee,
then `RETURN INVALID`.
2. Otherwise, debit the user's account by the tx fee.
3. Look up the contract by hash. If it does not exist, then `RETURN
   INVALID`.
4. Look up the contract's `define-public` function and compare the
   tx's arguments against it. If the tx does not call an existing
   method, or supplies invalid arguments, then `RETURN INVALID`.
5. Look up the cost to execute the given function, and if it is greater
   than the paid tx fee, `RETURN INVALID`.
6. Execute the public function code and commit the effects of running
   the code and `RETURN OK`

### Publish contract

A transaction which creates a new smart contract must pay a fee which
funds the static analysis required to determine the cost of the new
smart contract's public functions. To process such a transaction,
`blockstack-core` will:

1. Check the sender's account balance. If zero, then `RETURN INVALID`
2. Check the tx fee against the user's balance. If it's higher, then `RETURN INVALID`
3. Debit the tx fee from the user's balance.
4. Check the syntax, calculating the fee of verifying each code
   item. If the cost of checking the next item exceeds the tx fee, or
   if the syntax is invalid, then `RETURN INVALID`.
5. Build the AST, and assign a fee for adding each AST item. If the
   cost of adding the next item to the tree exceeds the tx fee (or if
   the AST gets too big), then `RETURN INVALID`.
6. Walk the AST. Each step in the walk incurs a small fee. Do the
   following while the tx fee is higher than the total cost incurred
   by walking to the next node in the AST:
   a. If the next node calls a contract method, then verify that
      the contract exists and the method arguments match the contract's
      `define-public` signature. If not, then `RETURN INVALID`.
   b. Compute the runtime cost of each node in the AST, adding it
      to the function's cost analysis.
7. Find all `define-map` calls to find all tables that need to
   exist. Each step in this incurs a small fee.
8. Create all the tables if the cost of creating them is smaller than
   the remaining tx fee. If not, then RETURN INVALID.
9. `RETURN OK`

## Database Requirements and Transaction Accounting

The smart contract VM needs to interact with a database somewhat
directly: the effects of an `map-insert!` or `map-set!` call are
realized later in the execution of the same transaction. The database
will need to support fairly fine-grained rollbacks as some contract
calls within a transaction's execution may fail, triggering a
rollback, while the transaction execution continues and successfully
completes other database operations.

The database API provided to the smart contract VM, therefore, must be
capable of (1) quickly responding to `map-get` queries, which are
essentially simply key-value _gets_ on the materialized view of the
operation log. The operation log itself is simply a log of the
`map-insert!` and `map-set!` calls. In addition to these
operations, the smart contract VM will be making token transfer calls.
The databasse log should track those operations as well.

In order to aid in accounting for the database operations created by a
given transaction, the underlying database should store, with each
operation entry, the corresponding transaction identifier. This will
be expanded in a future SIP to require the database to store enough
information to reconstruct each block, such that the blocks can be
relayed to bootstrapping peers.

## Clarity Type System

### Types

The Clarity language uses a strong static type system. Function arguments
and database schemas require specified types, and use of types is checked
during contract launch. The type system does _not_ have a universal
super type. The type system contains the following types:

* `(tuple (key-name-0 key-type-0) (key-name-1 key-type-1) ...)` -
  a typed tuple with named fields.
* `(list max-len entry-type)` - a list of maximum length `max-len`, with
  entries of type `entry-type`
* `(response ok-type err-type)` - object used by public functions to commit
  their changes or abort. May be returned or used by other functions as
  well, however, only public functions have the commit/abort behavior.
* `(optional some-type)` - an option type for objects that can either be
  `(some value)` or `none`
* `(buff max-len)` := byte buffer or maximum length `max-len`.
* `principal` := object representing a principal (whether a contract principal
  or standard principal).
* `bool` := boolean value (`true` or `false`)
* `int`  := signed 128-bit integer
* `uint` := unsigned 128-bit integer

### Type Admission

**UnknownType**. The Clarity type system does not allow for specifying
an "unknown" type, however, in type analysis, unknown types may be
constructed and used by the analyzer. Such unknown types are used
_only_ in the admission rules for `response` and `optional` types
(i.e., the variant types).

Type admission in Clarity follows the following rules:

* Types will only admit objects of the same type, i.e., lists will only
admit lists, tuples only admit tuples, bools only admit bools.
* A tuple type `A` admits another tuple type `B` iff they have the exact same
  key names, and every key type of `A` admits the corresponding key type of `B`.
* A list type `A` admits another list type `B` iff `A.max-len >= B.max-len` and
  `A.entry-type` admits `B.entry-type`.
* A buffer type `A` admits another buffer type `B` iff `A.max-len >= B.max-len`.
* An optional type `A` admits another optional type `B` iff:
  * `A.some-type` admits `B.some-type` _OR_ `B.some-type` is an unknown type:
    this is the case if `B` only ever corresponds to `none`
* A response type `A` admits another response type `B` if one of the following is true:
  * `A.ok-type` admits `B.ok-type` _AND_ `A.err-type` admits `B.err-type`
  * `B.ok-type` is unknown _AND_ `A.err-type` admits `B.err-type`
  * `B.err-type` is unknown _AND_ `A.ok-type` admits `B.ok-type`
* Principals, bools, ints, and uints only admit types of the exact same type.

Type admission is used for determining whether an object is a legal argument for
a function, or for insertion into the database. Type admission is _also_ used
during type analysis to determine the return types of functions. In particular,
a function's return type is the least common supertype of each type returned from any
control path in the function. For example:

```
(define-private (if-types (input bool))
  (if input
     (ok 1)
     (err false)))
```

The return type of `if-types` is the least common supertype of `(ok
1)` and `(err false)` (i.e., the most restrictive type that contains
all returns). In this case, that type `(response int bool)`. Because
Clarity _does not_ have a universal supertype, it may be impossible to
determine such a type. In these cases, the functions are illegal, and
will be rejected during type analysis.

## Measuring Transaction Costs for Fee Collection

Our smart contracting language admits static analysis to determine
many properties of transactions _before_ executing those
transactions. In particular, it allows for the VM to count the total
number of runtime operations required, the maximum amount of database
writes, and the maximum number of calls to any expensive primitive
functions like database reads or hash computations. Translating that
information into transaction costs, however, requires more than simply
counting those operations. It requires translating the operations into
a single cost metric (something like gas in Ethereum). Then, clients
can set the fee rate for that metric, and pay the corresponding
transaction fee. Notably, unlike Turing-complete smart contracting
languages, any such fees are known _before_ executing the transaction,
such that clients will no longer need to estimate gas fees. They will,
however, still need to estimate fee rates (much like Bitcoin clients
do today).

Developing such a cost metric is an important task that has
significant consequences. If the metric is a bad one, it could open up
the possibility of denial-of-service attacks against nodes in the
Stacks network. We leave the development of a cost metric to another
Stacks Improvement Proposal, as we believe that such a metric should
be designed by collecting real benchmarking data from something close
to a real system (such measurements will likely be collected through
a combination of hand-crafted benchmarks and fuzzing test suites).

### Maximum Operation Costs and Object Sizes

Even with a cost metric, it is a good idea to set maximums for the
cost of an operation, and the size of objects (like
buffers). Developing good values for constants such as maximum number
of database reads or writes per transaction, maximum size of buffers,
maximum number of arguments to a tuple, maximum size of a smart
contract definition, etc. is a process much like developing a
cost metric--- this is something best done in tandem with the 
production of a prototype. However, we should note that we do intend
to set such limits.


## Example: Simple Naming System

To demonstrate the expressiveness of this smart contracting language,
let's look at an example smart contract which implements a simple
naming system with just two kinds of transactions: _preorder_ and
_register_. The requirements of the system are as follows:

1. Names may only be owned by one principal
2. A register is only allowed if there is a corresponding preorder
   with a matching hash
3. A register transaction must be signed by the same principal who
   paid for the preorder
4. A preorder must have paid at least the price of the name. Names
   are represented as integers, and any name less than 100000 costs
   1000 microstacks, while all other names cost 100 microstacks.
5. Preorder hashs are _globally_ unique.

In this simple scheme, names are represented by integers, but in
practice, a buffer would probably be used.

```scheme
(define-constant burn-address '1111111111111111111114oLvT2)
(define-private (price-function name)
  (if (< name 1e5) 1000 100))

(define-map name-map 
  { name: uint } { buyer: principal })
(define-map preorder-map
  { name-hash: (buff 160) }
  { buyer: principal, paid: uint })

(define-public (preorder 
               (name-hash (buffer 20))
               (name-price integer))
  (if (and (is-ok? (stacks-transfer!
                    name-price burn-address))
           (map-insert! preorder-map
            (tuple (name-hash name-hash))
            (tuple (paid name-price)
                   (buyer tx-sender))))
      (ok 0)
      (err 1)))

(define-public (register 
               (recipient-principal principal)
               (name integer)
               (salt integer))
  (let ((preorder-entry
          (map-get preorder-map
                         (tuple (name-hash (hash160 name salt)))))
        (name-entry 
          (map-get name-map (tuple (name name)))))
    (if (and
         ;; must be preordered
         (not (is-none? preorder-entry))
         ;; name shouldn't *already* exist
         (is-none? name-entry)
         ;; preorder must have paid enough
         (<= (price-funcion name)
             (default-to 0 (get paid preorder-entry)))
         ;; preorder must have been the current principal
         (eq? tx-sender
              (expects! (get buyer preorder-entry) (err 1)))
         (map-insert! name-table
           (tuple (name name))
           (tuple (owner recipient))))
         (ok 0)
         (err 1))))
```


Note that Blockstack PBC intends to supply a full BNS (Blockstack
Naming System) smart contract, as well as formal proofs that certain
desirable properties hold (e.g. "names are globally unique", "a
revoked name cannot be updated or transferred", "names cost stacks
based on their namespace price function", "only the principal can
reveal a name on registration", etc.).

# Related Work

Smart contract languages are not new at the time of this writing.  By far the
most well-known one at this time is Solidity [1], a Turing-complete smart
contract programming language for the Ethereum blockchain [2].  Solidity is
compiled, which makes it hard to determine whether or not any differences
between the contract's behavior and its expected behavior are due to bugs in the
source code, the compiler, or the EVM itself.  Clarity is interpreted, so any
bugs that cannot be attributed to a Clarity contract's source code must be due
to bugs in the blockchain itself.  Removing this ambiguity is important for
determining how to triage such problems.  A bug in the runtime system would
justifiably be fixed with a backwards-incompatible change, whereas a bug in the
compiler may not be.

Clarity is decidable, which removes the need for implementing an "out-of-gas"
error condition that is common in Turing-complete smart contract languages.
Clarity programs either run to completion if they do not fail static analysis
checks, or the blockchain transactions that invoke them are never mined in the
first place.

There are many other smart contract programming languages with different design
goals than Clarity.  This section deserves future expansion, and may be added to
after this SIP is ratified.  

[1] Solidity Language.  https://docs.soliditylang.org/en/v0.8.0/
[2] Ethereum blockchain.  https://ethereum.github.io/yellowpaper/paper.pdf

# Backwards Compatibility

Not applicable

# Activation

At least 20 miners must register a name in the `.miner` namespace in Stacks 1.0.
Once the 20th miner has registered, the state of Stacks 1.0 will be snapshotted.
300 Bitcoin blocks later, the Stacks 2.0 blockchain will launch.  With Stacks
2.0 will come the Clarity VM.

# Reference Implementations

The frst reference implementation can be found at
https://github.com/blockstack/stacks-blockchain.

          
# Preamble

SIP Number: 018

Title: Signed Structured Data

Author: Marvin Janssen <https://github.com/MarvinJanssen>

Consideration: Technical

Type: Standard

Status: Ratified

Created: 28 December 2021

License: CC0-1.0

Sign-off: Jude Nelson <jude@stacks.org>

Layer: Applications

Discussions-To: https://github.com/stacksgov/sips

# Abstract

The Signed Structured Data specification describes a standard way to
deterministically encode and sign Structured Data. Structured Data is data that
can be represented in human-readable format and used in applications and smart
contracts. The aim of the standard is to produce signatures that are
straightforward and inexpensive to verify by smart contracts.

# License and Copyright

This SIP is made available under the terms of the Creative Commons CC0 1.0
Universal license, available at
https://creativecommons.org/publicdomain/zero/1.0/ This SIP's copyright is held
by the Stacks Open Internet Foundation.

# Introduction

Digital signatures are at the heart of blockchains. They allow users to transfer
assets, invoke smart contracts and more, without a designated trusted third
party. To perform these actions, a user signs a transaction and broadcasts it to
the network. However, there are situations in which it is desirable to produce
signed messages that are not transactions. A few common use-cases include:

1. prove to an external application or entity that a user is in control of an
   address;
2. authorise an action to be performed by a smart contract at a later stage
   (like a meta transaction, see below);
3. participate in an off-chain mechanism that is later settled on-chain.

It is important that signed messages are understandable for humans. For
transactions, this is obvious: wallet applications display whom is receiving how
many tokens of what kind. Likewise, the input parameters, function name, and
target contract of contract calls are properly listed. Signed messages that are
not transactions should be no different.

The language properties of Clarity make producing and verifying such signed
messages intuitive. The specification therefore leverages existing standards and
encoding schemes. Messages are represented as native Clarity values and encoded
in wire format in preparation for signing. It also makes sure that these signed
messages are chain and application specific. There is a focus on ease of
verification on the smart contract level.

Below is an example of what a wallet application could show when signing a
SIP018 message.

![Wallet SIP018 concept](SIP-018-001.png)

# Specification

The challenge lies in producing messages that are both meaningful to humans as
well as easy to be processed on-chain. Luckily, Clarity is a strongly-typed
interpreted language. Structured Data is therefore a Clarity Value expression as
detailed in _[SIP002: Smart Contract Language
](https://github.com/stacksgov/sips/blob/main/sips/sip-002/sip-002-smart-contract-language.md)_.
These value expressions are encoded in Stacks wire format, as detailed in
_[SIP005: Blocks and Transactions](https://github.com/stacksgov/sips/blob/main/sips/sip-005/sip-005-blocks-and-transactions.md)_,
and then hashed with SHA256. The resulting hash is used as the input for
signing.

## Definitions

_Clarity Value_: a native Clarity value as expressed in _[SIP002: Smart Contract
Language](https://github.com/stacksgov/sips/blob/main/sips/sip-002/sip-002-smart-contract-language.md)_.

_Meta Transaction_: a meta transaction can be understood of as a transaction
that contains another "transaction". For example, a user may sign a message
that, when received by a particular smart contract, causes it to transfer
tokens. A transaction carrying the signed message will then trigger the
underlying action when it is broadcast.

_SHA256_: a cryptographic hash function producing hashes with a length of 32
bytes.

_Signature_: a proof produced by a signing algorithm, in this case ECDSA with
the `secp256k1` curve, encoded in RSV order.

_Structured Data_: A structured representation of a message to sign expressed as
a Clarity Value.

_Wire format_: the underlying encoding for Clarity Values when generating a
transaction, as expressed in _[SIP005: Blocks and Transactions
](https://github.com/stacksgov/sips/blob/main/sips/sip-005/sip-005-blocks-and-transactions.md)_.

## Formal specification

To obtain a signature _S_ of structured data _D_ using private key _K_:

- `S = signStructuredData(D, K)`; where,

- `signStructuredData(D, K) = sign(messageHash(D), K)`; where `sign` produces a
  `secp256k1` signature of a message hash using private key _K_; and,

- `messageHash(D) = sha256(structuredDataPrefix || domainHash || structuredDataHash(D))`;
  where `structuredDataPrefix` is a static value of `"\x53\x49\x50\x30\x31\x38"`
  (which spells "SIP018" in ASCII), and `domainHash` and `structuredDataHash`
  are as described below.

**Definition of** `domainHash`

The domain hash ensures that messages are chain and application specific. A
`domainHash` is generated out of a `domain` Clarity Value:

- `domainHash = structuredDataHash(domain)` where `structuredDataHash` and
  `domain` are as described below.

**Definition of** `structuredDataHash`

The `structuredDataHash` function calculates a hash _H_ of the input Clarity
Value _D_.

- `H = structuredDataHash(D)`; where,
- `structuredDataHash(D) = sha256(ToCVWireFormat(D))`; where `ToCVWireFormat` is
  a function that encodes an input Clarity Value to its wire format
  representation as described in
  _[SIP005](https://github.com/stacksgov/sips/blob/main/sips/sip-005/sip-005-blocks-and-transactions.md)_.

**Definition of** `domain`

The `domain` is a `tuple` type Clarity Value that represents the domain in which
messages are signed. Its Clarity type definition is as follows:

```clarity
{
	name: (string-ascii len)
	version: (string-ascii len)
	chain-id: uint
}
```

`name` contains a readable application name for which Structured Data is being
signed.

`version` contains a string representation of the application version. The
application developer may choose to change this value to ensure that signed
Structured Data of one version of the application is incompatible with another
version.

`chain-id` contains the ID of the chain the Signed Structured Data is targeting.

The length `len` of the string fields are not important as long as the resulting
`domain` Clarity Value does not exceed the permitted maximum length. It is
encouraged that wallet applications display this information to the user. Other
fields may be added per the discretion of the application developer. For
example, another differentiating field may be a `verifying-contract` that
contains the principal of the contract performing the signature validation.

**Definition of Structured Data**

Structured Data can be any valid Clarity Value. It is assumed that it will
usually take the form of a `tuple` type, as it allows for multiple fields with
readable names to be encoded. The standard does permit it to be another type to
remain flexible. Still, developers should consider the informational content of
requesting a user to sign a simple type like a `principal`, `int`, `uint`, or
`bool`. A tuple can of course contain any type.

A future SIP may define standard types of Structured Data.

## Collisions and replay attacks

Structured Data hashes do not collide with _presign-sighashes_ as currently no
Clarity Value in wire format forms a valid Stacks transaction. Furthermore, the
`0x534950303138` byte prefix ensures that the input always differs in the first
six bytes, as Stacks transactions start with a version byte of `0x00` (mainnet)
or `0x80` (testnet), a four byte chain ID, and end with an authorisation type
that must be `0x04` or `0x05` in order to be valid. A Clarity Value in wire
format starts with a byte in the range of `[0x00, 0x0C]`. The prefix is easy to
remember because it spells "SIP018" in ASCII.

Replay attacks across applications, forks, and other signature-compatible chains
are mitigated by prepending a `domainHash` to the `messageHash`. A `domainHash`
is calculated by hashing a `domain` Clarity Value tuple containing the
application name, version, and chain ID.

This SIP is about signing and verifying application and chain-specific
structured data. Replay protection on the application level is out of scope for
the standard. Application developers need to make sure that their applications
behave properly when they receive the same signed structured data more than
once.

## TypeScript implementation

A reference implementation using `stacks.js` is as follows. The
`signStructuredData` function takes `StacksPrivateKey` and a `ClarityValue` and
will return a buffer of length `65` containing the signature in RSV order.

```ts
import {
  ClarityValue,
  serializeCV,
  signWithKey,
  StacksPrivateKey,
  stringAsciiCV,
  tupleCV,
  uintCV,
} from "@stacks/transactions";
import { createHash } from "crypto";

const structuredDataPrefix = Buffer.from([0x53, 0x49, 0x50, 0x30, 0x31, 0x38]);

const chainIds = {
  mainnet: 1,
  testnet: 2147483648,
};

function sha256(data: Buffer): Buffer {
  return createHash("sha256").update(data).digest();
}

function structuredDataHash(structuredData: ClarityValue): Buffer {
  return sha256(serializeCV(structuredData));
}

const domainHash = structuredDataHash(tupleCV({
  "name": stringAsciiCV("Dapp Name"),
  "version": stringAsciiCV("1.0.0"),
  "chain-id": uintCV(chainIds.mainnet),
}));

function signStructuredData(
  privateKey: StacksPrivateKey,
  structuredData: ClarityValue,
): Buffer {
  const messageHash = structuredDataHash(structuredData);
  const input = sha256(
    Buffer.concat([structuredDataPrefix, domainHash, messageHash]),
  );
  const data = signWithKey(privateKey, input.toString("hex")).data;
  return Buffer.from(data.slice(2) + data.slice(0, 2), "hex");
}
```

## Clarity implementation

Verifying signed Structured Data on Stacks 2.1 requires only minimal code. As
the Structured Data itself is application-specific, it is left out of this
reference. The `verify-signed-structured-data` function takes a structured data
hash, a signature, and a signer, and returns `true` or `false` depending on
whether the signature is valid.

```clarity
(define-constant chain-id u1)
(define-constant structured-data-prefix 0x534950303138)

(define-constant message-domain-hash (sha256 (to-consensus-buff
	{
		name: "Dapp Name",
		version: "1.0.0",
		chain-id: chain-id
	}
)))

(define-constant structured-data-header (concat structured-data-prefix message-domain-hash))

(define-read-only (verify-signature (hash (buff 32)) (signature (buff 65)) (signer principal))
	(is-eq (principal-of? (unwrap! (secp256k1-recover? hash signature) false)) (ok signer))
)

(define-read-only (verify-signed-structured-data (structured-data-hash (buff 32)) (signature (buff 65)) (signer principal))
	(verify-signature (sha256 (concat structured-data-header structured-data-hash)) signature signer)
)
```

# Related work

- [EIP712](https://eips.ethereum.org/EIPS/eip-712) is a standard in Ethereum
  (and adopted by other EVM-compatible chains) describing a similar mechanism
  that enables signing of structured data. Some aspects of this SIP were loosely
  inspired by the standard.
- [EIP191](https://eips.ethereum.org/EIPS/eip-191) now describes various signing
  schemes used in the EVM space. When `personal_sign` was introduced, it allowed
  users to sign data without running the risk of inadvertently signing a
  transaction by adding a prefix to the message. The authors were inspired by
  Bitcoin and thus settled on a message structure of
  `<varint_prefix_length><prefix><varint_message_length><message>`. For
  Ethereum, it effectively took the form of `\x19Ethereum Signed Message:\n`,
  followed by the message length, followed by the message. The `\x19` byte
  correctly encodes the prefix length, which is `25`. However, the message
  length was [mistakenly](https://github.com/ethereum/go-ethereum/issues/14794)
  encoded as an ASCII string and not an actual `varint`. It makes verifying
  messages signed with `personal_sign` needlessly expensive on-chain, as the
  message byte length has to be converted to an ASCII string representation
  before the message hash can be obtained. What is more, the messages signed
  with `personal_sign` do not have a predefined structure which makes it very
  difficult for wallet applications to display them properly.
  [MetaMask](https://docs.metamask.io/guide/signing-data.html#a-brief-history),
  a popular EVM browser wallet extension, assesses if a message passed to
  `personal_sign` is valid UTF-8, and will display it as human-readable text if
  so. Otherwise, it converts the message to a hex string and displays that
  instead. Complex messages cannot be meaningfully expressed in UTF-8 whilst
  still allowing smart contracts to process them efficiently. Applications
  developers thus often resorted to requesting users to sign hashes of complex
  messages, making the data being signed completely opaque to the user.
  Considering these problems and Clarity's inherent readability, there is no
  reason for there to ever be a `personal_sign` equivalent in Stacks.

# Backwards Compatibility

Not applicable

# Activation

This SIP will be considered activated if the following two conditions are met:

1. `to-consensus-buff` or equivalent functionality is added to Stacks 2.1.
2. At least one popular wallet application implements the standard before or
   within one year of the launch of Stacks 2.1.

# Reference Implementations

- https://github.com/MarvinJanssen/stx-signed-structured-data

# Test vectors

## Structured data hashing

Using `structuredDataHash(CV)` with an input Clarity Value.

- CV = `asciiCV("Hello World")`:
  `5297eef9765c466d945ad1cb2c81b30b9fed6c165575dc9226e9edf78b8cd9e8`
- CV = `asciiCV("")` (empty string):
  `3c8f1b104592e3ebb2b2602b3979a27e77f586fb4c655369fa4eccb6d545a0f8`
- CV =
  `tupleCV({"name": asciiCV("Test App"), "version": asciiCV("1.0.0"), "chain-id": uintCV(1)})`
  (domain tuple):
  `2538b5dc06c5ae2f11549261d7ae174d9f77a55a92b00f330884695497be5065`

## Message hashing

Using `messageHash(CV)`, which is
`sha256(Prefix || structuredDataHash(Domain) || structuredDataHash(CV))`.

- Prefix = `0x534950303138` (constant value)
- Domain =
  `tupleCV({"name": asciiCV("Test App"), "version": asciiCV("1.0.0"), "chain-id": uintCV(1)})`
- CV = `asciiCV("Hello World")`
- Message hash:
  `1bfdab6d4158313ce34073fbb8d6b0fc32c154d439def12247a0f44bb2225259`

## Message signing

Using the following parameters:

- Private key =
  `753b7cc01a1a2e86221266a154af739463fce51219d97e4f856cd7200c3bd2a601`
- Corresponding public key =
  `0390a5cac7c33fda49f70bc1b0866fa0ba7a9440d9de647fecb8132ceb76a94dfa`
- Corresponding address: `ST1PQHQKV0RJXZFY1DGX8MNSNYVE3VGZJSRTPGZGM`

And the following inputs to obtain the message hash for signing:

- Domain =
  `tupleCV({"name": asciiCV("Test App"), "version": asciiCV("1.0.0"), "chain-id": uintCV(1)})`
- CV = `asciiCV("Hello World")`
- (Message hash:
  `1bfdab6d4158313ce34073fbb8d6b0fc32c154d439def12247a0f44bb2225259`)

Produces the following signature:

- `8b94e45701d857c9f1d1d70e8b2ca076045dae4920fb0160be0642a68cd78de072ab527b5c5277a593baeb2a8b657c216b99f7abb5d14af35b4bf12ba6460ba401`

Which can be verified in Clarity:

```clarity
(secp256k1-verify 0x1bfdab6d4158313ce34073fbb8d6b0fc32c154d439def12247a0f44bb2225259 0x8b94e45701d857c9f1d1d70e8b2ca076045dae4920fb0160be0642a68cd78de072ab527b5c5277a593baeb2a8b657c216b99f7abb5d14af35b4bf12ba6460ba401 0x0390a5cac7c33fda49f70bc1b0866fa0ba7a9440d9de647fecb8132ceb76a94dfa)
true
```

          
# Preamble

SIP Number: 007

Title: Stacking Consensus

Author:
    Muneeb Ali <muneeb@blockstack.com>,
    Aaron Blankstein <aaron@blockstack.com>,
    Michael J. Freedman <mfreed@cs.princeton.edu>,
    Diwaker Gupta <diwaker@blockstack.com>,
    Jude Nelson <jude@blockstack.com>, 
    Jesse Soslow <jesse@blockstack.com>, 
    Patrick Stanley <patrick@blockstack.com>

Consideration: Technical

Type: Consensus

Status: Ratified

Created: 14 January 2020

License: BSD 2-Clause

Sign-off: Jude Nelson <jude@stacks.org>, Technical Steering Committee Chair

Discussions-To: https://github.com/stacksgov/sips

# Abstract

This SIP proposes a new consensus algorithm, called Stacking, that
uses the proof-of-work cryptocurrency of an established blockchain to
secure a new blockchain. An economic benefit of the Stacking consensus
algorithm is that the holders of the new cryptocurrency can earn a
reward in a base cryptocurrency by actively participating in the
consensus algorithm.

This SIP proposes to change the mining mechanism of the Stacks
blockchain. [SIP-001](../sip-001/sip-001-burn-election.md) introduced
proof-of-burn (PoB) where a base cryptocurrency is destroyed to
participate in mining of a new cryptocurrency. This proposal argues
that a new mining mechanism called proof-of-transfer (PoX) will be an
improvement over proof-of-burn.

With proof-of-transfer, instead of destroying the base cryptocurrency,
miners are required to distribute the base cryptocurrency to existing
holders of the new cryptocurrency who participate in the consensus
algorithm. Therefore, existing holders of the new cryptocurrency have
an economic incentive to participate, do useful work for the network,
and receive rewards.

Proof-of-transfer avoids burning of the base cryptocurrency which
destroys some supply of the base cryptocurrency. Stacking in general
can be viewed as a more "efficient" algorithm where instead of
destroying a valuable resource (like electricity or base
cryptocurrency), the valuable resource is distributed to holders of
the new cryptocurrency.

The SIP describes one potential implementation of the Stacking
consensus algorithm for the Stacks blockchain using Bitcoin as the
base cryptocurrency.

# Introduction

Consensus algorithms for public blockchains require computational or
financial resources to secure the blockchain state. Mining mechanisms
used by these algorithms are broadly divided into proof-of-work (PoW),
in which nodes dedicate computational resources, and proof-of-stake
(PoS), in which nodes dedicate financial resources. The intention
behind both proof-of-work and proof-of-stake is to make it practically
infeasible for any single malicious actor to have enough computational
power or ownership stake to attack the network.

With proof-of-work, a miner does some "work" that consumes electricity
and is rewarded with digital currency. The miner is, theoretically,
converting electricity and computing power into the newly minted
digital currency. Bitcoin is an example of this and is by far the
largest and most secure PoW blockchain.

With proof-of-stake, miners stake their holdings of a new digital
currency to participate in the consensus algorithm and bad behavior
can be penalized by "slashing" the funds of the miner. PoS requires
less energy/electricity to be consumed and can give holders of the new
cryptocurrency who participate in staking a reward on their holdings
in the new cryptocurrency.

In this SIP we introduce a new consensus algorithm called
Stacking. The Stacking consensus algorithm uses a new type of mining
mechanism called *proof-of-transfer* (PoX). With PoX, miners are not
converting electricity and computing power to newly minted tokens, nor
are they staking their cryptocurrency. Rather they use an existing PoW
cryptocurrency to secure a new, separate blockchain.

This SIP proposes to change the mining
mechanism of the Stacks blockchain from proof-of-burn (SIP-001) to
proof-of-transfer.

The PoX mining mechanism is a modification of proof-of-burn (PoB)
mining (See
the [Blockstack Technical Whitepaper](https://blockstack.org/papers)
and [SIP-001](./sip-001-burn-election.md)). In
proof-of-burn mining, miners burn a base cryptocurrency to participate
in mining — effectively destroying the base cryptocurrency to mint
units of a new cryptocurrency. **In proof-of-transfer, rather than
destroying the base cryptocurrency, miners transfer the base
cryptocurrency as a reward to owners of the new cryptocurrency**. In
the case of the Stacks blockchain, miners would transfer Bitcoin to
owners of Stacks tokens in order for miners to receive newly-minted
Stacks tokens. The security properties of proof-of-transfer are
comparable to proof-of-burn.

# Specification

## Stacking with Bitcoin

In the Stacking consensus protocol, we require the base cryptocurrency
to be a proof-of-work blockchain. In this proposed implementation of
Stacking we assume that the PoW crypto-currency is Bitcoin, given it
is by far the most secure PoW blockchain. Theoretically, other PoW
blockchains can be used, but the security properties of Bitcoin are
currently superior to other PoW blockchains.

As with PoB, in PoX, the protocol selects the winning miner (*i.e.*,
the leader) of a round using a verifiable random function (VRF). The
leader writes the new block of the Stacks blockchain and mints the
rewards (newly minted Stacks). However, instead of bitcoins being sent
to burn addresses, the bitcoins are sent to a set of specific
addresses corresponding to Stacks (STX) tokens holders that are adding
value to the network. Thus, rather than being destroyed, the bitcoins
consumed in the mining process go to productive Stacks holders as a
reward based on their holdings of Stacks and participation in the
Stacking algorithm.

## Stacking Consensus Algorithm

In addition to the normal tasks of PoB mining
(see [SIP-001](./sip-001-burn-election.md)), the Stacking consensus
algorithm *must* determine the set of addresses that miners may
validly transfer funds to. PoB mining does not need to perform these
steps, because the address is always the same — the burn
address. However, with Stacking, network participants must be able to
validate the addresses that are sent to.

Progression in Stacking consensus happens over *reward cycles*. In
each reward cycle, a set of Bitcoin addresses are iterated over, such
that each Bitcoin address in the set of reward addresses has exactly
one Bitcoin block in which miners will transfer funds to the reward
address.

To qualify for a reward cycle, an STX holder must:


* Control a Stacks wallet with >= 0.02% of the total share of unlocked
  Stacks tokens (currently, there are ~470m unlocked Stacks tokens,
  meaning this would require ~94k Stacks). This threshold level
  adjusts based on the participation levels in the Stacking protocol.
* Broadcast a signed message before the reward cycle begins that:
    * Locks the associated Stacks tokens for a protocol-specified
      lockup period.
    * Specifies a Bitcoin address to receive the funds.
    * Votes on a Stacks chain tip.

Miners participating in the Stacks blockchain compete to lead blocks
by transferring Bitcoin. Leaders for particular Stacks blocks are
chosen by sortition, weighted by the amount of Bitcoin sent (see
SIP-001). Before a reward cycle begins, the Stacks network must reach
consensus on which addresses are valid recipients. Reaching consensus
on this is non-trivial: the Stacks blockchain itself has many
properties independent from the Bitcoin blockchain, and may experience
forks, missing block data, etc., all of which make reaching consensus
difficult. As an extreme example, consider a miner that forks the
Stacks chain with a block that claims to hold a large fraction (e.g.,
100%) of all Stacks holdings, and proceeds to issue block commitments
that pay all of the fees to themselves. How can other nodes on the
network detect that this miner’s commitment transfers are invalid?

The Stacking algorithm addresses this with a two-phase cycle. Before
each reward cycle, Stacks nodes engage in a *prepare* phase, in which
two items are decided:


1. An **anchor block** — the anchor block is a Stacks chain block. For
   the duration of the reward cycle, mining any descendant forks of
   the anchor block requires transferring mining funds to the
   appropriate reward addresses.
2. The **reward set** -- the reward set is the set of Bitcoin
   addresses which will receive funds in the reward cycle. This set is
   determined using Stacks chain state from the anchor block.

During the reward cycle, miners contend with one another to become the
leader of the next Stacks block by broadcasting *block commitments* on
the Bitcoin chain. These block commitments send Bitcoin funds to
either a burn address or a PoX reward address.

Address validity is determined according to two different rules:


1. If a miner is building off of any chain tip *which is not a
   descendant of the anchor block*, all of the miner's commitment
   funds must be burnt.
2. If a miner is building off a descendant of the anchor block, the
   miner must send commitment funds to 2 addresses from the reward
   set, chosen as follows:
    * Use the verifiable random function (also used by sortition) to
      choose 2 addresses from the reward set. These 2 addresses are
      the reward addresses for this block.
    * Once addresses have been chosen for a block, these addresses are
      removed from the reward set, so that future blocks in the reward
      cycle do not repeat the addresses.

Note that the verifiable random function (VRF) used for address
selection ensures that the same addresses are chosen by each miner
selecting reward addresses. If a miner submits a burn commitment which
*does not* send funds to a valid address, those commitments are
ignored by the rest of the network (because other network participants
can deduce that the transfer addresses are invalid).

To reduce the complexity of the consensus algorithm, Stacking reward
cycles are fixed length --- if fewer addresses participate in the
Stacking rewards than there are slots in the cycle, then the remaining
slots are filled with *burn* addresses. Burn addresses are included
in miner commitments at fixed intervals (e.g, if there are 1000 burn
addresses for a reward cycle, then each miner commitment would have
1 burn address as an output).

### Adjusting Reward Threshold Based on Participation

Each reward cycle may transfer miner funds to up to 4000 Bitcoin
addresses (2 addresses in a 2000 burn block cycle). To ensure that
this number of addresses is sufficient to cover the pool of
participants (given 100% participation of liquid STX), the threshold
for participation must be 0.025% (1/4000th) of the liquid supply of
STX. However, if participation is _lower_ than 100%, the reward pool
could admit lower STX holders. The Stacking protocol specifies **2
operating levels**:

* **25%** If fewer than `0.25 * STX_LIQUID_SUPPLY` STX participate in
  a reward cycle, participant wallets controlling `x` STX may include
  `floor(x / (0.0000625*STX_LIQUID_SUPPLY))` addresses in the reward set.
  That is, the minimum participation threshold is 1/16,000th of the liquid
  supply.
* **25%-100%** If between `0.25 * STX_LIQUID_SUPPLY` and `1.0 *
  STX_LIQUID_SUPPLY` STX participate in a reward cycle, the reward
  threshold is optimized in order to maximize the number of slots that
  are filled. That is, the minimum threshold `T` for participation will be
  roughly 1/4,000th of the participating STX (adjusted in increments
  of 10,000 STX). Participant wallets controlling `x` STX may
  include `floor(x / T)` addresses in the
  reward set.

In the event that a Stacker signals and locks up enough STX to submit
multiple reward addresses, but only submits one reward address, that
reward address will be included in the reward set multiple times.

### Submitting Reward Address and Chain Tip Signaling

Stacking participants must broadcast signed messages for three purposes:

1. Indicating to the network how many STX should be locked up, and for
   how many reward cycles.
2. Indicate support for a particular chain tip.
3. Specifying the Bitcoin address for receiving Stacking rewards.

These messages may be broadcast either on the Stacks chain or the
Bitcoin chain. If broadcast on the Stacks chain, these messages must
be confirmed on the Stacks chain _before_ the anchor block for the
reward period. If broadcast on the Bitcoin chain, they may be
broadcast during the prepare phase, but must be included before
the prepare phase finishes.

These signed messages are valid for at most 12 reward cycles (25200 Bitcoin
blocks or ~7 months). If the signed message specifies a lockup period `x` less
than 25200 blocks, then the signed message is only valid for Stacking
participation for `floor(x / 2100)` reward cycles (the minimum participation
length is one cycle: 2100 blocks).


## Anchor Blocks and Reward Consensus

In the **prepare** phase of the Stacking algorithm, miners and network
participants determine the anchor block and the reward set. The
prepare phase is a window `w` of Bitcoin blocks *before* the reward
cycle begins (e.g., the window may be 100 Bitcoin blocks).

At a high-level, nodes determine whether any block was confirmed by
`F*w` blocks during the phase, where `F` is a large fraction (e.g.,
`0.8`). Once the window `w` closes at time `cur`, Stacks nodes find
the potential anchor block as described in the following pseudocode:


```python
def find_anchor_block(cur):
  blocks_worked_on = get_all_stacks_blocks_between(cur - w, cur)

  # get the highest/latest ancestor before the PREPARE phase for each block worked
  # on during the PREPARE phase.

  candidate_anchors = {}
  for block in blocks_worked_on:
    pre_window_ancestor = last_ancestor_of_block_before(block, cur - w)
    if pre_window_ancestor is None:
      continue
    if pre_window_ancestor in candidate_anchors:
      candidate_anchors[pre_window_ancestor] += 1
    else:
      candidate_anchors[pre_window_ancestor] = 1

  # if any block is confirmed by at least F*w, then it is the anchor block.
  for candidate, confirmed_by_count in candidate_anchors.items():
    if confirmed_by_count >= F*w
      return candidate

  return None
```

Note that there can be at most one anchor block (so long as `F >
0.5`), because:

* Each of the `w` blocks in the prepare phase has at most one
  candidate ancestor.
* The total possible number of confirmations for anchor blocks is `w`.
* If any block is confirmed by `>= 0.5*w`, then any other block must
  have been confirmed by `< 0.5*w`.

The prepare phase, and the high threshold for `F`, are necessary to
protect the Stacking consensus protocol from damage due to natural
forks, missing block data, and potentially malicious participants. As
proposed, PoX and the Stacking protocol require that Stacks nodes are
able to use the anchor block to determine the *reward set*. If, by
accident or malice, the data associated with the anchor block is
unavailable to nodes, then the Stacking protocol cannot operate
normally — nodes cannot know whether or not a miner is submitting
valid block commitments. A high threshold for `F` ensures that a large
fraction of the Stacks mining power has confirmed the receipt of the
data associated with the anchor block.

### Recovery from Missing Data

In the extreme event that a malicious miner *is* able to get a hidden
or invalid block accepted as an anchor block, Stacks nodes must be
able to continue operation. To do so, Stacks nodes treat missing
anchor block data as if no anchor block was chosen for the reward
cycle — the only valid election commitments will therefore be *burns*
(this is essentially a fallback to PoB). If anchor block data which
was previously missing is revealed to the Stacks node, it must
reprocess all of the leader elections for that anchor block's
associated reward cycle, because there may now be many commitments
which were previously invalid that are now valid.

Reprocessing leader elections is computationally expensive, and
would likely result in a large reorganization of the Stacks
chain. However, such an election reprocessing may only occur once per
reward window (only one valid anchor block may exist for a reward
cycle, whether it was hidden or not). Crucially, intentionally
performing such an attack would require collusion amongst a large
fraction `F` of the Stacks mining power — because such a hidden block
must have been confirmed by `w*F` subsequent blocks. If collusion
amongst such a large fraction of the Stacks mining power is possible,
we contend that the security of the Stacks chain would be compromised
through other means beyond attacking anchor blocks.

### Anchoring with Stacker Support.

The security of anchor block selection is further increased through
Stacker support transactions. In this protocol, when Stacking
participants broadcast their signed participation messages, they
signal support of anchor blocks. This is specified by the chain tip's
hash, and the support signal is valid as long as the message itself is
valid.

This places an additional requirement on anchor block selection. In
addition to an anchor block needing to reach a certain number of miner
confirmations, it must also pass some threshold `t` of valid Stacker
support message signals. This places an additional burden on an anchor
block attack --- not only must the attacker collude amongst a large
fraction of mining power, but they must also collude amongst a
majority of the Stacking participants in their block.

## Stacker Delegation

The process of delegation allows a Stacks wallet address (the
represented address) to designate another address (the delegate
address) for participating in the Stacking protocol. This delegate
address, for as long as the delegation is valid, is able to sign and
broadcast Stacking messages (i.e., messages which lock up Stacks,
designate the Bitcoin reward address, and signal support for chain
tips) on behalf of the represented address. This allows the owner of
the represented address to contribute to the security of the network
by having the delegate address signal support for chain tips. This
combats potential attacks on the blockchain stability by miners that
may attempt to mine hidden forks, hide eventually invalid forks, and
other forms of miner misbehavior.

Supporting delegation adds two new transaction types to the Stacks
blockchain:

* **Delegate Funds.** This transaction initiates a
  represented-delegate relationship. It carries the following data:
    * Delegate address
    * End Block: the Bitcoin block height at which this relationship
      terminates, unless a subsequent delegate funds transaction updates
      the relationship.
    * Delegated Amount: the total amount of STX from this address
      that the delegate address will be able to issue Stacking messages
      on behalf of.
    * Reward Address (_optional_): a Bitcoin address that must be
      designated as the funds recipient in the delegate’s Stacking
      messages. If unspecified, the delegate can choose the address.
* **Terminate Delegation.** This transaction terminates a
  represented-delegate relationship. It carries the following data:
    * Delegate Address
    
_Note_: There is only ever one active represented-delegate
relationship between a given represented address and delegate address
(i.e., the pair _(represented-address, delegate-address)_ uniquely
identifies a relationship). If a represented-delegate relationship is
still active and the represented address signs and broadcasts a new
"delegate funds" transaction, the information from the new transaction
replaces the prior relationship.

Both types of delegation transactions must be signed by the
represented address. These are transactions on the Stacks blockchain,
and will be implemented via a native smart contract, loaded into the
blockchain during the Stacks 2.0 genesis block. These transactions,
therefore, are `contract-call` invocations. The invoked methods are
guarded by:

```
    (asserts! (is-eq contract-caller tx-sender) (err u0))
```

This insures that the methods can only be invoked by direct
transaction execution.

**Evaluating Stacking messages in the context of delegation.** In
order to determine which addresses’ STX should be locked by a given
Stacking message, the message must include the represented address in
the Stacking message. Therefore, if a single Stacks address is the
delegate for many represented Stacks addresses, the delegate address
must broadcast a Stacking message for each of the represented
addresses.

## Addressing Miner Consolidation in Stacking

PoX when used for Stacking rewards could lead to miner
consolidation. Because miners that _also_ participate as Stackers
could gain an advantage over miners who do not participate as
Stackers, miners would be strongly incentivized to buy Stacks and use
it to crowd out other miners. In the extreme case, this consolidation
could lead to centralization of mining, which would undermine the
decentralization goals of the Stacks blockchain. While we are actively
investigating additional mechanisms to address this potential
consolidation, we propose a time-bounded PoX mechanism and a Stacker-
driven mechanism here.

**Time-Bounded PoX.** Stacking rewards incentivize miner consolidation
if miners obtain _permanent_ advantages for obtaining the new
cryptocurrency. However, by limiting the time period of PoX, this
advantage declines over time. To do this, we define two time periods for Pox:

1. **Initial Phase.** In this phase, Stacking rewards proceed as
   described above -- commitment funds are sent to Stacking rewards
   addresses, except if a miner is not mining a descendant of the
   anchor block, or if the registered reward addresses for a given
   reward cycle have all been exhausted. This phase will last for
   approximately 2 years (100,000 Bitcoin blocks).

2. **Sunset Phase.** After the initial phase, a _sunset_ block is
   determined. This sunset block will be ~8 years (400,000 Bitcoin
   blocks) after the sunset phase begins. After the sunset block,
   _all_ miner commitments must be burned, rather than transfered to
   reward addresses. During the sunset phase, the reward / burn ratio
   linearly decreases by `0.25%` (1/400) on each reward cycle, such
   that in the 200th reward cycle, the ratio of funds that are
   transfered to reward addresses versus burnt must be equal to
   `0.5`. For example, if a miner commits 10 BTC, the miner must send
   5 BTC to reward addresses and 5 BTC to the burn address.

By time-bounding the PoX mechanism, we allow the Stacking protocol to
use PoX to help bootstrap support for the new blockchain, providing
miners and holders with incentives for participating in the network
early on. Then, as natural use cases for the blockchain develop and
gain steam, the PoX system could gradually scale down.

**Stacker-driven PoX.**  To further discourage miners from consolidating,
holders of liquid (i.e. non-Stacked) STX tokens may vote to disable PoX in the next upcoming
reward cycle.  This can be done with any amount of STX, and the act of voting
to disable PoX does not lock the tokens.

This allows a community of vigilent
users guard the chain from bad miner behavior arising from consolidation
on a case-by-case basis.  Specifically, if a fraction _R_ of liquid STX
tokens vote to disable PoX, it is disabled
only for the next reward cycle.  To continuously deactivate PoX, the STX
holders must continuously vote to disable it.

Due to the costs of remaining vigilent, this proposal recommends _R = 0.25_.
At the time of this writing, this is higher than any single STX allocation, but
not so high that large-scale cooperation is needed to stop a mining cartel.

## Bitcoin Wire Formats

Supporting PoX in the Stacks blockchain requires modifications to the
wire format for leader block commitments, and the introduction of new
wire formats for burnchain PoX participation (e.g., performing the STX
lockup on the burnchain).


### Leader Block Commits

For PoX, leader block commitments are similar to PoB block commits: the constraints on the
BTC transaction's inputs are the same, and the `OP_RETURN` output is identical. However,
the _burn output_ is no longer the same. For PoX, the following constraints are applied to
the second through nth outputs:

1. If the block commitment is in a reward cycle, with a chosen anchor block, and this block
   commitment builds off a descendant of the PoX anchor block (or the anchor block itself),
   then the commitment must use the chosen PoX recipients for the current block.
      * PoX recipients are chosen as described in "Stacking Consensus Algorithm": addresses
        are chosen without replacement, by using the previous burn block's sortition hash,
        mixed with the previous burn block's burn header hash as the seed for the ChaCha12
        pseudorandom function to select M addresses.
      * The leader block commit transaction must use the selected M addresses as outputs [1, M]
        That is, the second through (M+1)th output correspond to the select PoX addresses.
        The order of these addresses does not matter. Each of these outputs must receive the
        same amount of BTC.
      * If the number of remaining addresses in the reward set N is less than M, then the leader
        block commit transaction must burn BTC by including (M-N) burn outputs.
2. Otherwise, the second through (M+1)th output must be burn addresses, and the amount burned by
   these outputs will be counted as the amount committed to by the block commit.

In addition, during the sunset phase (i.e., between the 100,000th and 500,000th burn block in the chain),
the miner must include a _sunset burn_ output. This is an M+1 indexed output that includes the burn amount
required to fulfill the sunset burn ratio, and must be sent to the burn address:

```
sunset_burn_amount = (total_block_commit_amount) * (reward_cycle_start_height - 100,000) / (400,000)
```

Where `total_block_commit_amount` is equal to the sum of outputs [1, M+1].

After the sunset phase _ends_ (i.e., blocks >= 500,000th burn block), block commits are _only_ burns, with
a single burn output at index 1.

### STX Operations on Bitcoin

As described above, PoX allows stackers to submit `stack-stx`
operations on Bitcoin as well as on the Stacks blockchain. The Stacks
chain also allows addresses to submit STX transfers on the Bitcoin
chain. Such operations are only evaluated by the miner of an anchor block
elected in the burn block that immediately follows the burn block that included the
operations. For example, if a `TransferStxOp` occurs in burnchain block 100, then the
Stacks block elected by burnchain block 101 will process that transfer.

In order to submit on the Bitcoin chain, stackers must submit two Bitcoin transactions:

* `PreStxOp`: this operation prepares the Stacks blockchain node to validate the subsequent
  `StackStxOp` or `TransferStxOp`.
* `StackStxOp`: this operation executes the `stack-stx` operation.
* `TransferStxOp`: this operation transfers STX from a sender to a recipient

The wire formats for the above operations are as follows:

#### PreStxOp

This operation includes an `OP_RETURN` output for the first Bitcoin output that looks as follows:

```
            0      2  3
            |------|--|
             magic  op 
```

Where `op = p` (ascii encoded).

Then, the second Bitcoin output _must_ be Stacker address that will be used in a `StackStxOp`. This
address must be a standard address type parseable by the stacks-blockchain node.

#### StackStxOp

The first input to the Bitcoin operation _must_ consume a UTXO that is
the second output of a `PreStxOp`. This validates that the `StackStxOp` was signed
by the appropriate Stacker address.

This operation includes an `OP_RETURN` output for the first Bitcoin output:

```
            0      2  3                             19        20
            |------|--|-----------------------------|---------|
             magic  op         uSTX to lock (u128)     cycles (u8)
```

Where `op = x` (ascii encoded).

Where the unsigned integer is big-endian encoded.

The second Bitcoin output will be used as the reward address for any stacking rewards.

#### TransferStxOp

The first input to the Bitcoin operation _must_ consume a UTXO that is
the second output of a `PreStxOp`. This validates that the `TransferStxOp` was signed
by the appropriate STX address.

This operation includes an `OP_RETURN` output for the first Bitcoin output:

```
            0      2  3                             19        80
            |------|--|-----------------------------|---------|
             magic  op     uSTX to transfer (u128)     memo (up to 61 bytes)
```

Where `op = $` (ascii encoded).

Where the unsigned integer is big-endian encoded.

The second Bitcoin output is either a `p2pkh` or `p2sh` output such
that the recipient Stacks address can be derived from the
corresponding 20-byte hash (hash160).

# Related Work

This section will be expanded upon after this SIP is ratified.

# Backwards Compatibility

Not applicable.

# Activation

At least 20 miners must register a name in the `.miner` namespace in Stacks 1.0.
Once the 20th miner has registered, the state of Stacks 1.0 will be snapshotted.
300 Bitcoin blocks later (Bitcoin block 666050), the Stacks 2.0 blockchain will launch.  Stacks 2.0
implements this SIP.

# Reference Implementations

Implemented in Rust.  See https://github.com/blockstack/stacks-blockchain.


          